<html><head><title>A Little Riak Book</title></head><meta charset="utf-8"><link href="style.css" media="screen" rel="stylesheet" type="text/css"><link href='http://fonts.googleapis.com/css?family=Playfair+Display' rel='stylesheet' type='text/css'><body><h1 class="title">A Little Riak Book</h1>

<h3 class="author">Eric Redmond</h3>

<h3 class="author">John Daily</h3>

<div class="intro">
Here's a free little book about <a href="http://docs.basho.com/riak/latest/">Riak</a>, a scalable, high availability NoSQL datastore. 

Other formats:
<a href="https://github.com/coderoshi/little_riak_book/blob/master/rendered/riaklil-en.epub?raw=true" class="epub">ePub</a>
<a href="https://github.com/coderoshi/little_riak_book/blob/master/rendered/riaklil-en.mobi?raw=true" class="mobi">mobi</a>
<a href="https://github.com/coderoshi/little_riak_book/blob/master/rendered/riaklil-en.pdf?raw=true" class="pdf">PDF</a>
</div>
<ul>
<li>
<a href="#introduction">Introduction</a>
<ul>
<li>
<a href="#downtime-roulette">Downtime Roulette</a>
</li>
<li>
<a href="#what-is-riak">What is Riak</a>
</li>
<li>
<a href="#about-this-book">About This Book</a>
</li>
<li>
<a href="#new-in-2.0">New in 2.0</a>
</li>
</ul>
</li>
</ul>
<ul>
<li>
<a href="#concepts">Concepts</a>
<ul>
<li>
<a href="#the-landscape">The Landscape</a>
</li>
<li>
<a href="#riak-components">Riak Components</a>
</li>
<li>
<a href="#replication-and-partitions">Replication and Partitions</a>
</li>
<li>
<a href="#practical-tradeoffs">Practical Tradeoffs</a>
</li>
<li>
<a href="#wrapup">Wrapup</a>
</li>
</ul>
</li>
</ul>
<ul>
<li>
<a href="#developers">Developers</a>
<ul>
<li>
<a href="#lookup">Lookup</a>
</li>
<li>
<a href="#conditional-requests">Conditional requests</a>
<ul>
<li>
<a href="#get">GET</a>
</li>
<li>
<a href="#put-&-delete">PUT &amp; DELETE</a>
</li>
</ul>
</li>
<li>
<a href="#bucket-types/buckets">Bucket Types/Buckets</a>
</li>
<li>
<a href="#datatypes">Datatypes</a>
</li>
<li>
<a href="#entropy">Entropy</a>
</li>
<li>
<a href="#querying">Querying</a>
</li>
<li>
<a href="#wrap-up">Wrap-up</a>
</li>
</ul>
</li>
</ul>
<ul>
<li>
<a href="#operators">Operators</a>
<ul>
<li>
<a href="#clusters">Clusters</a>
</li>
<li>
<a href="#managing-a-cluster">Managing a Cluster</a>
</li>
<li>
<a href="#new-in-riak-2.0">New in Riak 2.0</a>
</li>
<li>
<a href="#how-riak-is-built">How Riak is Built</a>
</li>
<li>
<a href="#tools">Tools</a>
</li>
<li>
<a href="#wrap-up">Wrap-up</a>
</li>
</ul>
</li>
</ul>
<ul>
<li>
<a href="#writing-riak-applications">Writing Riak Applications</a>
<ul>
<li>
<a href="#how-not-to-write-a-riak-app">How Not to Write a Riak App</a>
<ul>
<li>
<a href="#dynamic-querying">Dynamic querying</a>
</li>
<li>
<a href="#normalization">Normalization</a>
</li>
<li>
<a href="#ducking-conflict-resolution">Ducking conflict resolution</a>
</li>
<li>
<a href="#mutability">Mutability</a>
</li>
<li>
<a href="#select-*-from-&lt;table&gt;">SELECT * FROM &amp;lt;table&amp;gt;</a>
</li>
<li>
<a href="#large-objects">Large objects</a>
</li>
<li>
<a href="#running-a-single-server">Running a single server</a>
</li>
<li>
<a href="#further-reading">Further reading</a>
</li>
</ul>
</li>
<li>
<a href="#denormalization">Denormalization</a>
<ul>
<li>
<a href="#disk-space">Disk space</a>
</li>
<li>
<a href="#performance-over-time">Performance over time</a>
</li>
<li>
<a href="#performance-per-request">Performance per request</a>
</li>
<li>
<a href="#what-about-updates?">What about updates?</a>
</li>
<li>
<a href="#further-reading">Further reading</a>
</li>
</ul>
</li>
<li>
<a href="#data-modeling">Data modeling</a>
<ul>
<li>
<a href="#rules-to-live-by">Rules to live by</a>
</li>
<li>
<a href="#further-reading">Further reading</a>
</li>
</ul>
</li>
<li>
<a href="#conflict-resolution">Conflict Resolution</a>
<ul>
<li>
<a href="#conflict-resolution-strategies">Conflict resolution strategies</a>
</li>
<li>
<a href="#last-write-wins">Last write wins</a>
</li>
<li>
<a href="#data-types">Data types</a>
</li>
<li>
<a href="#strong-consistency">Strong consistency</a>
</li>
<li>
<a href="#conflicting-resolution">Conflicting resolution</a>
</li>
<li>
<a href="#further-reading">Further reading</a>
</li>
</ul>
</li>
<li>
<a href="#request-tuning">Request tuning</a>
<ul>
<li>
<a href="#key-concepts">Key concepts</a>
</li>
<li>
<a href="#tuning-parameters">Tuning parameters</a>
<ul>
<li>
<a href="#leave-this-alone">Leave this alone</a>
</li>
<li>
<a href="#configure-at-the-bucket">Configure at the bucket</a>
</li>
<li>
<a href="#configure-at-the-bucket-or-per-request">Configure at the bucket or per-request</a>
</li>
<li>
<a href="#impact">Impact</a>
</li>
</ul>
</li>
<li>
<a href="#write-failures">Write failures</a>
<ul>
<li>
<a href="#eventual-consistency">Eventual consistency</a>
</li>
<li>
<a href="#strong-consistency">Strong consistency</a>
</li>
</ul>
</li>
<li>
<a href="#tuning-for-immutable-data">Tuning for immutable data</a>
</li>
<li>
<a href="#further-reading">Further reading</a>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li>
<a href="#notes">Notes</a>
<ul>
<li>
<a href="#a-short-note-on-riakcs">A Short Note on RiakCS</a>
</li>
<li>
<a href="#a-short-note-on-mdc">A Short Note on MDC</a>
</li>
<li>
<a href="#locks,-a-cautionary-tale">Locks, a cautionary tale</a>
<ul>
<li>
<a href="#lock,-a-first-draft">Lock, a first draft</a>
</li>
<li>
<a href="#lock,-a-second-draft">Lock, a second draft</a>
</li>
<li>
<a href="#lock,-a-third-draft">Lock, a third draft</a>
</li>
<li>
<a href="#lock,-a-fourth-draft">Lock, a fourth draft</a>
</li>
<li>
<a href="#conclusion">Conclusion</a>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="introduction">Introduction</h1>

<h2 id="downtime-roulette">Downtime Roulette</h2>

<p><img src="../assets/decor/roulette.png" alt="Gambling With Uptime"></p>

<p>Picture a roulette wheel in a casino, where any particular number has a 1 in 37 chance of being hit. Imagine you could place a single bet that a given number will <em>not</em> hit (about 97.3% in your favor), and winning would pay out 10 times your wager. Would you make that bet? I&#39;d reach for my wallet so fast my thumb would start a fire on my pocket.</p>

<p>Now imagine you could bet again, but only win if the wheel made a sequential 100 spins in your favor, otherwise you lose. Would you still play? Winning a single bet might be easy, but over many trials the odds are not in your favor.</p>

<p>People make these sorts of bets with data all of the time. A single server has a good chance of remaining available. When you run a cluster with thousands of servers, or billions of requests, the odds of any one breaking down becomes the rule.</p>

<p>A once-in-a-million disaster is commonplace in light of a billion opportunities.</p>

<h2 id="what-is-riak">What is Riak</h2>

<p>Riak is an open-source, distributed key/value database for high availability, fault-tolerance, and near-linear scalability. In short, Riak has remarkably high uptime and grows with you.</p>

<!-- image: phone with 1/0's flying from it to a disk array -->

<p>As the modern world stitches itself together with increasingly intricate connections, major shifts are occurring in information management. The web and networked devices spur an explosion of data collection and access unseen in the history of the world. The magnitude of values stored and managed continues to grow at a staggering rate, and in parallel, more people than ever require fast and reliable access to this data. This trend is known as <em>Big Data</em>.</p>

<p><aside id="big-data" class="sidebar"><h3>So What is Big Data?</h3></p>

<p>There&#39;s a lot of discussion around what constitutes <em>Big Data</em>.</p>

<p>I have a 6 Terabyte RAID in my house to store videos and other backups. Does that count? On the other hand, CERN grabbed about 200 Petabytes looking for the Higgs boson.</p>

<!-- image: raid box -->

<p>It&#39;s a hard number to pin down, because Big Data is a personal figure. What&#39;s big to one might be small to another. This is why many definitions don&#39;t refer to byte count at all, but instead about relative potentials. A reasonable, albeit wordy, definition of Big Data is provided by Gartner:</p>

<p><blockquote><em>Big Data are high-volume, high-velocity, and/or high-variety information figures that require new forms of processing to enable enhanced decision making, insight discovery and process optimization.</em></blockquote></aside></p>

<h3>Always Bet on Riak</h3>

<p>The sweet spot of Riak is high-volume (data that&#39;s available to read and write when you need it), high-velocity (easily responds to growth), and high-variety information figures (you can store any type of data as a value).</p>

<p>Riak was built as a solution to real Big Data problems, based on the <em>Amazon Dynamo</em> design. Dynamo is a highly available design---meaning that it responds to requests quickly at very large scales, even if your application is storing and serving terabytes of data a day. Riak had been used in production prior to being open-sourced in 2009. It&#39;s currently used by Github, Comcast, Voxer, Disqus and others, with the larger systems storing hundreds of TBs of data, and handling several GBs per node daily.</p>

<p>Riak was written in the Erlang programming language. Erlang was chosen due to its strong support for concurrency, solid distributed communication, hot code loading, and fault-tolerance. It runs on a virtual machine, so running Riak requires an Erlang installation.</p>

<p>So should you use Riak? A good rule of thumb for potential users is to ask yourself if every moment of downtime will cost you in some way (money, users, etc). Not all systems require such extreme amounts of uptime, and if you don&#39;t, Riak may not be for you.</p>

<h2 id="about-this-book">About This Book</h2>

<p>This is not an &quot;install and follow along&quot; guide. This is a &quot;read and comprehend&quot; guide. Don&#39;t feel compelled to have Riak, or even have a computer handy, when starting this book. You may feel like installing at some point, and if so, instructions can be found in the <a href="http://docs.basho.com">Riak docs</a>.</p>

<p>In my opinion, the most important section of this book is the <a href="#concepts">concepts chapter</a>. If you already have a little knowledge it may start slow, but it picks up in a hurry. After laying the theoretical groundwork, we&#39;ll move onto helping <a href="#developers">developers</a> use Riak, by learning how to query it and tinker with some settings. Finally, we&#39;ll go over the basic details that <a href="#operators">operators</a> should know, such as how to set up a Riak cluster, configure some values, use optional tools, and more.</p>

<h2 id="new-in-2.0">New in 2.0</h2>

<p>Riak 2.0 represents a major shift in the capabilities and focus of Riak as a data store. Riak has always been primarily focused on operational simplicity, and that has not changed. But when it came to design decisions, operations were always given priority over the needs of developers. This is changing. With the launch of 2.0, we&#39;ve added some features that developers have wanted to see for quite a while. Namely, the following:</p>

<ul>
<li><strong>Strong Consistency</strong>. Riak is still Eventually Consistent, but now you have a choice. Riak is now the easiest to manage database for adjusting the spectrum smoothly between AP and CP... per bucket, no less.</li>
<li><strong>Better Search</strong>. The makers of Riak have improved search by leveraging the power of the Solr search engine. You now get all of the queryability of distributed Solr, without the hassle of manual indexing.</li>
<li><strong>Datatypes</strong>. Riak historically has provided storage flexibility by allowing the storage of any binary object. This is still the case, but you now have the option of storing distributed maps, sets, counters, and flags that automatically converge in the face of conflicts.</li>
<li><strong>Security</strong>. A long-standing request whose day has finally come. Native Group/User access controls.</li>
<li><strong>Bucket types</strong>. Now you can support unlimited custom bucket properties, without the overhead of the old gossip protocol.</li>
<li><strong>Ring Resizing</strong>. Finally! Where in the past you were limited to a fixed ring size, you now have the option to dynamically increase/decrease the number of vnodes in your cluster.</li>
<li><strong>Other improvements</strong>. We&#39;ve also made many other improvements, like simplified configuration management (no more messing with <code>app.config</code> and <code>vm.args</code>), reduced sibling explosions (via a new logical clock called DVV), improved internal metadata sharing (reducing gossip chatter), better AAE, and more.</li>
</ul>

<p>This book also includes a new chapter written by John Daily, to help guide developers to write productive applications with Riak. We hope you enjoy the new, improved, <em>Not Quite So Little Riak Book</em>.</p>
<h1 id="concepts">Concepts</h1>

<p>Believe me, dear reader, when I suggest that thinking in a distributed fashion is awkward. When I had first encountered Riak, I was not prepared for some of its more preternatural concepts. Our brains just aren&#39;t hardwired to think in a distributed, asynchronous manner. Richard Dawkins coined the term <em>Middle World</em>---the serial, rote land humans encounter every day, which exists between the extremes of the very small strangeness of quarks and the vastness of outer space.</p>

<p>We don&#39;t consider these extremes clearly because we don&#39;t encounter them on a daily basis, just like distributed computations and storage. So we create models and tools to bring the physical act of scattered parallel resources in line to our more ordinary synchronous terms. While Riak takes great pains to simplify the hard parts, it does not pretend that they don&#39;t exist. Just like you can never hope to program at an expert level without any knowledge of memory or CPU management, so too can you never safely develop a highly available cluster without a firm grasp of a few underlying concepts.</p>

<!-- image: caveman confused by a bunch of atoms -->

<h2 id="the-landscape">The Landscape</h2>

<p>The existence of databases like Riak is the culmination of two basic trends: accessible technology spurring different data requirements, and gaps in the data management market.</p>

<!-- image: landscape -->

<p>First, as we&#39;ve seen steady improvements in technology along with reductions in cost, vast amounts of computing power and storage are now within the grasp of nearly anyone. Along with our increasingly interconnected world caused by the web and shrinking, cheaper computers (like smartphones), this has catalyzed an exponential growth of data, and a demand for more predictability and speed by savvier users. In other words, more data is being created on the front-end, while more data is being managed on the backend.</p>

<p>Second, relational database management systems (RDBMS) have become focused over the years for a standard set of use-cases, like business intelligence. They were also technically tuned for squeezing performance out of single larger servers, like optimizing disk access, even while cheap commodity (and virtualized) servers made horizontal growth increasingly attractive. As cracks in relational implementations became apparent, custom implementations arose in response to specific problems not originally envisioned by the relational DBs.</p>

<p>These new databases are collected under the moniker <em>NoSQL</em>, and Riak is of its ilk.</p>

<h3>Database Models</h3>

<p>Modern databases can be loosely grouped into the ways they represent data. Although I&#39;m presenting 5 major types (the last 4 are considered NoSQL models), these lines are often blurred---you can use some key/value stores as a document store, you can use a relational database to just store key/value data.</p>

<p><aside id="joins" class="sidebar"><h3>A Quick note on JOINs</h3></p>

<p>Unlike relational databases, but similar to document and columnar stores, objects cannot be joined by Riak. Client code is responsible for accessing values and merging them, or by other code such as MapReduce.</p>

<p>The ability to easily join data across physical servers is a tradeoff that separates single node databases like relational and graph, from <em>naturally partitionable</em> systems like document, columnar, and key/value stores.</p>

<p>This limitation changes how you model data. Relational normalization (organizing data to reduce redundancy) exists for systems that can cheaply join data together per request. However, the ability to spread data across multiple nodes requires a denormalized approach, where some data is duplicated, and computed values may be stored for the sake of performance.
</aside></p>

<!-- image: icons for each of these types -->

<ol>
<li><p><strong>Relational</strong>. Traditional databases usually use SQL to model and query data.
They are useful for data which can be stored in a highly structured schema, yet
require flexible querying. Scaling a relational database (RDBMS) traditionally
occurs by more powerful hardware (vertical growth).</p>

<p>Examples: <em>PostgreSQL</em>, <em>MySQL</em>, <em>Oracle</em></p></li>
<li><p><strong>Graph</strong>. These exist for highly interconnected data. They excel in
modeling complex relationships between nodes, and many implementations can
handle multiple billions of nodes and relationships (or edges and vertices). I tend to include <em>triplestores</em> and <em>object DBs</em> as specialized variants.</p>

<p>Examples: <em>Neo4j</em>, <em>Graphbase</em>, <em>InfiniteGraph</em></p></li>
<li><p><strong>Document</strong>. Document datastores model hierarchical values called documents,
represented in formats such as JSON or XML, and do not enforce a document schema.
They generally support distributing across multiple servers (horizontal growth).</p>

<p>Examples: <em>CouchDB</em>, <em>MongoDB</em>, <em>Couchbase</em></p></li>
<li><p><strong>Columnar</strong>. Popularized by <a href="http://research.google.com/archive/bigtable.html">Google&#39;s BigTable</a>,
this form of database exists to scale across multiple servers, and groups similar data into
column families. Column values can be individually versioned and managed, though families
are defined in advance, not unlike RDBMS schemas.</p>

<p>Examples: <em>HBase</em>, <em>Cassandra</em>, <em>BigTable</em></p></li>
<li><p><strong>Key/Value</strong>. Key/Value, or KV stores, are conceptually like hashtables,
where values are stored and accessed by an immutable key. They range from
single-server varieties like <em>Memcached</em> used for high-speed caching, to
multi-datacenter distributed systems like <em>Riak Enterprise</em>.</p>

<p>Examples: <em>Riak</em>, <em>Redis</em>, <em>Voldemort</em></p></li>
</ol>

<h2 id="riak-components">Riak Components</h2>

<p>Riak is a Key/Value (KV) database, built from the ground up to safely distribute data across a cluster of physical servers, called nodes. A Riak cluster is also known as a ring (we&#39;ll cover why later).</p>

<!-- For now, we'll only consider the concepts required to be a Riak users, and cover operations later. -->

<p>Riak functions similarly to a very large hash space. Depending on your background, you may call it hashtable, a map, a dictionary, or an object. But the idea is the same: you store a value with an immutable key, and retrieve it later.</p>

<h3>Key and Value</h3>

<p><img src="../assets/decor/addresses.png" alt="A Key is an Address"></p>

<p>Key/value is the most basic construct in all of computerdom. You can think of a key like a home address, such as Bob&#39;s house with the unique key 5124, while the value would be maybe Bob (and his stuff).</p>

<pre><code class="javascript">hashtable[&quot;5124&quot;] = &quot;Bob&quot;
</code></pre>

<p>Retrieving Bob is as easy as going to his house.</p>

<pre><code class="javascript">bob = hashtable[&quot;5124&quot;]
</code></pre>

<p>Let&#39;s say that poor old Bob dies, and Claire moves into this house. The address remains the same, but the contents have changed.</p>

<pre><code class="javascript">hashtable[&quot;5124&quot;] = &quot;Claire&quot;
</code></pre>

<p>Successive requests for <code>5124</code> will now return <code>Claire</code>.</p>

<h3>Buckets</h3>

<!-- image: address streets metaphore -->

<p>Addresses in Riakville are more than a house number, but also a street. There could be another 5124 on another street, so the way we can ensure a unique address is by requiring both, as in <em>5124 Main Street</em>.</p>

<p><em>Buckets</em> in Riak are analogous to street names: they provide logical <a href="http://en.wikipedia.org/wiki/Namespace">namespaces</a> so that identical keys in different buckets will not conflict.</p>

<p>For example, while Alice may live at <em>5122 Main Street</em>, there may be a gas station at <em>5122 Bagshot Row</em>.</p>

<pre><code class="javascript">main[&quot;5122&quot;] = &quot;Alice&quot;
bagshot[&quot;5122&quot;] = &quot;Gas&quot;
</code></pre>

<p>Certainly you could have just named your keys <code>main_5122</code> and <code>bagshot_5122</code>, but buckets allow for cleaner key naming, and have other benefits, such as custom properties. For example, to add new Riak Search 2.0 indexes to a bucket, you might tell Riak to index all values under a bucket like this:</p>

<pre><code class="javascript">main.props = {&quot;search_index&quot;:&quot;homes&quot;}
</code></pre>

<p>Buckets are so useful in Riak that all keys must belong to a bucket. There is no global namespace. The true definition of a unique key in Riak is actually <code>bucket/key</code>.</p>

<h3>Bucket Types</h3>

<p>Starting in Riak 2.0, there now exists a level above buckets, called bucket types. Bucket types are groups of buckets with a similar set of properties. So for the example above, it would be like a bucket of keys:</p>

<pre><code class="javascript">places[&quot;main&quot;][&quot;5122&quot;] = &quot;Alice&quot;
places[&quot;bagshot&quot;][&quot;5122&quot;] = &quot;Gas&quot;
</code></pre>

<p>The benefit here is that a group of distinct buckets can share properties.</p>

<pre><code class="javascript">places.props = {&quot;search_index&quot;:&quot;anyplace&quot;}
</code></pre>

<p>This has practical implications. Previously, you were limited to how many custom bucket properties Riak could support, because any slight change from the default would have to be propogated to every other node in the cluster (via the gossip protocol). If you had ten thousand custom buckets, that&#39;s ten thousand values that were routinely sent amongst every member. Quickly, your system could be overloaded with that chatter, called a <em>gossip storm</em>.</p>

<p>With the addition of bucket types, and the improved communication mechanism that accompanies it, there&#39;s no limit to your bucket count. It also makes managing multiple buckets easier, since every bucket of a type inherits the common properties, you can make across-the-board changes trivially.</p>

<p>Due to its versatility (and downright necessity in some cases) and improved performance, Basho recommends using bucket types whenever possible from this point into the future.</p>

<p>For convenience, we call a <em>type/bucket/key + value</em> pair an <em>object</em>, sparing ourselves the verbosity of &quot;X key in the Y bucket with the Z type, and its value&quot;.</p>

<h2 id="replication-and-partitions">Replication and Partitions</h2>

<p>Distributing data across several nodes is how Riak is able to remain highly available, tolerating outages and network partitions. Riak combines two styles of distribution to achieve this: <a href="http://en.wikipedia.org/wiki/Replication">replication</a> and <a href="http://en.wikipedia.org/wiki/Partition">partitions</a>.</p>

<h3>Replication</h3>

<p><strong>Replication</strong> is the act of duplicating data across multiple servers. Riak replicates by default.</p>

<p>The obvious benefit of replication is that if one node goes down, nodes that contain replicated data remain available to serve requests. In other words, the system remains <em>available</em>.</p>

<p>For example, imagine you have a list of country keys, whose values are those countries&#39; capitals. If all you do is replicate that data to 2 servers, you would have 2 duplicate databases.</p>

<p><img src="../assets/replication.svg" alt="Replication"></p>

<p>The downside with replication is that you are multiplying the amount of storage required for every duplicate. There is also some network overhead with this approach, since values must also be routed to all replicated nodes on write. But there is a more insidious problem with this approach, which I will cover shortly.</p>

<h3>Partitions</h3>

<p>A <strong>partition</strong> is how we divide a set of keys onto separate  physical servers. Rather than duplicate values, we pick one server to exclusively host a range of keys, and the other servers to host remaining non-overlapping ranges.</p>

<p>With partitioning, our total capacity can increase without any big expensive hardware, just lots of cheap commodity servers. If we decided to partition our database into 1000 parts across 1000 nodes, we have (hypothetically) reduced the amount of work any particular server must do to 1/1000th.</p>

<p>For example, if we partition our countries into 2 servers, we might put all countries beginning with letters A-N into Node A, and O-Z into Node B.</p>

<p><img src="../assets/partitions.svg" alt="Partitions"></p>

<p>There is a bit of overhead to the partition approach. Some service must keep track of what range of values live on which node. A requesting application must know that the key <code>Spain</code> will be routed to Node B, not Node A.</p>

<p>There&#39;s also another downside. Unlike replication, simple partitioning of data actually <em>decreases</em> uptime. If one node goes down, that entire partition of data is unavailable. This is why Riak uses both replication and partitioning.</p>

<h3>Replication+Partitions</h3>

<p>Since partitions allow us to increase capacity, and replication improves availability, Riak combines them. We partition data across multiple nodes, as well as replicate that data into multiple nodes.</p>

<p>Where our previous example partitioned data into 2 nodes, we can replicate each of those partitions into 2 more nodes, for a total of 4.</p>

<p>Our server count has increased, but so has our capacity and reliability. If you&#39;re designing a horizontally scalable system by partitioning data, you must deal with replicating those partitions.</p>

<p>The Riak team suggests a minimum of 5 nodes for a Riak cluster, and replicating to 3 nodes (this setting is called <code>n_val</code>, for the number of <em>nodes</em> on which to replicate each object).</p>

<p><img src="../assets/replpart.svg" alt="Replication Partitions"></p>

<!-- If the odds of a node going down on any day is 1%, then the odds of any server going down each day when you have 100 of them is about (1-(0.99^100)) 63%. For sufficiently large systems, servers going down are no longer edge-cases. They become regular cases that must be planned for, and designed into your system.
-->

<h3>The Ring</h3>

<p>Riak applies <em>consistent hashing</em> to map objects along the edge of a circle (the ring).</p>

<p>Riak partitions are not mapped alphabetically (as we used in the examples above), but instead a partition marks a range of key hashes (SHA-1 function applied to a key). The maximum hash value is 2^160, and divided into some number of partitions---64 partitions by default (the Riak config setting is <code>ring_creation_size</code>).</p>

<p>Let&#39;s walk through what all that means. If you have the key <code>favorite</code>, applying the SHA-1 algorithm would return <code>7501 7a36 ec07 fd4c 377a 0d2a 0114 00ab 193e 61db</code> in hexadecimal. With 64 partitions, each has 1/64 of the <code>2^160</code> possible values, making the first partition range from 0 to <code>2^154-1</code>, the second range is <code>2^154</code> to <code>2*2^154-1</code>, and so on, up to the last partition <code>63*2^154-1</code> to <code>2^160-1</code>.</p>

<!-- V=lists:sum([lists:nth(X, H)*math:pow(16, X-1) || X <- lists:seq(1,string:len(H))]) / 64. -->

<!-- V / 2.28359630832954E46. // 2.2.. is 2^154 -->

<p>We won&#39;t do all of the math, but trust me when I say <code>favorite</code> falls within the range of partition 3.</p>

<p>If we visualize our 64 partitions as a ring, <code>favorite</code> falls here.</p>

<p><img src="../assets/ring0.svg" alt="Riak Ring"></p>

<p>&quot;Didn&#39;t he say that Riak suggests a minimum of 5 nodes? How can we put 64 partitions on 5 nodes?&quot; We just give each node more than one partition, each of which is managed by a <em>vnode</em>, or <em>virtual node</em>.</p>

<p>We count around the ring of vnodes in order, assigning each node to the next available vnode, until all vnodes are accounted for. So partition/vnode 1 would be owned by Node A, vnode 2 owned by Node B, up to vnode 5 owned by Node E. Then we continue by giving Node A vnode 6, Node B vnode 7, and so on, until our vnodes have been exhausted, leaving us this list.</p>

<ul>
<li>A = [1,6,11,16,21,26,31,36,41,46,51,56,61]</li>
<li>B = [2,7,12,17,22,27,32,37,42,47,52,57,62]</li>
<li>C = [3,8,13,18,23,28,33,38,43,48,53,58,63]</li>
<li>D = [4,9,14,19,24,29,34,39,44,49,54,59,64]</li>
<li>E = [5,10,15,20,25,30,35,40,45,50,55,60]</li>
</ul>

<p>So far we&#39;ve partitioned the ring, but what about replication? When we write a new value to Riak, it will replicate the result in some number of nodes, defined by a setting called <code>n_val</code>. In our 5 node cluster it defaults to 3.</p>

<p>So when we write our <code>favorite</code> object to vnode 3, it will be replicated to vnodes 4 and 5. This places the object in physical nodes C, D, and E. Once the write is complete, even if node C crashes, the value is still available on 2 other nodes. This is the secret of Riak&#39;s high availability.</p>

<p>We can visualize the Ring with its vnodes, managing nodes, and where <code>favorite</code> will go.</p>

<p><img src="../assets/ring1.svg" alt="Riak Ring"></p>

<p>The Ring is more than just a circular array of hash partitions. It&#39;s also a system of metadata that gets copied to every node. Each node is aware of every other node in the cluster, which nodes own which vnodes, and other system data.</p>

<p>Armed with this information, requests for data can target any node. It will horizontally access data from the proper nodes, and return the result.</p>

<h2 id="practical-tradeoffs">Practical Tradeoffs</h2>

<p>So far we&#39;ve covered the good parts of partitioning and replication: highly available when responding to requests, and inexpensive capacity scaling on commodity hardware. With the clear benefits of horizontal scaling, why is it not more common?</p>

<h3>CAP Theorem</h3>

<p>Classic RDBMS databases are <em>write consistent</em>. Once a write is confirmed, successive reads are guaranteed to return the newest value. If I save the value <code>cold pizza</code> to my key <code>favorite</code>, every future read will consistently return <code>cold pizza</code> until I change it.</p>

<!-- The very act of placing our data in multiple servers carries some inherent risk. -->

<p>But when values are distributed, <em>consistency</em> might not be guaranteed. In the middle of an object&#39;s replication, two servers could have different results. When we update <code>favorite</code> to <code>cold pizza</code> on one node, another node might contain the older value <code>pizza</code>, because of a network connectivity problem. If you request the value of <code>favorite</code> on either side of a network partition, two different results could possibly be returned---the database is inconsistent.</p>

<p>If consistency should not be compromised in a distributed database, we can choose to sacrifice <em>availability</em> instead. We may, for instance, decide to lock the entire database during a write, and simply refuse to serve requests until that value has been replicated to all relevant nodes. Clients have to wait while their results can be brought into a consistent state (ensuring all replicas will return the same value) or fail if the nodes have trouble communicating. For many high-traffic read/write use-cases, like an online shopping cart where even minor delays will cause people to just shop elsewhere, this is not an acceptable sacrifice.</p>

<p>This tradeoff is known as Brewer&#39;s CAP theorem. CAP loosely states that you can have a C (consistent), A (available), or P (partition-tolerant) system, but you can only choose 2. Assuming your system is distributed, you&#39;re going to be partition-tolerant, meaning, that your network can tolerate packet loss. If a network partition occurs between nodes, your servers still run. So your only real choices are CP or AP. Riak 2.0 supports both modes.</p>

<!-- A fourth concept not covered by the CAP theorem, latency, is especially important here. -->

<h3>Strong Consistency</h3>

<p>Since version 2.0, Riak now supports strong Consistency (SC), as well as High Availability (HA). &quot;Waitaminute!&quot; I hear you say, &quot;doesn&#39;t that break the CAP theorem?&quot; Not the way Riak does it. Riak supports setting a bucket type property as strongly consistent. Any bucket of that type is now SC. Meaning, that a request is either successfully replicated to a majority of partitions, or it fails (if you want to sound fancy at parties, just say &quot;Riak SC uses a variant of the vertical Paxos leader election algorithm&quot;).</p>

<p>This, naturally, comes at a cost. As we know from the CAP theorem, if too many nodes are down, the write will fail. You&#39;ll have to repair your node or network, and try the write again. In short, you&#39;ve lost high availability. If you don&#39;t absolutely need strong consistency, consider staying with the high availability default, and tuning it to your needs as we&#39;ll see in the next section.</p>

<h3>Tunable Availability with N/R/W</h3>

<p>A question the CAP theorem demands you answer with a distributed system is: do I give up strong consistency, or give up ensured availability? If a request comes in, do I lock out requests until I can enforce consistency across the nodes? Or do I serve requests at all costs, with the caveat that the database may become inconsistent?</p>

<p>Riak&#39;s solution is based on Amazon Dynamo&#39;s novel approach of a <em>tunable</em> AP system. It takes advantage of the fact that, though the CAP theorem is true, you can choose what kind of tradeoffs you&#39;re willing to make. Riak is highly available to serve requests, with the ability to tune its level of availability---nearing, but never quite reaching, strong consistency. If you want strong consistency, you&#39;ll need to create a special SC bucket type, which we&#39;ll see in a later chapter.</p>

<p><aside class="sidebar"><h3>Not Quite C</h3></p>

<p>Strictly speaking, altering R and W values actually creates a tunable availability/latency tradeoff, rather than availability/consistency. Making Riak run faster by keeping R and W values low will increase the likelihood of temporarily inconsistent results (higher availability). Setting those values higher will improve the <em>odds</em> of consistent responses (never quite reaching strong consistency), but will slow down those responses and increase the likelihood that Riak will fail to respond (in the event of a partition).
</aside></p>

<p>Riak allows you to choose how many nodes you want to replicate an object to, and how many nodes must be written to or read from per request. These values are settings labeled <code>n_val</code> (the number of nodes to replicate to), <code>r</code> (the number of nodes read from before returning), and <code>w</code> (the number of nodes written to before considered successful).</p>

<p>A thought experiment may help clarify things.</p>

<p><img src="../assets/nrw.svg" alt="NRW"></p>

<h4>N</h4>

<p>With our 5 node cluster, having an <code>n_val=3</code> means values will eventually replicate to 3 nodes, as we&#39;ve discussed above. This is the <em>N value</em>. You can set other values (R,W) to equal the <code>n_val</code> number with the shorthand <code>all</code>.</p>

<h4>W</h4>

<p>But you may not wish to wait for all nodes to be written to before returning. You can choose to wait for all 3 to finish writing (<code>w=3</code> or <code>w=all</code>), which means my values are more likely to be consistent. Or you could choose to wait for only 1 complete write (<code>w=1</code>), and allow the remaining 2 nodes to write asynchronously, which returns a response quicker but increases the odds of reading an inconsistent value in the short term. This is the <em>W value</em>.</p>

<p>In other words, setting <code>w=all</code> would help ensure your system was more likely to be consistent, at the expense of waiting longer, with a chance that your write would fail if fewer than 3 nodes were available (meaning, over half of your total servers are down).</p>

<p>A failed write, however, is not necessarily a true failure. The client will receive an error message, but the write will typically still have succeeded on some number of nodes smaller than the <em>W</em> value, and will typically eventually be propagated to all of the nodes that should have it.</p>

<h4>R</h4>

<p>Reading involves similar tradeoffs. To ensure you have the most recent value, you can read from all 3 nodes containing objects (<code>r=all</code>). Even if only 1 of 3 nodes has the most recent value, we can compare all nodes against each other and choose the latest one, thus ensuring some consistency. Remember when I mentioned that RDBMS databases were <em>write consistent</em>? This is close to <em>read consistency</em>. Just like <code>w=all</code>, however, the read will fail unless 3 nodes are available to be read. Finally, if you only want to quickly read any value, <code>r=1</code> has low latency, and is likely consistent if <code>w=all</code>.</p>

<p>In general terms, the N/R/W values are Riak&#39;s way of allowing you to trade lower consistency for more availability.</p>

<h3>Logical Clock</h3>

<p>If you&#39;ve followed thus far, I only have one more conceptual wrench to throw at you. I wrote earlier that with <code>r=all</code>, we can &quot;compare all nodes against each other and choose the latest one.&quot; But how do we know which is the latest value? This is where logical clocks like <em>vector clocks</em> (aka <em>vclocks</em>) come into play.</p>

<p><aside class="sidebar"><h3>DVV</h3></p>

<p>Since Riak 2.0, some internal values have been migrated over to an alternative logical timestamp called Dot Version Vectors (DVV). How they operate isn&#39;t germain to this short lesson, but rather, what is important is basic idea of a logical clock. You can read more about DVVs (or any Riak concept) on the <a href="http://docs.basho.com">Basho docs website</a>.
</aside></p>

<p>Vector clocks measure a sequence of events, just like a normal clock. But since we can&#39;t reasonably keep the clocks on dozens, or hundreds, or thousands of servers in sync (without really exotic hardware, like geosynchronized atomic clocks, or quantum entanglement), we instead keep a running history of updates, and look for logical, rather than temporal, causality.</p>

<p>Let&#39;s use our <code>favorite</code> example again, but this time we have 3 people trying to come to a consensus on their favorite food: Aaron, Britney, and Carrie. These people are called <em>actors</em>, ie. the things responsible for the updates. We&#39;ll track the value each actor has chosen along with the relevant vector clock.</p>

<p>(To illustrate vector clocks in action, we&#39;re cheating a bit. Riak doesn&#39;t track vector clocks via the client that initiated the request, but rather, via the server that coordinates the write request; nonetheless, the concept is the same. We&#39;ll cheat further by disregarding the timestamp that is stored with vector clocks.)</p>

<p>When Aaron sets the <code>favorite</code> object to <code>pizza</code>, a vector clock could contain his name and the number of updates he&#39;s performed.</p>

<pre><code class="yaml">bucket: food
key:    favorite

vclock: {Aaron: 1}
value:  pizza
</code></pre>

<p>Britney now comes along, and reads <code>favorite</code>, but decides to update <code>pizza</code> to <code>cold pizza</code>. When using vclocks, she must provide the vclock returned from the request she wants to update. This is how Riak can help ensure you&#39;re updating a previous value, and not merely overwriting with your own.</p>

<pre><code class="yaml">bucket: food
key:    favorite

vclock: {Aaron: 1, Britney: 1}
value:  cold pizza
</code></pre>

<p>At the same time as Britney, Carrie decides that pizza was a terrible choice, and tried to change the value to <code>lasagna</code>.</p>

<pre><code class="yaml">bucket: food
key:    favorite

vclock: {Aaron: 1, Carrie: 1}
value:  lasagna
</code></pre>

<p>This presents a problem, because there are now two vector clocks in play that diverge from <code>{Aaron: 1}</code>. By default, Riak will store both values.</p>

<p>Later in the day Britney checks again, but this time she gets the two conflicting values (aka <em>siblings</em>, which we&#39;ll discuss in more detail in the next chapter), with two vclocks.</p>

<pre><code class="yaml">bucket: food
key:    favorite

vclock: {Aaron: 1, Britney: 1}
value:  cold pizza
---
vclock: {Aaron: 1, Carrie: 1}
value:  lasagna
</code></pre>

<p>It&#39;s clear that a decision must be made. Perhaps Britney knows that Aaron&#39;s original request was for <code>pizza</code>, and thus two people generally agreed on <code>pizza</code>, so she resolves the conflict choosing that and providing a new vclock.</p>

<pre><code class="yaml">bucket: food
key:    favorite

vclock: {Aaron: 1, Carrie: 1, Britney: 2}
value:  pizza
</code></pre>

<p>Now we are back to the simple case, where requesting the value of <code>favorite</code> will just return the agreed upon <code>pizza</code>.</p>

<p>If you&#39;re a programmer, you may notice that this is not unlike a version control system, like <strong>git</strong>, where conflicting branches may require manual merging into one.</p>

<h3>Datatypes</h3>

<p>New in Riak 2.0 is the concept of datatypes. In the preceding logical clock example, we were responsible for resolving the conflicting values. This is because in the normal case, Riak has no idea what object&#39;s you&#39;re giving it. That is to say, Riak values are <em>opaque</em>. This is actually a powerful construct, since it allows you to store any type of value you want, from plain text, to semi-structured data like XML or JSON, to binary objects like images.</p>

<p>When you decide to use datatypes, you&#39;ve given Riak some information about the type of object you want to store. With this information, Riak can figure out how to resolve conflicts automatically for you, based on some pre-defined behavior.</p>

<p>Let&#39;s try another example. Let&#39;s imagine a shopping cart in an online retailer. You can imagine a shopping cart like a set of items. So each key in our cart contains a <em>set</em> of values.</p>

<p>Let&#39;s say you log into the retailer&#39;s website on your laptop with your username <em>ponies4evr</em>, and choose the Season 2 DVD of <em>My Little Pony: Friendship is Magic</em>. This time, the logical clock will act more like Riak&#39;s, where the node that coordinates the request will be the actor.</p>

<pre><code class="yaml">type:   set
bucket: cart
key:    ponies4evr

vclock: {Node_A: 1}
value:  [&quot;MYPFIM-S2-DVD&quot;]
</code></pre>

<p>Once the DVD was added to the cart bucket, your laptop runs out of batteries. So you take out your trusty smartphone, and log into the retailer&#39;s mobile app. You decide to also add the <em>Bloodsport III</em> DVD. Little did you know, a temporary network partition caused your write to redirect to another node. This partition had no knowledge of your other purchase.</p>

<pre><code class="yaml">type:   set
bucket: cart
key:    ponies4evr

vclock: {Node_B: 1}
value:  [&quot;BS-III-DVD&quot;]
</code></pre>

<p>Happily, the network hiccup was temporary, and thus the cluster heals itself. Under normal circumstances, since the logical clocks did not descend from one another, you&#39;d end up with siblings like this:</p>

<pre><code class="yaml">type:   set
bucket: cart
key:    ponies4evr

vclock: {Node_A: 1}
value:  [&quot;MYPFIM-S2-DVD&quot;]
---
vclock: {Node_B: 1}
value:  [&quot;BS-III-DVD&quot;]
</code></pre>

<p>But since the bucket was designed to hold a <em>set</em>, Riak knows how to automatically resolve this conflict. In the case of conflicting sets, it performs a set union. So when you go to checkout of the cart, the system returns this instead:</p>

<pre><code class="yaml">type:   set
bucket: cart
key:    ponies4evr

vclock: [{Node_A: 1}, {Node_B: 1}]
value:  [&quot;MYPFIM-S2-DVD&quot;, &quot;BS-III-DVD&quot;]
</code></pre>

<p>Datatypes will never return conflicts. This is an important claim to make, because as a developer, you get all of the benefits of dealing with a simple value, with all of the benefits of a distributed, available system. You don&#39;t have to think about handling conflicts. It would be like a version control system where (<em>git</em>, <em>svn</em>, etc) where you never had to merge code---the VCS simply <em>knew</em> what you wanted.</p>

<p>How this all works is beyond the scope of this document. Under the covers it&#39;s implemented by something called <a href="http://docs.basho.com/riak/2.0.0/theory/concepts/crdts/">CRDTs</a> (Conflict-free Replicated Data Types). What&#39;s important to note is that Riak supports four datatypes: <em>map</em>, <em>set</em>, <em>counter</em>, <em>flag</em> (a boolean value). Best of all, maps can nest arbitrarily, so you can create a map whose values are sets, counters, or even other maps. It also supports plain string values called <em>register</em>s.</p>

<p>We&#39;ll see how to use datatypes in the next chapter.</p>

<h3>Riak and ACID</h3>

<p><aside id="acid" class="sidebar"><h3>Distributed Relational is Not Exempt</h3></p>

<p>So why don&#39;t we just distribute a standard relational database? MySQL has the ability to cluster, and it&#39;s ACID (<em>Atomic</em>, <em>Consistent</em>, <em>Isolated</em>, <em>Durable</em>), right? Yes and no.</p>

<p>A single node in the cluster is ACID, but the entire cluster is not without a loss of availability and (often worse) increased latency. When you write to a primary node, and a secondary node is replicated to, a network partition can occur. To remain available, the secondary will not be in sync (eventually consistent). Have you ever loaded from a backup on database failure, but the dataset was incomplete by a few hours? Same idea.</p>

<p>Or, the entire transaction can fail, making the whole cluster unavailable. Even ACID databases cannot escape the scourge of CAP.
</aside></p>

<p>Unlike single node databases like Neo4j or PostgreSQL, Riak does not support <em>ACID</em> transactions. Locking across multiple servers would can write availability, and equally concerning, increase latency. While ACID transactions promise <em>Atomicity</em>, <em>Consistency</em>, <em>Isolation</em>, and <em>Durability</em>---Riak and other NoSQL databases follow <em>BASE</em>, or <em>Basically Available</em>, <em>Soft state</em>, <em>Eventually consistent</em>.</p>

<p>The BASE acronym was meant as shorthand for the goals of non-ACID-transactional databases like Riak. It is an acceptance that distribution is never perfect (basically available), all data is in flux (soft state), and that strong consistency is untenable (eventually consistent) if you want high availability.</p>

<p>Look closely at promises of distributed transactions---it&#39;s often couched in some diminishing adjective or caveat like <em>row transactions</em>, or <em>per node transactions</em>, which basically mean <em>not transactional</em> in terms you would normally use to define it. I&#39;m not claiming it&#39;s impossible, but certainly worth due consideration.</p>

<p>As your server count grows---especially as you introduce multiple datacenters---the odds of partitions and node failures drastically increase. My best advice is to design for it upfront.</p>

<h2 id="wrapup">Wrapup</h2>

<p>Riak is designed to bestow a range of real-world benefits, but equally, to handle the fallout of wielding such power. Consistent hashing and vnodes are an elegant solution to horizontally scaling across servers. N/R/W allows you to dance with the CAP theorem by fine-tuning against its constraints. And vector clocks allow another step closer to consistency by allowing you to manage conflicts that will occur at high load.</p>

<p>We&#39;ll cover other technical concepts as needed, including the gossip protocol, hinted handoff, and read-repair.</p>

<p>Next we&#39;ll review Riak from the user (developer) perspective. We&#39;ll check out lookups, take advantage of write hooks, and examine alternative query options like secondary indexing, search, and MapReduce.</p>
<h1 id="developers">Developers</h1>

<p><aside class="sidebar"><h3>A Note on &quot;Node&quot;</h3></p>

<p>It&#39;s worth mentioning that I use the word &quot;node&quot; a lot. Realistically, this means a physical/virtual server, but really, the workhorses of Riak are vnodes.</p>

<p>When you write to multiple vnodes, Riak will attempt to spread values to as many physical servers as possible. However, this isn&#39;t guaranteed (for example, if you have only 2 physical servers with the default <code>n_val</code> of 3, some data will be copied to the same server twice). You&#39;re safe conceptualizing nodes as Riak instances, and it&#39;s simpler than qualifying &quot;vnode&quot; all the time. If something applies specifically to a vnode, I&#39;ll mention it.
</aside></p>

<p><em>We&#39;re going to hold off on the details of installing Riak at the moment. If you&#39;d like to follow along, it&#39;s easy enough to get started by following the <a href="http://docs.basho.com/riak/latest/">install documentation</a> on the website (<a href="http://docs.basho.com">http://docs.basho.com</a>). If not, this is a perfect section to read while you sit on a train without an Internet connection.</em></p>

<p>Developing with a Riak database is quite easy to do, once you understand some of the finer points. It is a key/value store, in the technical sense (you associate values with keys, and retrieve them using the same keys) but it offers so much more. You can embed write hooks to fire before or after a write, or index data for quick retrieval. Riak has SOLR search, and lets you run MapReduce functions to extract and aggregate data across a huge cluster in relatively short timespans. We&#39;ll show some configurable bucket-specific settings.</p>

<h2 id="lookup">Lookup</h2>

<p><aside class="sidebar"><h3>Supported Languages</h3></p>

<p>Riak 2.0 has official drivers for the following languages:
Erlang, Java, Python, Ruby.</p>

<p>Including community-supplied drivers, supported languages are even more numerous: C/C++, PHP, Clojure, Common Lisp, Dart, Go, Groovy, Haskell, JavaScript (jQuery and NodeJS), Lisp Flavored Erlang, .NET, Perl, PHP, Play, Racket, Scala, Smalltalk.</p>

<p>Dozens of other project-specific addons can be found in the <a href="http://docs.basho.com/riak/latest/">Basho docs</a>.
</aside></p>

<p>Since Riak is a KV database, the most basic commands are setting and getting values. We&#39;ll use the HTTP interface, via curl, but we could just as easily use Erlang, Ruby, Java, or any other supported language.</p>

<p>The basic structure of a Riak request is setting a value, reading it,
and maybe eventually deleting it. The actions are related to HTTP methods
(PUT, GET, POST, DELETE).</p>

<pre><code class="bash">PUT    /types/&lt;type&gt;/buckets/&lt;bucket&gt;/keys/&lt;key&gt;
GET    /types/&lt;type&gt;/buckets/&lt;bucket&gt;/keys/&lt;key&gt;
DELETE /types/&lt;type&gt;/buckets/&lt;bucket&gt;/keys/&lt;key&gt;
</code></pre>

<p>For the examples in this chapter, let&#39;s call an environment variable <code>$RIAK</code> that points to our access node&#39;s URL.</p>

<pre><code class="bash">export RIAK=http://localhost:8098
</code></pre>

<h4>PUT</h4>

<p>The simplest write command in Riak is putting a value. It requires a key, value, and a bucket. In curl, all HTTP methods are prefixed with <code>-X</code>. Putting the value <code>pizza</code> into the key <code>favorite</code> under the <code>food</code> bucket and <code>items</code> bucket type is done like this:</p>

<pre><code class="bash">curl -XPUT &quot;$RIAK/types/items/buckets/food/keys/favorite&quot; \
  -H &quot;Content-Type:text/plain&quot; \
  -d &quot;pizza&quot;
</code></pre>

<p>I threw a few curveballs in there. The <code>-d</code> flag denotes the next string will be the value. We&#39;ve kept things simple with the string <code>pizza</code>, declaring it as text with the proceeding line <code>-H &#39;Content-Type:text/plain&#39;</code>. This defines the HTTP MIME type of this value as plain text. We could have set any value at all, be it XML or JSON---even an image or a video. Riak does not care at all what data is uploaded, so long as the object size doesn&#39;t get much larger than 4MB (a soft limit but one that it is unwise to exceed).</p>

<h4>GET</h4>

<p>The next command reads the value <code>pizza</code> under the type/bucket/key <code>items</code>/<code>food</code>/<code>favorite</code>.</p>

<pre><code class="bash">curl -XGET &quot;$RIAK/types/items/buckets/food/keys/favorite&quot;
pizza
</code></pre>

<p>This is the simplest form of read, responding with only the value. Riak contains much more information, which you can access if you read the entire response, including the HTTP header.</p>

<p>In <code>curl</code> you can access a full response by way of the <code>-i</code> flag. Let&#39;s perform the above query again, adding that flag (<code>-XGET</code> is the default curl method, so we can leave it off).</p>

<pre><code class="bash">curl -i &quot;$RIAK/types/items/buckets/food/keys/favorite&quot;
HTTP/1.1 200 OK
X-Riak-Vclock: a85hYGBgzGDKBVIcypz/fgaUHjmdwZTImMfKcN3h1Um+LAA=
Vary: Accept-Encoding
Server: MochiWeb/1.1 WebMachine/1.9.0 (someone had painted...
Last-Modified: Wed, 10 Oct 2012 18:56:23 GMT
ETag: &quot;1yHn7L0XMEoMVXRGp4gOom&quot;
Date: Thu, 11 Oct 2012 23:57:29 GMT
Content-Type: text/plain
Content-Length: 5

pizza
</code></pre>

<p>The anatomy of HTTP is a bit beyond this little book, but let&#39;s look at a few parts worth noting.</p>

<h5>Status Codes</h5>

<p>The first line gives the HTTP version 1.1 response code <code>200 OK</code>. You may be familiar with the common website code <code>404 Not Found</code>. There are many kinds of <a href="http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html">HTTP status codes</a>, and the Riak HTTP interface stays true to their intent: <strong>1xx Informational</strong>, <strong>2xx Success</strong>, <strong>3xx Further Action</strong>, <strong>4xx Client Error</strong>, <strong>5xx Server Error</strong></p>

<p>Different actions can return different response/error codes. Complete lists can be found in the <a href="http://docs.basho.com/riak/latest/references/apis/">official API docs</a>.</p>

<h5>Timings</h5>

<p>A block of headers represents different timings for the object or the request.</p>

<ul>
<li><strong>Last-Modified</strong> - The last time this object was modified (created or updated).</li>
<li><strong>ETag</strong> - An <em><a href="http://en.wikipedia.org/wiki/HTTP_ETag">entity tag</a></em> which can be used for cache validation by a client.</li>
<li><strong>Date</strong> - The time of the request.</li>
<li><strong>X-Riak-Vclock</strong> - A logical clock which we&#39;ll cover in more detail later.</li>
</ul>

<h5>Content</h5>

<p>These describe the HTTP body of the message (in Riak&#39;s terms, the <em>value</em>).</p>

<ul>
<li><strong>Content-Type</strong> - The type of value, such as <code>text/xml</code>.</li>
<li><strong>Content-Length</strong> - The length, in bytes, of the message body.</li>
</ul>

<p>Some other headers like <code>Link</code> will be covered later in this chapter.</p>

<h4>POST</h4>

<p>Similar to PUT, POST will save a value. But with POST a key is optional. All it requires is a bucket name (and should include a type), and it will generate a key for you.</p>

<p>Let&#39;s add a JSON value to represent a person under the <code>json</code>/<code>people</code> type/bucket. The response header is where a POST will return the key it generated for you.</p>

<pre><code class="bash">curl -i -XPOST &quot;$RIAK/types/json/buckets/people/keys&quot; \
  -H &quot;Content-Type:application/json&quot; \
  -d &#39;{&quot;name&quot;:&quot;aaron&quot;}&#39;
HTTP/1.1 201 Created
Vary: Accept-Encoding
Server: MochiWeb/1.1 WebMachine/1.9.2 (someone had painted...
Location: /riak/people/DNQGJY0KtcHMirkidasA066yj5V
Date: Wed, 10 Oct 2012 17:55:22 GMT
Content-Type: application/json
Content-Length: 0
</code></pre>

<p>You can extract this key from the <code>Location</code> value. Other than not being pretty, this key is treated the same as if you defined your own key via PUT.</p>

<h5>Body</h5>

<p>You may note that no body was returned with the response. For any kind of write, you can add the <code>returnbody=true</code> parameter to force a value to return, along with value-related headers like <code>X-Riak-Vclock</code> and <code>ETag</code>.</p>

<pre><code class="bash">curl -i -XPOST &quot;$RIAK/types/json/buckets/people/keys?returnbody=true&quot; \
  -H &quot;Content-Type:application/json&quot; \
  -d &#39;{&quot;name&quot;:&quot;billy&quot;}&#39;
HTTP/1.1 201 Created
X-Riak-Vclock: a85hYGBgzGDKBVIcypz/fgaUHjmdwZTImMfKkD3z10m+LAA=
Vary: Accept-Encoding
Server: MochiWeb/1.1 WebMachine/1.9.0 (someone had painted...
Location: /riak/people/DnetI8GHiBK2yBFOEcj1EhHprss
Last-Modified: Tue, 23 Oct 2012 04:30:35 GMT
ETag: &quot;7DsE7SEqAtY12d8T1HMkWZ&quot;
Date: Tue, 23 Oct 2012 04:30:35 GMT
Content-Type: application/json
Content-Length: 16

{&quot;name&quot;:&quot;billy&quot;}
</code></pre>

<p>This is true for PUTs and POSTs.</p>

<h4>DELETE</h4>

<p>The final basic operation is deleting keys, which is similar to getting a value, but sending the DELETE method to the <code>type</code>/<code>bucket</code>/<code>key</code>.</p>

<pre><code class="bash">curl -XDELETE &quot;$RIAK/types/json/buckets/people/keys/DNQGJY0KtcHMirkidasA066yj5V&quot;
</code></pre>

<p>A deleted object in Riak is internally marked as deleted, by writing a marker known as a <em>tombstone</em>. Unless configured otherwise, another process called a <em>reaper</em> will later finish deleting the marked objects.</p>

<p>This detail isn&#39;t normally important, except to understand two things:</p>

<ol>
<li>In Riak, a <em>delete</em> is actually a <em>read</em> and a <em>write</em>, and should be considered as such when calculating read/write ratios.</li>
<li>Checking for the existence of a key is not enough to know if an object exists. You might be reading a key after it has been deleted, so you should check for tombstone metadata.</li>
</ol>

<h4>Lists</h4>

<p>Riak provides two kinds of lists. The first lists all <em>buckets</em> in your cluster, while the second lists all <em>keys</em> under a specific bucket. Both of these actions are called in the same way, and come in two varieties.</p>

<p>The following will give us all of our buckets as a JSON object.</p>

<pre><code class="bash">curl &quot;$RIAK/types/default/buckets?buckets=true&quot;

{&quot;buckets&quot;:[&quot;food&quot;]}
</code></pre>

<p>And this will give us all of our keys under the <code>food</code> bucket.</p>

<pre><code class="bash">curl &quot;$RIAK/types/default/buckets/food/keys?keys=true&quot;
{
  ...
  &quot;keys&quot;: [
    &quot;favorite&quot;
  ]
}
</code></pre>

<p>If we had very many keys, clearly this might take a while. So Riak also provides the ability to stream your list of keys. <code>keys=stream</code> will keep the connection open, returning results in chunks of arrays. When it has exhausted its list, it will close the connection. You can see the details through curl in verbose (<code>-v</code>) mode (much of that response has been stripped out below).</p>

<pre><code class="bash">curl -v &quot;$RIAK/types/default/buckets/food/keys?keys=stream&quot;
...

* Connection #0 to host localhost left intact
...
{&quot;keys&quot;:[&quot;favorite&quot;]}
{&quot;keys&quot;:[]}
* Closing connection #0
</code></pre>

<!-- Transfer-Encoding -->

<p>You should note that list actions should <em>not</em> be used in production (they&#39;re really expensive operations). But they are useful for development, investigations, or for running occasional analytics at off-peak hours.</p>

<h2 id="conditional-requests">Conditional requests</h2>

<p>It is possible to use conditional requests with Riak, but these are
fragile due to the nature of its availability/eventual consistency
model.</p>

<h3 id="get">GET</h3>

<p>When retrieving values from Riak via HTTP, a last-modified timestamp
and an <a href="https://en.wikipedia.org/wiki/HTTP_ETag">ETag</a> are
included. These may be used for future <code>GET</code> requests; if the value
has not changed, a <code>304 Not Modified</code> status will be returned.</p>

<p>For example, let&#39;s assume you receive the following headers.</p>

<pre><code class="bash">Last-Modified: Thu, 17 Jul 2014 21:01:16 GMT
ETag: &quot;3VhRP0vnXbk5NjZllr0dDE&quot;
</code></pre>

<p>Note that the quotes are part of the ETag.</p>

<p>If the ETag is used via the <code>If-None-Match</code> header in the next request:</p>

<pre><code class="bash">curl -i &quot;$RIAK/types/default/buckets/food/keys/dinner&quot; \
  -H &#39;If-None-Match: &quot;3VhRP0vnXbk5NjZllr0dDE&quot;&#39;
HTTP/1.1 304 Not Modified
Vary: Accept-Encoding
Server: MochiWeb/1.1 WebMachine/1.10.5 (jokes are better explained)
ETag: &quot;3VhRP0vnXbk5NjZllr0dDE&quot;
Date: Mon, 28 Jul 2014 19:48:13 GMT
</code></pre>

<p>Similarly, the last-modified timestamp may be used with <code>If-Modified-Since</code>:</p>

<pre><code class="bash">curl -i &quot;$RIAK/types/default/buckets/food/keys/dinner&quot; \
  -H &#39;If-Modified-Since: Thu, 17 Jul 2014 21:01:16 GMT&#39;
HTTP/1.1 304 Not Modified
Vary: Accept-Encoding
Server: MochiWeb/1.1 WebMachine/1.10.5 (jokes are better explained)
ETag: &quot;3VhRP0vnXbk5NjZllr0dDE&quot;
Date: Mon, 28 Jul 2014 19:51:39 GMT
</code></pre>

<h3 id="put-&amp;-delete">PUT &amp; DELETE</h3>

<p>When adding, updating, or removing content, the HTTP headers
<code>If-None-Match</code>, <code>If-Match</code>, <code>If-Modified-Since</code>, and
<code>If-Unmodified-Since</code> can be used to specify ETags and timestamps.</p>

<p>If the specified condition cannot be met, a <code>412 Precondition Failed</code>
status will be the result.</p>

<h2 id="bucket-types/buckets">Bucket Types/Buckets</h2>

<p>Although we&#39;ve been using bucket types and buckets as namespaces up to now, they are capable of more.</p>

<p>Different use-cases will dictate whether a bucket is heavily written to, or largely read from. You may use one bucket to store logs, one bucket could store session data, while another may store shopping cart data. Sometimes low latency is important, while other times it&#39;s high durability. And sometimes we just want buckets to react differently when a write occurs.</p>

<h3>Quorum</h3>

<p>The basis of Riak&#39;s availability and tolerance is that it can read from, or write to, multiple nodes. Riak allows you to adjust these N/R/W values (which we covered under <a href="#practical-tradeoffs">Concepts</a>) on a per-bucket basis.</p>

<h4>N/R/W</h4>

<p>N is the number of total nodes that a value should be replicated to, defaulting to 3. But we can set this <code>n_val</code> to less than the total number of nodes.</p>

<p>Any bucket property, including <code>n_val</code>, can be set by sending a <code>props</code> value as a JSON object to the bucket URL. Let&#39;s set the <code>n_val</code> to 5 nodes, meaning that objects written to <code>cart</code> will be replicated to 5 nodes.</p>

<pre><code class="bash">curl -i -XPUT &quot;$RIAK/types/default/buckets/cart/props&quot; \
  -H &quot;Content-Type: application/json&quot; \
  -d &#39;{&quot;props&quot;:{&quot;n_val&quot;:5}}&#39;
</code></pre>

<p>You can take a peek at the bucket&#39;s properties by issuing a GET to the bucket.</p>

<p><em>Note: Riak returns unformatted JSON. If you have a command-line tool like jsonpp (or json_pp) installed, you can pipe the output there for easier reading. The results below are a subset of all the <code>props</code> values.</em></p>

<pre><code class="bash">curl &quot;$RIAK/types/default/buckets/cart/props&quot; | jsonpp
{
  &quot;props&quot;: {
    ...
    &quot;dw&quot;: &quot;quorum&quot;,
    &quot;n_val&quot;: 5,
    &quot;name&quot;: &quot;cart&quot;,
    &quot;postcommit&quot;: [],
    &quot;pr&quot;: 0,
    &quot;precommit&quot;: [],
    &quot;pw&quot;: 0,
    &quot;r&quot;: &quot;quorum&quot;,
    &quot;rw&quot;: &quot;quorum&quot;,
    &quot;w&quot;: &quot;quorum&quot;,
    ...
  }
}
</code></pre>

<p>As you can see, <code>n_val</code> is 5. That&#39;s expected. But you may also have noticed that the cart <code>props</code> returned both <code>r</code> and <code>w</code> as <code>quorum</code>, rather than a number. So what is a <em>quorum</em>?</p>

<h5>Symbolic Values</h5>

<p>A <em>quorum</em> is one more than half of all the total replicated nodes (<code>floor(N/2) + 1</code>). This figure is important, since if more than half of all nodes are written to, and more than half of all nodes are read from, then you will get the most recent value (under normal circumstances).</p>

<p>Here&#39;s an example with the above <code>n_val</code> of 5 ({A,B,C,D,E}). Your <code>w</code> is a quorum (which is <code>3</code>, or <code>floor(5/2)+1</code>), so a PUT may respond successfully after writing to {A,B,C} ({D,E} will eventually be replicated to). Immediately after, a read quorum may GET values from {C,D,E}. Even if D and E have older values, you have pulled a value from node C, meaning you will receive the most recent value.</p>

<p>What&#39;s important is that your reads and writes <em>overlap</em>. As long as <code>r+w &gt; n</code>, in the absence of <em>sloppy quorum</em> (below), you&#39;ll be able to get the newest values. In other words, you&#39;ll have a reasonable level of consistency.</p>

<p>A <code>quorum</code> is an excellent default, since you&#39;re reading and writing from a balance of nodes. But if you have specific requirements, like a log that is often written to, but rarely read, you might find it make more sense to wait for a successful write from a single node, but read from all of them. This affords you an overlap</p>

<pre><code class="bash">curl -i -XPUT &quot;$RIAK/types/default/buckets/logs/props&quot; \
  -H &quot;Content-Type: application/json&quot; \
  -d &#39;{&quot;props&quot;:{&quot;w&quot;:&quot;one&quot;,&quot;r&quot;:&quot;all&quot;}}&#39;
</code></pre>

<ul>
<li><code>all</code> - All replicas must reply, which is the same as setting <code>r</code> or <code>w</code> equal to <code>n_val</code></li>
<li><code>one</code> - Setting <code>r</code> or <code>w</code> equal to <code>1</code></li>
<li><code>quorum</code> - A majority of the replicas must respond, that is, “half plus one”.</li>
</ul>

<h4>Sloppy Quorum</h4>

<p>In a perfect world, a strict quorum would be sufficient for most write requests. However, at any moment a node could go down, or the network could partition, or squirrels get caught in the tubes, triggering the unavailability of a required nodes. This is known as a strict quorum. Riak defaults to what&#39;s known as a <em>sloppy quorum</em>, meaning that if any primary (expected) node is unavailable, the next available node in the ring will accept requests.</p>

<p>Think about it like this. Say you&#39;re out drinking with your friend. You order 2 drinks (W=2), but before they arrive, she leaves temporarily. If you were a strict quorum, you could merely refuse both drinks, since the required people (N=2) are unavailable. But you&#39;d rather be a sloppy drunk... erm, I mean sloppy <em>quorum</em>. Rather than deny the drink, you take both, one accepted <em>on her behalf</em> (you also get to pay).</p>

<p><img src="../assets/decor/drinks.png" alt="A Sloppy Quorum"></p>

<p>When she returns, you slide her drink over. This is known as <em>hinted handoff</em>, which we&#39;ll look at again in the next chapter. For now it&#39;s sufficient to note that there&#39;s a difference between the default sloppy quorum (W), and requiring a strict quorum of primary nodes (PW).</p>

<h5>More than R's and W's</h5>

<p>Some other values you may have noticed in the bucket&#39;s <code>props</code> object are <code>pw</code>, <code>pr</code>, and <code>dw</code>.</p>

<p><code>pr</code> and <code>pw</code> ensure that many <em>primary</em> nodes are available before a read or write. Riak will read or write from backup nodes if one is unavailable, because of network partition or some other server outage. This <code>p</code> prefix will ensure that only the primary nodes are used, <em>primary</em> meaning the vnode which matches the bucket plus N successive vnodes.</p>

<p>(We mentioned above that <code>r+w &gt; n</code> provides a reasonable level of consistency, violated when sloppy quorums are involved.  <code>pr+pw &gt; n</code> allows for a much stronger assertion of consistency, although there are always scenarios involving conflicting writes or significant disk failures where that too may not be enough.)</p>

<p>Finally <code>dw</code> represents the minimal <em>durable</em> writes necessary for success. For a normal <code>w</code> write to count a write as successful, a vnode need only promise a write has started, with no guarantee that write has been written to disk, aka, is durable. The <code>dw</code> setting means the backend service (for example Bitcask) has agreed to write the value. Although a high <code>dw</code> value is slower than a high <code>w</code> value, there are cases where this extra enforcement is good to have, such as dealing with financial data.</p>

<h5>Per Request</h5>

<p>It&#39;s worth noting that these values (except for <code>n_val</code>) can be overridden <em>per request</em>.</p>

<p>Consider a scenario in which you have data that you find very important (say, credit card checkout), and want to help ensure it will be written to every relevant node&#39;s disk before success. You could add <code>?dw=all</code> to the end of your write.</p>

<pre><code class="bash">curl -i -XPUT &quot;$RIAK/types/default/buckets/cart/keys/cart1?dw=all&quot; \
  -H &quot;Content-Type: application/json&quot; \
  -d &#39;{&quot;paid&quot;:true}&#39;
</code></pre>

<p>If any of the nodes currently responsible for the data cannot complete the request (i.e., hand off the data to the storage backend), the client will receive a failure message. This doesn&#39;t mean that the write failed, necessarily: if two of three primary vnodes successfully wrote the value, it should be available for future requests. Thus trading availability for consistency by forcing a high <code>dw</code> or <code>pw</code> value can result in unexpected behavior.</p>

<h3>Hooks</h3>

<p>Another utility of buckets are their ability to enforce behaviors on writes by way of hooks. You can attach functions to run either before, or after, a value is committed to a bucket.</p>

<p>Precommit hooks are functions that run before a write is called. A precommit hook has the ability to cancel a write altogether if the incoming data is considered bad in some way. A simple precommit hook is to check if a value exists at all.</p>

<p>I put my custom Erlang code files under the riak installation <code>./custom/my_validators.erl</code>.</p>

<pre><code class="java">-module(my_validators).
-export([value_exists/1]).

%% Object size must be greater than 0 bytes
value_exists(RiakObject) -&gt;
  Value = riak_object:get_value(RiakObject),
  case erlang:byte_size(Value) of
    0 -&gt; {fail, &quot;A value sized greater than 0 is required&quot;};
    _ -&gt; RiakObject
  end.
</code></pre>

<p>Then compile the file.</p>

<pre><code class="bash">erlc my_validators.erl
</code></pre>

<p>Install the file by informing the Riak installation of your new code with an <code>advanced.config</code> file that lives alongside <code>riak.conf</code> in each node, then rolling restart each node.</p>

<pre><code class="bash">{riak_kv,
  {add_paths, [&quot;./custom&quot;]}
}
</code></pre>

<p>Then you need to do set the Erlang module (<code>my_validators</code>) and function (<code>value_exists</code>) as a JSON value to the bucket&#39;s precommit array <code>{&quot;mod&quot;:&quot;my_validators&quot;,&quot;fun&quot;:&quot;value_exists&quot;}</code>.</p>

<pre><code class="bash">curl -i -XPUT &quot;$RIAK/types/default/buckets/cart/props&quot; \
  -H &quot;Content-Type:application/json&quot; \
  -d &#39;{&quot;props&quot;:{&quot;precommit&quot;:[{&quot;mod&quot;:&quot;my_validators&quot;,&quot;fun&quot;:&quot;value_exists&quot;}]}}&#39;
</code></pre>

<p>If you try and post to the <code>cart</code> bucket without a value, you should expect a failure.</p>

<pre><code class="bash">curl -XPOST &quot;$RIAK/types/default/buckets/cart/keys&quot; \
  -H &quot;Content-Type:application/json&quot;
A value sized greater than 0 is required
</code></pre>

<p>You can also write precommit functions in JavaScript, though Erlang code will execute faster.</p>

<p>Post-commits are similar in form and function, albeit executed after the write has been performed. Key differences:</p>

<ul>
<li>The only language supported is Erlang.</li>
<li>The function&#39;s return value is ignored, thus it cannot cause a failure message to be sent to the client.</li>
</ul>

<h2 id="datatypes">Datatypes</h2>

<p>A new feature in Riak 2.0 are datatypes. Rather than the opaque values of days past, these new additions allow a user to define the type of values that are accepted under a given bucket type. In addition to the benefits listed in the previous chapter of automatic conflict resolution, you also interact with datatypes in a different way.</p>

<p><aside id="crdt" class="sidebar"><h3>CRDT</h3></p>

<p>In the previous chapter I said that Riak datatypes are implemented as CRDTs. The definition of CRDT given was Conflict-free Replicated Data Types. This is only partially correct. In fact, there are two variants of CRDTs, namely, describing how they attempt to keep the replicated datatypes Conflict-free. They are Convergent (CvRDT) and Commutative (CmRDT).</p>

<p>CmRDTs are datatypes that are updated with commutative operations. CvRDTs ensure that disparate states converge to a single value. This distinction is interesting in Riak, because Basho actually implements both. You interface with datatypes by commutative operations (meaning, it doesn&#39;t matter which takes place first), while any underlying divergent states will eventually converge.
</aside></p>

<p>In normal Riak operations, as we&#39;ve seen, you put a value with a given key into a type/bucket object. If you wanted to store a map, say, as a JSON object representing a person, you would put the entire object with every field/value as an operation.</p>

<pre><code class="bash">curl -XPOST &quot;$RIAK/types/json/buckets/people/keys/joe&quot; \
  -H &quot;Content-Type:application/json&quot;
  -d &#39;{&quot;name_register&quot;:&quot;Joe&quot;, &quot;pets_set&quot;:[&quot;cat&quot;]}&#39;
</code></pre>

<p>But if you wanted to add a <code>fish</code> as a pet, you&#39;d have to replace the entire object.</p>

<pre><code class="bash">curl -XPOST &quot;$RIAK/types/json/buckets/people/keys/joe&quot; \
  -H &quot;Content-Type:application/json&quot;
  -d &#39;{&quot;name_register&quot;:&quot;Joe&quot;, &quot;pets_set&quot;:[&quot;cat&quot;, &quot;fish&quot;]}&#39;
</code></pre>

<p>As we saw in the previous chapter, this runs the risk of conflicting, thus creating a sibling.</p>

<pre><code>{&quot;name_register&quot;:&quot;Joe&quot;, &quot;pets_set&quot;:[&quot;cat&quot;]}
{&quot;name_register&quot;:&quot;Joe&quot;, &quot;pets_set&quot;:[&quot;cat&quot;, &quot;fish&quot;]}
</code></pre>

<p>But if we used a map, we&#39;d instead issue only updates to create a map. So, assume that the bucket type <code>map</code> is of a map datatype (we&#39;ll see how operators can assign datatypes to bucket types in the next chapter). This command will insert a map object with two fields (<code>name_register</code> and <code>pets_set</code>).</p>

<pre><code class="bash">curl -XPOST &quot;$RIAK/types/map/buckets/people/keys/joe&quot; \
  -H &quot;Content-Type:application/json&quot;
  -d &#39;{
    &quot;update&quot;: {
      &quot;name_register&quot;: &quot;Joe&quot;
      &quot;pets_set&quot;: {
        &quot;add_all&quot;: &quot;cat&quot;
      }
    }
  }&#39;
</code></pre>

<p>Next, we want to update the <code>pets_set</code> contained within <code>joe</code>&#39;s map. Rather than set Joe&#39;s name and his pet cat, we only need to inform the object of the change. Namely, that we want to add a <code>fish</code> to his <code>pets_set</code>.</p>

<pre><code class="bash">curl -XPOST &quot;$RIAK/types/map/buckets/people/keys/joe&quot; \
  -H &quot;Content-Type:application/json&quot;
  -d &#39;{
    &quot;update&quot;: {
      &quot;pets_set&quot;: {
        &quot;add&quot;: &quot;fish&quot;
      }
    }
  }&#39;
</code></pre>

<p>This has a few benefits. Firstly, we don&#39;t need to send duplicate data. Second, it doesn&#39;t matter what order the two requests happen in, the outcome will be the same. Third, because the operations are CmRDTs, there is no possibility of a datatype returning siblings, making your client code that much easier.</p>

<p>As we&#39;ve noted before, there are four Riak datatypes: <em>map</em>, <em>set</em>, <em>counter</em>, <em>flag</em>. The object type is set as a bucket type property. However, when populating a map, as we&#39;ve seen, you must suffix the field name with the datatype that you wish to store: *_map, *_set, *_counter, *_flag. For plain string values, there&#39;s a special *_register datatype suffix.</p>

<p>You can read more about <a href="http://docs.basho.com/riak/latest/dev/using/data-types">datatypes in the docs</a>.</p>

<h2 id="entropy">Entropy</h2>

<p>Entropy is a byproduct of eventual consistency. In other words: although eventual consistency says a write will replicate to other nodes in time, there can be a bit of delay during which all nodes do not contain the same value.</p>

<p>That difference is <em>entropy</em>, and so Riak has created several <em>anti-entropy</em> strategies (abbreviated as <em>AE</em>). We&#39;ve already talked about how an R/W quorum can deal with differing values when write/read requests overlap at least one node. Riak can repair entropy, or allow you the option to do so yourself.</p>

<p>Riak has two basic strategies to address conflicting writes.</p>

<h3>Last Write Wins</h3>

<p>The most basic, and least reliable, strategy for curing entropy is called <em>last write wins</em>. It&#39;s the simple idea that the last write based on a node&#39;s system clock will overwrite an older one. This is currently the default behavior in Riak (by virtue of the <code>allow_mult</code> property defaulting to <code>false</code>). You can also set the <code>last_write_wins</code> property to <code>true</code>, which improves performance by never retaining vector clock history.</p>

<p>Realistically, this exists for speed and simplicity, when you really don&#39;t care about true order of operations, or the possibility of losing data. Since it&#39;s impossible to keep server clocks truly in sync (without the proverbial geosynchronized atomic clocks), this is a best guess as to what &quot;last&quot; means, to the nearest millisecond.</p>

<h3>Vector Clocks</h3>

<p>As we saw under <a href="#practical-tradeoffs">Concepts</a>, <em>vector clocks</em> are Riak&#39;s way of tracking a true sequence of events of an object. Let&#39;s take a look at using vector clocks to allow for a more sophisticated conflict resolution approach than simply retaining the last-written value.</p>

<h4>Siblings</h4>

<p><em>Siblings</em> occur when you have conflicting values, with no clear way for Riak to know which value is correct. As of Riak 2.0, as long as you use a custom (not <code>default</code>) bucket type that isn&#39;t a datatype, conflicting writes should create siblings. This is a good thing, since it ensures no data is ever lost.</p>

<p>In the case where you forgo a custom bucket type, Riak will try to resolve these conflicts itself if the <code>allow_mult</code> parameter is configured to <code>false</code>. You should generally always have your buckets set to retain siblings, to be resolved by the client by ensuring <code>allow_mult</code> is <code>true</code>.</p>

<pre><code class="bash">curl -i -XPUT &quot;$RIAK/types/default/buckets/cart/props&quot; \
  -H &quot;Content-Type:application/json&quot; \
  -d &#39;{&quot;props&quot;:{&quot;allow_mult&quot;:true}}&#39;
</code></pre>

<p>Siblings arise in a couple cases.</p>

<ol>
<li>A client writes a value using a stale (or missing) vector clock.</li>
<li>Two clients write at the same time with the same vector clock value.</li>
</ol>

<p>We used the second scenario to manufacture a conflict in the previous chapter when we introduced the concept of vector clocks, and we&#39;ll do so again here.</p>

<h4>Creating an Example Conflict</h4>

<p>Imagine we create a shopping cart for a single refrigerator, but several people in a household are able to order food for it. Because losing orders would result in an unhappy household, Riak is using a custom bucket type <code>shopping</code> which keeps the default <code>allow_mult=true</code>.</p>

<p>First Casey (a vegan) places 10 orders of kale in the cart.</p>

<p>Casey writes <code>[{&quot;item&quot;:&quot;kale&quot;,&quot;count&quot;:10}]</code>.</p>

<pre><code class="bash">curl -i -XPUT &quot;$RIAK/types/shopping/buckets/fridge/keys/97207?returnbody=true&quot; \
  -H &quot;Content-Type:application/json&quot; \
  -d &#39;[{&quot;item&quot;:&quot;kale&quot;,&quot;count&quot;:10}]&#39;
HTTP/1.1 200 OK
X-Riak-Vclock: a85hYGBgzGDKBVIcypz/fgaUHjmTwZTImMfKsMKK7RRfFgA=
Vary: Accept-Encoding
Server: MochiWeb/1.1 WebMachine/1.9.0 (someone had painted...
Last-Modified: Thu, 01 Nov 2012 00:13:28 GMT
ETag: &quot;2IGTrV8g1NXEfkPZ45WfAP&quot;
Date: Thu, 01 Nov 2012 00:13:28 GMT
Content-Type: application/json
Content-Length: 28

[{&quot;item&quot;:&quot;kale&quot;,&quot;count&quot;:10}]
</code></pre>

<p>Note the opaque vector clock (via the <code>X-Riak-Vclock</code> header) returned by Riak. That same value will be returned with any read request issued for that key until another write occurs.</p>

<p>His roommate Mark, reads the order and adds milk. In order to allow Riak to track the update history properly, Mark includes the most recent vector clock with his PUT.</p>

<p>Mark writes <code>[{&quot;item&quot;:&quot;kale&quot;,&quot;count&quot;:10},{&quot;item&quot;:&quot;milk&quot;,&quot;count&quot;:1}]</code>.</p>

<pre><code class="bash">curl -i -XPUT &quot;$RIAK/types/shopping/buckets/fridge/keys/97207?returnbody=true&quot; \
  -H &quot;Content-Type:application/json&quot; \
  -H &quot;X-Riak-Vclock:a85hYGBgzGDKBVIcypz/fgaUHjmTwZTImMfKsMKK7RRfFgA=&quot;&quot; \
  -d &#39;[{&quot;item&quot;:&quot;kale&quot;,&quot;count&quot;:10},{&quot;item&quot;:&quot;milk&quot;,&quot;count&quot;:1}]&#39;
HTTP/1.1 200 OK
X-Riak-Vclock: a85hYGBgzGDKBVIcypz/fgaUHjmTwZTIlMfKcMaK7RRfFgA=
Vary: Accept-Encoding
Server: MochiWeb/1.1 WebMachine/1.9.0 (someone had painted...
Last-Modified: Thu, 01 Nov 2012 00:14:04 GMT
ETag: &quot;62NRijQH3mRYPRybFneZaY&quot;
Date: Thu, 01 Nov 2012 00:14:04 GMT
Content-Type: application/json
Content-Length: 54

[{&quot;item&quot;:&quot;kale&quot;,&quot;count&quot;:10},{&quot;item&quot;:&quot;milk&quot;,&quot;count&quot;:1}]
</code></pre>

<p>If you look closely, you&#39;ll notice that the vector clock changed with the second write request</p>

<ul>
<li>a85hYGBgzGDKBVIcypz/fgaUHjmTwZTI<strong>mMfKsMK</strong>K7RRfFgA= (after the write by Casey)</li>
<li>a85hYGBgzGDKBVIcypz/fgaUHjmTwZTI<strong>lMfKcMa</strong>K7RRfFgA= (after the write by Mark)</li>
</ul>

<p>Now let&#39;s consider a third roommate, Andy, who loves almonds. Before Mark updates the shared cart with milk, Andy retrieved Casey&#39;s kale order and appends almonds. As with Mark, Andy&#39;s update includes the vector clock as it existed after Casey&#39;s original write.</p>

<p>Andy writes <code>[{&quot;item&quot;:&quot;kale&quot;,&quot;count&quot;:10},{&quot;item&quot;:&quot;almonds&quot;,&quot;count&quot;:12}]</code>.</p>

<pre><code class="bash">curl -i -XPUT &quot;$RIAK/types/shopping/buckets/fridge/keys/97207?returnbody=true&quot; \
  -H &quot;Content-Type:application/json&quot; \
  -H &quot;X-Riak-Vclock:a85hYGBgzGDKBVIcypz/fgaUHjmTwZTImMfKsMKK7RRfFgA=&quot;&quot; \
  -d &#39;[{&quot;item&quot;:&quot;kale&quot;,&quot;count&quot;:10},{&quot;item&quot;:&quot;almonds&quot;,&quot;count&quot;:12}]&#39;
HTTP/1.1 300 Multiple Choices
X-Riak-Vclock: a85hYGBgzGDKBVIcypz/fgaUHjmTwZTInMfKoG7LdoovCwA=
Vary: Accept-Encoding
Server: MochiWeb/1.1 WebMachine/1.9.0 (someone had painted...
Last-Modified: Thu, 01 Nov 2012 00:24:07 GMT
ETag: &quot;54Nx22W9M7JUKJnLBrRehj&quot;
Date: Thu, 01 Nov 2012 00:24:07 GMT
Content-Type: multipart/mixed; boundary=Ql3O0enxVdaMF3YlXFOdmO5bvrs
Content-Length: 491


--Ql3O0enxVdaMF3YlXFOdmO5bvrs
Content-Type: application/json
Etag: 62NRijQH3mRYPRybFneZaY
Last-Modified: Thu, 01 Nov 2012 00:14:04 GMT

[{&quot;item&quot;:&quot;kale&quot;,&quot;count&quot;:10},{&quot;item&quot;:&quot;milk&quot;,&quot;count&quot;:1}]
--Ql3O0enxVdaMF3YlXFOdmO5bvrs
Content-Type: application/json
Etag: 7kfvPXisoVBfC43IiPKYNb
Last-Modified: Thu, 01 Nov 2012 00:24:07 GMT

[{&quot;item&quot;:&quot;kale&quot;,&quot;count&quot;:10},{&quot;item&quot;:&quot;almonds&quot;,&quot;count&quot;:12}]
--Ql3O0enxVdaMF3YlXFOdmO5bvrs--
</code></pre>

<p>Whoa! What&#39;s all that?</p>

<p>Since there was a conflict between what Mark and Andy set the fridge value to be, Riak kept both values.</p>

<h4>VTag</h4>

<p>Since we&#39;re using the HTTP client, Riak returned a <code>300 Multiple Choices</code> code with a <code>multipart/mixed</code> MIME type. It&#39;s up to you to parse the results (or you can request a specific value by its Etag, also called a Vtag).</p>

<p>Issuing a plain get on the <code>shopping/fridge/97207</code> key will also return the vtags of all siblings.</p>

<pre><code>curl &quot;$RIAK/types/shopping/buckets/fridge/keys/97207&quot;
Siblings:
62NRijQH3mRYPRybFneZaY
7kfvPXisoVBfC43IiPKYNb
</code></pre>

<p>What can you do with this tag? Namely, you request the value of a specific sibling by its <code>vtag</code>. To get the first sibling in the list (Mark&#39;s milk):</p>

<pre><code class="bash">curl &quot;$RIAK/types/shopping/buckets/fridge/keys/97207?vtag=62NRijQH3mRYPRybFneZaY&quot;
[{&quot;item&quot;:&quot;kale&quot;,&quot;count&quot;:10},{&quot;item&quot;:&quot;milk&quot;,&quot;count&quot;:1}]
</code></pre>

<p>If you want to retrieve all sibling data, tell Riak that you&#39;ll accept the multipart message by adding <code>-H &quot;Accept:multipart/mixed&quot;</code>.</p>

<pre><code class="bash">curl &quot;$RIAK/types/shopping/buckets/fridge/keys/97207&quot; \
  -H &quot;Accept:multipart/mixed&quot;
</code></pre>

<p><aside class="sidebar"><h3>Use-Case Specific?</h3></p>

<p>When siblings are created, it&#39;s up to the application to know how to deal
with the conflict. In our example, do we want to accept only one of the
orders? Should we remove both milk and almonds and only keep the kale?
Should we calculate the cheaper of the two and keep the cheapest option?
Should we merge all of the results into a single order? This is why we asked
Riak not to resolve this conflict automatically... we want this flexibility.
</aside></p>

<h4>Resolving Conflicts</h4>

<p>When we have conflicting writes, we want to resolve them. Since that problem is typically <em>use-case specific</em>, Riak defers it to us, and our application must decide how to proceed.</p>

<p>For our example, let&#39;s merge the values into a single result set, taking the larger <em>count</em> if the <em>item</em> is the same. When done, write the new results back to Riak with the vclock of the multipart object, so Riak knows you&#39;re resolving the conflict, and you&#39;ll get back a new vector clock.</p>

<p>Successive reads will receive a single (merged) result.</p>

<pre><code class="bash">curl -i -XPUT &quot;$RIAK/types/shopping/buckets/fridge/keys/97207?returnbody=true&quot; \
  -H &quot;Content-Type:application/json&quot; \
  -H &quot;X-Riak-Vclock:a85hYGBgzGDKBVIcypz/fgaUHjmTwZTInMfKoG7LdoovCwA=&quot; \
  -d &#39;[{&quot;item&quot;:&quot;kale&quot;,&quot;count&quot;:10},{&quot;item&quot;:&quot;milk&quot;,&quot;count&quot;:1},\
      {&quot;item&quot;:&quot;almonds&quot;,&quot;count&quot;:12}]&#39;
</code></pre>

<h3>Last write wins vs. siblings</h3>

<p>Your data and your business needs will dictate which approach to conflict resolution is appropriate. You don&#39;t need to choose one strategy globally; instead, feel free to take advantage of Riak&#39;s buckets to specify which data uses siblings and which blindly retains the last value written.</p>

<p>A quick recap of the two configuration values you&#39;ll want to set:</p>

<ul>
<li><code>allow_mult</code> defaults to <code>false</code>, which means that the last write wins.</li>
<li>Setting <code>allow_mult</code> to <code>true</code> instructs Riak to retain conflicting writes as siblings.</li>
<li><code>last_write_wins</code> defaults to <code>false</code>, which (perhaps counter-intuitively) still can mean that the behavior is last write wins: <code>allow_mult</code> is the key parameter for the behavioral toggle.</li>
<li>Setting <code>last_write_wins</code> to true will optimize writes by assuming that previous vector clocks have no inherent value.</li>
<li>Setting both <code>allow_mult</code> and <code>last_write_wins</code> to <code>true</code> is unsupported and will result in undefined behavior.</li>
</ul>

<h3>Read Repair</h3>

<p>When a successful read happens, but not all replicas agree upon the value, this triggers a <em>read repair</em>. This means that Riak will update the replicas with the most recent value. This can happen either when an object is not found (the vnode has no copy) or a vnode contains an older value (older means that it is an ancestor of the newest vector clock). Unlike <code>last_write_wins</code> or manual conflict resolution, read repair is (obviously, I hope, by the name) triggered by a read, rather than a write.</p>

<p>If your nodes get out of sync (for example, if you increase the <code>n_val</code> on a bucket), you can force read repair by performing a read operation for all of that bucket&#39;s keys. They may return with <code>not found</code> the first time, but later reads will pull the newest values.</p>

<h3>Active Anti-Entropy (AAE)</h3>

<p>Although resolving conflicting data during get requests via read repair is sufficient for most needs, data which is never read can eventually be lost as nodes fail and are replaced.</p>

<p>Riak supports active anti-entropy (AAE), to proactively identify and repair inconsistent data. This feature is also helpful for recovering data loss in the event of disk corruption or administrative error.</p>

<p>The overhead for this functionality is minimized by maintaining sophisticated hash trees (&quot;Merkle trees&quot;) which make it easy to compare data sets between vnodes, but if desired the feature can be disabled.</p>

<h2 id="querying">Querying</h2>

<p>So far we&#39;ve only dealt with key-value lookups. The truth is, key-value is a pretty powerful mechanism that spans a spectrum of use-cases. However, sometimes we need to lookup data by value, rather than key. Sometimes we need to perform some calculations, or aggregations, or search.</p>

<h3>Secondary Indexing (2i)</h3>

<p>A <em>secondary index</em> (2i) is a data structure that lowers the cost of
finding non-key values. Like many other databases, Riak has the
ability to index data. However, since Riak has no real knowledge of
the data it stores (they&#39;re just binary values), it uses metadata to
index defined by a name pattern to be either integers or binary values.</p>

<p>If your installation is configured to use 2i (shown in the next chapter),
simply writing a value to Riak with the header will be indexes,
provided it&#39;s prefixed by <code>X-Riak-Index-</code> and suffixed by <code>_int</code> for an
integer, or <code>_bin</code> for a string.</p>

<pre><code class="bash">curl -i -XPUT $RIAK/types/shopping/buckets/people/keys/casey \
  -H &quot;Content-Type:application/json&quot; \
  -H &quot;X-Riak-Index-age_int:31&quot; \
  -H &quot;X-Riak-Index-fridge_bin:97207&quot; \
  -d &#39;{&quot;work&quot;:&quot;rodeo clown&quot;}&#39;
</code></pre>

<p>Querying can be done in two forms: exact match and range. Add a couple more people and we&#39;ll see what we get: <code>mark</code> is <code>32</code>, and <code>andy</code> is <code>35</code>, they both share <code>97207</code>.</p>

<p>What people own <code>97207</code>? It&#39;s a quick lookup to receive the
keys that have matching index values.</p>

<pre><code class="bash">curl &quot;$RIAK/types/shopping/buckets/people/index/fridge_bin/97207&quot;
{&quot;keys&quot;:[&quot;mark&quot;,&quot;casey&quot;,&quot;andy&quot;]}
</code></pre>

<p>With those keys it&#39;s a simple lookup to get the bodies.</p>

<p>The other query option is an inclusive ranged match. This finds all
people under the ages of <code>32</code>, by searching between <code>0</code> and <code>32</code>.</p>

<pre><code class="bash">curl &quot;$RIAK/types/shopping/buckets/people/index/age_int/0/32&quot;
{&quot;keys&quot;:[&quot;mark&quot;,&quot;casey&quot;]}
</code></pre>

<p>That&#39;s about it. It&#39;s a basic form of 2i, with a decent array of utility.</p>

<h3>MapReduce</h3>

<p>MapReduce is a method of aggregating large amounts of data by separating the
processing into two phases, map and reduce, that themselves are executed
in parts. Map will be executed per object to convert/extract some value,
then those mapped values will be reduced into some aggregate result. What
do we gain from this structure? It&#39;s predicated on the idea that it&#39;s cheaper
to move the algorithms to where the data lives, than to transfer massive
amounts of data to a single server to run a calculation.</p>

<p>This method, popularized by Google, can be seen in a wide array of NoSQL
databases. In Riak, you execute a MapReduce job on a single node, which
then propagates to the other nodes. The results are mapped and reduced,
then further reduced down to the calling node and returned.</p>

<p><img src="../assets/mapreduce.svg" alt="MapReduce Returning Name Char Count"></p>

<p>Let&#39;s assume we have a bucket for log values that stores messages
prefixed by either INFO or ERROR. We want to count the number of INFO
logs that contain the word &quot;cart&quot;.</p>

<pre><code class="bash">LOGS=$RIAK/types/default/buckets/logs/keys
curl -XPOST $LOGS -d &quot;INFO: New user added&quot;
curl -XPOST $LOGS -d &quot;INFO: Kale added to shopping cart&quot;
curl -XPOST $LOGS -d &quot;INFO: Milk added to shopping cart&quot;
curl -XPOST $LOGS -d &quot;ERROR: shopping cart cancelled&quot;
</code></pre>

<p>MapReduce jobs can be either Erlang or JavaScript code. This time we&#39;ll go the
easy route and write JavaScript. You execute MapReduce by posting JSON to the
<code>/mapred</code> path.</p>

<pre><code class="bash">curl -XPOST &quot;$RIAK/mapred&quot; \
  -H &quot;Content-Type: application/json&quot; \
  -d @- \
&lt;&lt;EOF
{
  &quot;inputs&quot;:&quot;logs&quot;,
  &quot;query&quot;:[{
    &quot;map&quot;:{
      &quot;language&quot;:&quot;javascript&quot;,
      &quot;source&quot;:&quot;function(riakObject, keydata, arg) {
        var m = riakObject.values[0].data.match(/^INFO.*cart/);
        return [(m ? m.length : 0 )];
      }&quot;
    },
    &quot;reduce&quot;:{
      &quot;language&quot;:&quot;javascript&quot;,
      &quot;source&quot;:&quot;function(values, arg){
        return [values.reduce(
          function(total, v){ return total + v; }, 0)
        ];
      }&quot;
    }
  }]
}
EOF
</code></pre>

<p>The result should be <code>[2]</code>, as expected. Both map and reduce phases should
always return an array. The map phase receives a single riak object, while
the reduce phase received an array of values, either the result of multiple
map function outputs, or of multiple reduce outputs. I probably cheated a
bit by using JavaScript&#39;s <code>reduce</code> function to sum the values, but, well,
welcome to the world of thinking in terms of MapReduce!</p>

<h4>MR + 2i</h4>

<p>Another option when using MapReduce is to combine it with secondary indexes.
You can pipe the results of a 2i query into a MapReducer, simply specify the
index you wish to use, and either a <code>key</code> for an index lookup, or <code>start</code> and
<code>end</code> values for a ranged query.</p>

<pre><code class="json">    ...
    &quot;inputs&quot;:{
       &quot;bucket&quot;:&quot;people&quot;,
       &quot;index&quot;: &quot;age_int&quot;,
       &quot;start&quot;: 18,
       &quot;end&quot;:   32
    },
    ...
</code></pre>

<p>MapReduce in Riak is a powerful way of pulling data out of an
otherwise straight key/value store. But we have one more method of finding
data in Riak.</p>

<p><aside class="sidebar"><h3>Whatever Happened to Riak Search 1.x?</h3></p>

<p>If you&#39;ve used Riak before, or have some older documentation,
you may wonder what the difference is between Riak Search 1.0 and 2.0.</p>

<p>In an attempt to make Riak Search user friendly, it was originally developed
with a &quot;Solr like&quot; interface. Sadly, due to the complexity of building
distributed search engines, it was woefully incomplete. Basho decided that,
rather than attempting to maintain parity with Solr, a popular and featureful
search engine in its own right, it made more sense to integrate the two.
</aside></p>

<h3>Search 2.0</h3>

<p>Search 2.0 is a complete, from scratch, reimagining of distributed search 
in Riak. It&#39;s an extension to Riak that lets you perform searches to find
values in a Riak cluster. Unlike the original Riak Search, Search 2.0
leverages distributed Solr to perform the inverted indexing and management of
retrieving matching values.</p>

<p>Before using Search 2.0, you&#39;ll have to have it installed and a bucket set
up with an index (these details can be found in the next chapter).</p>

<p>The simplest example is a full-text search. Here we add <code>ryan</code> to the
<code>people</code> table (with a default index).</p>

<pre><code class="bash">curl -XPUT &quot;$RIAK/type/default/buckets/people/keys/ryan&quot; \
  -H &quot;Content-Type:text/plain&quot; \
  -d &quot;Ryan Zezeski&quot;
</code></pre>

<p>To execute a search, request <code>/solr/&lt;index&gt;/select</code> along with any distributed
<a href="http://wiki.apache.org/solr/CommonQueryParameters">Solr parameters</a>. Here we
query for documents that contain a word starting with <code>zez</code>, request the
results to be in json format (<code>wt=json</code>), only return the Riak key
(<code>fl=_yz_rk</code>).</p>

<pre><code class="bash">curl &quot;$RIAK/solr/people/select?wt=json&amp;omitHeader=true&amp;fl=_yz_rk&amp;q=zez*&quot;
{
  &quot;response&quot;: {
    &quot;numFound&quot;: 1,
    &quot;start&quot;: 0,
    &quot;maxScore&quot;: 1.0,
    &quot;docs&quot;: [
      {
        &quot;_yz_rk&quot;: &quot;ryan&quot;
      }
    ]
  }
}
</code></pre>

<p>With the matching <code>_yz_rk</code> keys, you can retrieve the bodies with a simple
Riak lookup.</p>

<p>Search 2.0 supports Solr 4.0, which includes filter queries, ranges, page scores,
start values and rows (the last two are useful for pagination). You can also
receive snippets of matching
<a href="http://wiki.apache.org/solr/HighlightingParameters">highlighted text</a>
(<code>hl</code>,<code>hl.fl</code>), which is useful for building a search engine (and something
we use for <a href="http://search.basho.com">search.basho.com</a>). You can perform
facet searches, stats, geolocation, bounding shapes, or any other search
possible with distributed Solr.</p>

<h4>Tagging</h4>

<p>Another useful feature of Search 2.0 is the tagging of values. Tagging
values give additional context to a Riak value. The current implementation
requires all tagged values begin with <code>X-Riak-Meta</code>, and be listed under
a special header named <code>X-Riak-Meta-yz-tags</code>.</p>

<pre><code class="bash">curl -XPUT &quot;$RIAK/types/default/buckets/people/keys/dave&quot; \
  -H &quot;Content-Type:text/plain&quot; \
  -H &quot;X-Riak-Meta-yz-tags: X-Riak-Meta-nickname_s&quot; \
  -H &quot;X-Riak-Meta-nickname_s:dizzy&quot; \
  -d &quot;Dave Smith&quot;
</code></pre>

<p>To search by the <code>nickname_s</code> tag, just prefix the query string followed
by a colon.</p>

<pre><code class="bash">curl &quot;$RIAK/solr/people/select?wt=json&amp;omitHeader=true&amp;q=nickname_s:dizzy&quot;
{
  &quot;response&quot;: {
    &quot;numFound&quot;: 1,
    &quot;start&quot;: 0,
    &quot;maxScore&quot;: 1.4054651,
    &quot;docs&quot;: [
      {
        &quot;nickname_s&quot;: &quot;dizzy&quot;,
        &quot;id&quot;: &quot;dave_25&quot;,
        &quot;_yz_ed&quot;: &quot;20121102T215100 dave m7psMIomLMu/+dtWx51Kluvvrb8=&quot;,
        &quot;_yz_fpn&quot;: &quot;23&quot;,
        &quot;_yz_node&quot;: &quot;dev1@127.0.0.1&quot;,
        &quot;_yz_pn&quot;: &quot;25&quot;,
        &quot;_yz_rk&quot;: &quot;dave&quot;,
        &quot;_version_&quot;: 1417562617478643712
      }
    ]
  }
}
</code></pre>

<p>Notice that the <code>docs</code> returned also contain <code>&quot;nickname_s&quot;:&quot;dizzy&quot;</code> as a
value. All tagged values will be returned on matching results.</p>

<h4>Datatypes</h4>

<p>One of the more powerful combinations in Riak 2.0 are datatypes and Search.
If you set both a datatype and a search index in a bucket type&#39;s properties,
values you set are indexed as you&#39;d expect. Map fields are indexed as their
given types, sets are multi-field strings, counters as indexed as integers,
and flags are boolean. Nested maps are also indexed, seperated by dots, and
queryable in such a manner.</p>

<p>For example, remember Joe, from the datatype section? Let&#39;s assume that
this <code>people</code> bucket is indexed. And let&#39;s also add another pet.</p>

<pre><code class="bash">curl -XPUT &quot;$RIAK/types/map/buckets/people/keys/joe&quot; \
  -H &quot;Content-Type:application/json&quot;
  -d &#39;{&quot;update&quot;: {&quot;pets_set&quot;: {&quot;add&quot;:&quot;dog&quot;}}}&#39;
</code></pre>

<p>Then let&#39;s search for <code>pets_set:dog</code>, filtering only <code>type/bucket/key</code>.</p>

<pre><code class="bash">{
  &quot;response&quot;: {
    &quot;numFound&quot;: 1,
    &quot;start&quot;: 0,
    &quot;maxScore&quot;: 1.0,
    &quot;docs&quot;: [
      {
        &quot;_yz_rt&quot;: &quot;map&quot;
        &quot;_yz_rb&quot;: &quot;people&quot;
        &quot;_yz_rk&quot;: &quot;joe&quot;
      }
    ]
  }
}
</code></pre>

<p>Bravo. You&#39;ve now found the object you wanted. Thanks to Solr&#39;s customizable
schema, you can even store the field you want to return, if it&#39;s really that
important to save a second lookup.</p>

<p>This provides the best of both worlds. You can update and query values without
fear of conflicts, and can query Riak based on field values. It doesn&#39;t require
much imagination to see that this combination effectively turns Riak into
a scalable, stable, highly available, document datastore. Throw strong consistency
into the mix (which we&#39;ll do in the next chapter) and you can store and query
pretty much anything in Riak, in any way.</p>

<p>If you&#39;re wondering to yourself, &quot;What exactly does Mongo provide, again?&quot;, well,
I didn&#39;t ask it. You did. But that is a great question...</p>

<p>Well, moving on.</p>

<h2 id="wrap-up">Wrap-up</h2>

<p>Riak is a distributed data store with several additions to improve upon the
standard key-value lookups, like specifying replication values. Since values
in Riak are opaque, many of these methods either require custom code to
extract and give meaning to values, such as <em>MapReduce*m or allow for
header metadata to provide an added descriptive dimension to the object,
such as *secondary indexes</em> or <em>search</em>.</p>

<p>Next, we&#39;ll peek further under the hood and show you how to set up and manage
a cluster of your own.</p>
<h1 id="operators">Operators</h1>

<!-- What Riak is famous for is its simplicity to operate and stability at increasing scales. -->

<p>In some ways, Riak is downright mundane in its role as the easiest
NoSQL database to operate. Want more servers? Add them. A network
cable is cut at 2am? Deal with it after a few more hours of
sleep. Understanding this integral part of your application stack is
still important, however, despite Riak&#39;s reliability.</p>

<p>We&#39;ve covered the core concepts of Riak, and I&#39;ve provided a taste of
how to use it, but there is more to the database than that. There are
details you should know if you plan on operating a Riak cluster of
your own.</p>

<h2 id="clusters">Clusters</h2>

<p>Up to this point you&#39;ve conceptually read about &quot;clusters&quot; and the &quot;Ring&quot; in
nebulous summations. What exactly do we mean, and what are the practical
implications of these details for Riak developers and operators?</p>

<p>A <em>cluster</em> in Riak is a managed collection of nodes that share a common Ring.</p>

<h3>The Ring</h3>

<p><em>The Ring</em> in Riak is actually a two-fold concept.</p>

<p>Firstly, the Ring represents the consistent hash partitions (the partitions
managed by vnodes). This partition range is treated as circular, from 0 to
2^160-1 back to 0 again. (If you&#39;re wondering, yes this means that we are
limited to 2^160 nodes, which is a limit of a 1.46 quindecillion, or
<code>1.46 x 10^48</code>, node cluster. For comparison, there are only <code>1.92 x 10^49</code>
<a href="http://education.jlab.org/qa/mathatom_05.html">silicon atoms on Earth</a>.)</p>

<p>When we consider replication, the N value defines how many nodes an object is
replicated to. Riak makes a best attempt at spreading that value to as many
nodes as it can, so it copies to the next N adjacent nodes, starting with the
primary partition and counting around the Ring, if it reaches the last
partition, it loops around back to the first one.</p>

<p>Secondly, the Ring is also used as a shorthand for describing the state of the
circular hash ring I just mentioned. This Ring (aka <em>Ring State</em>) is a
data structure that gets passed around between nodes, so each knows the state
of the entire cluster. Which node manages which vnodes? If a node gets a
request for an object managed by other nodes, it consults the Ring and forwards
the request to the proper nodes. It&#39;s a local copy of a contract that all of
the nodes agree to follow.</p>

<p>Obviously, this contract needs to stay in sync between all of the nodes. If a node is permanently taken
offline or a new one added, the other nodes need to readjust, balancing the partitions around the cluster,
then updating the Ring with this new structure. This Ring state gets passed between the nodes by means of
a <em>gossip protocol</em>.</p>

<h3>Gossip and CMD</h3>

<p>Riak has two methods of keeping nodes current on the state of the Ring. The first, and oldest, is the <em>gossip protocol</em>. If a node&#39;s state in the cluster is altered, information is propagated to other nodes. Periodically, nodes will also send their status to a random peer for added consistency.</p>

<p>A newer method of information exchange in Riak is <em>cluster metadata</em> (CMD), which uses a more sophisticated method (plum-tree, DVV consistent state) to pass large amounts of metadata between nodes. The superiority of CMD is one of the benefits of using bucket types in Riak 2.0, discussed below.</p>

<p>In both cases, propagating changes in Ring is an asynchronous operation, and can take a couple minutes depending on Ring size.</p>

<!-- Transfers will not start while a gossip is in progress. -->

<h3>How Replication Uses the Ring</h3>

<p>Even if you are not a programmer, it&#39;s worth taking a look at this Ring example. It&#39;s also worth
remembering that partitions are managed by vnodes, and in conversation are sometimes interchanged,
though I&#39;ll try to be more precise here.</p>

<p>Let&#39;s start with Riak configured to have 8 partitions, which are set via <code>ring_creation_size</code>
in the <code>etc/riak.conf</code> file (we&#39;ll dig deeper into this file later).</p>

<pre><code class="bash">## Number of partitions in the cluster (only valid when first
## creating the cluster). Must be a power of 2, minimum 8 and maximum
## 1024.
## 
## Default: 64
## 
## Acceptable values:
##   - an integer
ring_size = 8
</code></pre>

<p>In this example, I have a total of 4 Riak nodes running on <code>riak@AAA.cluster</code>,
<code>riak@BBB.cluster</code>, <code>riak@CCC.cluster</code>, and <code>riak@DDD.cluster</code>, each with two partitions (and thus vnodes)</p>

<p>Riak has the amazing, and dangerous, <code>attach</code> command that attaches an Erlang console to a live Riak
node, with access to all of the Riak modules.</p>

<p>The <code>riak_core_ring:chash(Ring)</code> function extracts the total count of partitions (8), with an array
of numbers representing the start of the partition, some fraction of the 2^160 number, and the node
name that represents a particular Riak server in the cluster.</p>

<pre><code class="bash">$ bin/riak attach
(riak@AAA.cluster)1&gt; {ok,Ring} = riak_core_ring_manager:get_my_ring().
(riak@AAA.cluster)2&gt; riak_core_ring:chash(Ring).
{8,
 [{0,&#39;riak@AAA.cluster&#39;},
  {182687704666362864775460604089535377456991567872, &#39;riak@BBB.cluster&#39;},
  {365375409332725729550921208179070754913983135744, &#39;riak@CCC.cluster&#39;},
  {548063113999088594326381812268606132370974703616, &#39;riak@DDD.cluster&#39;},
  {730750818665451459101842416358141509827966271488, &#39;riak@AAA.cluster&#39;},
  {913438523331814323877303020447676887284957839360, &#39;riak@BBB.cluster&#39;},
  {1096126227998177188652763624537212264741949407232, &#39;riak@CCC.cluster&#39;},
  {1278813932664540053428224228626747642198940975104, &#39;riak@DDD.cluster&#39;}]}
</code></pre>

<p>To discover which partition the bucket/key <code>food/favorite</code> object would be stored in, for example,
we execute <code>riak_core_util:chash_key( {&lt;&lt;&quot;food&quot;&gt;&gt;, &lt;&lt;&quot;favorite&quot;&gt;&gt;} )</code> and get a wacky 160 bit Erlang
number we named <code>DocIdx</code> (document index).</p>

<p>Just to illustrate that Erlang binary value is a real number, the next line makes it a more
readable format, similar to the ring partition numbers.</p>

<pre><code class="bash">(riak@AAA.cluster)3&gt; DocIdx = 
(riak@AAA.cluster)3&gt; riak_core_util:chash_key({&lt;&lt;&quot;food&quot;&gt;&gt;,&lt;&lt;&quot;favorite&quot;&gt;&gt;}).
&lt;&lt;80,250,1,193,88,87,95,235,103,144,152,2,21,102,201,9,156,102,128,3&gt;&gt;

(riak@AAA.cluster)4&gt; &lt;&lt;I:160/integer&gt;&gt; = DocIdx. I.
462294600869748304160752958594990128818752487427
</code></pre>

<p>With this <code>DocIdx</code> number, we can order the partitions, starting with first number greater than
<code>DocIdx</code>. The remaining partitions are in numerical order, until we reach zero, then
we loop around and continue to exhaust the list.</p>

<pre><code class="bash">(riak@AAA.cluster)5&gt; Preflist = riak_core_ring:preflist(DocIdx, Ring).
[{548063113999088594326381812268606132370974703616, &#39;riak@DDD.cluster&#39;},
 {730750818665451459101842416358141509827966271488, &#39;riak@AAA.cluster&#39;},
 {913438523331814323877303020447676887284957839360, &#39;riak@BBB.cluster&#39;},
 {1096126227998177188652763624537212264741949407232, &#39;riak@CCC.cluster&#39;},
 {1278813932664540053428224228626747642198940975104, &#39;riak@DDD.cluster&#39;},
 {0,&#39;riak@AAA.cluster&#39;},
 {182687704666362864775460604089535377456991567872, &#39;riak@BBB.cluster&#39;},
 {365375409332725729550921208179070754913983135744, &#39;riak@CCC.cluster&#39;}]
</code></pre>

<p>So what does all this have to do with replication? With the above list, we simply replicate a write
down the list N times. If we set N=3, then the <code>food/favorite</code> object will be written to
the <code>riak@DDD.cluster</code> node&#39;s partition <code>5480631...</code> (I truncated the number here),
<code>riak@AAA.cluster</code> partition <code>7307508...</code>, and <code>riak@BBB.cluster</code> partition <code>9134385...</code>.</p>

<p>If something has happened to one of those nodes, like a network split
(confusingly also called a partition---the &quot;P&quot; in &quot;CAP&quot;), the remaining
active nodes in the list become candidates to hold the data.</p>

<p>So if the node coordinating the write could not reach node
<code>riak@AAA.cluster</code> to write to partition <code>7307508...</code>, it would then attempt
to write that partition <code>7307508...</code> to <code>riak@CCC.cluster</code> as a fallback
(it&#39;s the next node in the list preflist after the 3 primaries).</p>

<p>The way that the Ring is structured allows Riak to ensure data is always
written to the appropriate number of physical nodes, even in cases where one
or more physical nodes are unavailable. It does this by simply trying the next
available node in the preflist.</p>

<h3>Hinted Handoff</h3>

<p>When a node goes down, data is replicated to a backup node. This is
not permanent; Riak will periodically examine whether each vnode
resides on the correct physical node and hands them off to the proper
node when possible.</p>

<p>As long as the temporary node cannot connect to the primary, it will continue
to accept write and read requests on behalf of its incapacitated brethren.</p>

<p>Hinted handoff not only helps Riak achieve high availability, it also facilitates
data migration when physical nodes are added or removed from the Ring.</p>

<h2 id="managing-a-cluster">Managing a Cluster</h2>

<p>Now that we have a grasp of the general concepts of Riak, how users query it,
and how Riak manages replication, it&#39;s time to build a cluster. It&#39;s so easy to
do, in fact, I didn&#39;t bother discussing it for most of this book.</p>

<h3>Install</h3>

<p>The Riak docs have all of the information you need to <a href="http://docs.basho.com/riak/latest/tutorials/installation/">install</a> it per operating system. The general sequence is:</p>

<ol>
<li>Install Erlang</li>
<li>Get Riak from a package manager (<em>a la</em> <code>apt-get</code> or Homebrew), or build from source (the results end up under <code>rel/riak</code>, with the binaries under <code>bin</code>).</li>
<li>Run <code>riak start</code></li>
</ol>

<p>Install Riak on four or five nodes---five being the recommended safe minimum for production. Fewer nodes are OK during software development and testing.</p>

<h3>Command Line</h3>

<p>Most Riak operations can be performed through the command line. We&#39;ll concern ourselves with two commands: <code>riak</code> and <code>riak-admin</code>.</p>

<h4>riak</h4>

<p>Simply typing the <code>riak</code> command will give a usage list. If you want more information, you can try <code>riak help</code>.</p>

<pre><code class="bash">Usage: riak &lt;command&gt;
where &lt;command&gt; is one of the following:
    { help | start | stop | restart | ping | console | attach
      attach-direct | ertspath | chkconfig | escript | version | getpid
      top [-interval N] [-sort { reductions | memory | msg_q }] [-lines N] } |
      config { generate | effective | describe VARIABLE } [-l debug]

Run &#39;riak help&#39; for more detailed information.
</code></pre>

<p>Most of these commands are self explanatory, once you know what they mean. <code>start</code> and <code>stop</code> are simple enough. <code>restart</code> means to stop the running node and restart it inside of the same Erlang VM (virtual machine), while <code>reboot</code> will take down the Erlang VM and restart everything.</p>

<p>You can print the current running <code>version</code>. <code>ping</code> will return <code>pong</code> if the server is in good shape, otherwise you&#39;ll get the <em>just-similar-enough-to-be-annoying</em> response <code>pang</code> (with an <em>a</em>), or a simple <code>Node X not responding to pings</code> if it&#39;s not running at all.</p>

<p><code>chkconfig</code> is useful if you want to ensure your <code>etc/riak.conf</code> is not broken
(that is to say, it&#39;s parsable). I mentioned <code>attach</code> briefly above, when
we looked into the details of the Ring---it attaches a console to the local
running Riak server so you can execute Riak&#39;s Erlang code. <code>escript</code> is similar
to <code>attach</code>, except you pass in script file of commands you wish to run automatically.</p>

<!--
If you want to build this on a single dev machine, here is a truncated guide.
Download the Riak source code, then run the following:
make deps
make devrel
for i in {1..5}; do dev/dev$i/bin/riak start; done
for i in {1..5}; do dev/dev$i/bin/riak ping; done
for i in {2..5}; do dev/dev$i/bin/riak-admin cluster join riak@AAA.cluster; done
dev/dev1/bin/riak-admin cluster plan
dev/dev1/bin/riak-admin cluster commit
You should now have a 5 node cluster running locally.
-->

<h4>riak-admin</h4>

<p>The <code>riak-admin</code> command is the meat operations, the tool you&#39;ll use most often. This is where you&#39;ll join nodes to the Ring, diagnose issues, check status, and trigger backups.</p>

<pre><code class="bash">Usage: riak-admin { cluster | join | leave | backup | restore | test |
                    reip | js-reload | erl-reload | wait-for-service |
                    ringready | transfers | force-remove | down |
                    cluster-info | member-status | ring-status | vnode-status |
                    aae-status | diag | status | transfer-limit | reformat-indexes |
                    top [-interval N] [-sort reductions|memory|msg_q] [-lines N] |
                    downgrade-objects | security | bucket-type | repair-2i |
                    search | services | ensemble-status }
</code></pre>

<p>For more information on commands, you can try <code>man riak-admin</code>.</p>

<p>A few of these commands are deprecated, and many don&#39;t make sense without a
cluster, but some we can look at now.</p>

<p><code>status</code> outputs a list of information about this cluster. It&#39;s mostly the same information you can get from getting <code>/stats</code> via HTTP, although the coverage of information is not exact (for example, riak-admin status returns <code>disk</code>, and <code>/stats</code> returns some computed values like <code>gossip_received</code>).</p>

<pre><code class="bash">$ riak-admin status
1-minute stats for &#39;riak@AAA.cluster&#39;
-------------------------------------------
vnode_gets : 0
vnode_gets_total : 2
vnode_puts : 0
vnode_puts_total : 1
vnode_index_reads : 0
vnode_index_reads_total : 0
vnode_index_writes : 0
vnode_index_writes_total : 0
vnode_index_writes_postings : 0
vnode_index_writes_postings_total : 0
vnode_index_deletes : 0
...
</code></pre>

<p>New JavaScript or Erlang files (as we did in the <a href="#developers">developers</a> chapter) are not usable by the nodes until they are informed about them by the <code>js-reload</code> or <code>erl-reload</code> command.</p>

<p><code>riak-admin</code> also provides a little <code>test</code> command, so you can perform a read/write cycle
to a node, which I find useful for testing a client&#39;s ability to connect, and the node&#39;s
ability to write.</p>

<p>Finally, <code>top</code> is an analysis command checking the Erlang details of a particular node in
real time. Different processes have different process ids (Pids), use varying amounts of memory,
queue up so many messages at a time (MsgQ), and so on. This is useful for advanced diagnostics,
and is especially useful if you know Erlang or need help from other users, the Riak team, or
Basho.</p>

<p><img src="../assets/top.png" alt="Top"></p>

<h3>Making a Cluster</h3>

<p>With several solitary nodes running---assuming they are networked and are able to communicate to
each other---launching a cluster is the simplest part.</p>

<p>Executing the <code>cluster</code> command will output a descriptive set of commands.</p>

<pre><code class="bash">$ riak-admin cluster
The following commands stage changes to cluster membership. These commands
do not take effect immediately. After staging a set of changes, the staged
plan must be committed to take effect:

 join &lt;node&gt;                  Join node to the cluster containing &lt;node&gt;
 leave                        Have this node leave the cluster and shutdown
 leave &lt;node&gt;                 Have &lt;node&gt; leave the cluster and shutdown

 force-remove &lt;node&gt;          Remove &lt;node&gt; from the cluster without
                              first handing off data. Designed for
                              crashed, unrecoverable nodes

 replace &lt;node1&gt; &lt;node2&gt;      Have &lt;node1&gt; transfer all data to &lt;node2&gt;,
                              and then leave the cluster and shutdown

 force-replace &lt;node1&gt; &lt;node2&gt;  Reassign all partitions owned by &lt;node1&gt;
                              to &lt;node2&gt; without first handing off data,
                              and remove &lt;node1&gt; from the cluster.

Staging commands:
 plan                         Display the staged changes to the cluster
 commit                       Commit the staged changes
 clear                        Clear the staged changes
</code></pre>

<p>To create a new cluster, you must <code>join</code> another node (any will do). Taking a
node out of the cluster uses <code>leave</code> or <code>force-remove</code>, while swapping out
an old node for a new one uses <code>replace</code> or <code>force-replace</code>.</p>

<p>I should mention here that using <code>leave</code> is the nice way of taking a node
out of commission. However, you don&#39;t always get that choice. If a server
happens to explode (or simply smoke ominously), you don&#39;t need its approval
to remove it from the cluster, but can instead mark it as <code>down</code>.</p>

<p>But before we worry about removing nodes, let&#39;s add some first.</p>

<pre><code class="bash">$ riak-admin cluster join riak@AAA.cluster
Success: staged join request for &#39;riak@BBB.cluster&#39; to &#39;riak@AAA.cluster&#39;
$ riak-admin cluster join riak@AAA.cluster
Success: staged join request for &#39;riak@CCC.cluster&#39; to &#39;riak@AAA.cluster&#39;
</code></pre>

<p>Once all changes are staged, you must review the cluster <code>plan</code>. It will give you
all of the details of the nodes that are joining the cluster, and what it
will look like after each step or <em>transition</em>, including the <code>member-status</code>,
and how the <code>transfers</code> plan to handoff partitions.</p>

<p>Below is a simple plan, but there are cases when Riak requires multiple
transitions to enact all of your requested actions, such as adding and removing
nodes in one stage.</p>

<pre><code class="bash">$ riak-admin cluster plan
=============================== Staged Changes ==============
Action         Nodes(s)
-------------------------------------------------------------
join           &#39;riak@BBB.cluster&#39;
join           &#39;riak@CCC.cluster&#39;
-------------------------------------------------------------


NOTE: Applying these changes will result in 1 cluster transition

#############################################################
                         After cluster transition 1/1
#############################################################

================================= Membership ================
Status     Ring    Pending    Node
-------------------------------------------------------------
valid     100.0%     34.4%    &#39;riak@AAA.cluster&#39;
valid       0.0%     32.8%    &#39;riak@BBB.cluster&#39;
valid       0.0%     32.8%    &#39;riak@CCC.cluster&#39;
-------------------------------------------------------------
Valid:3 / Leaving:0 / Exiting:0 / Joining:0 / Down:0

WARNING: Not all replicas will be on distinct nodes

Transfers resulting from cluster changes: 42
  21 transfers from &#39;riak@AAA.cluster&#39; to &#39;riak@CCC.cluster&#39;
  21 transfers from &#39;riak@AAA.cluster&#39; to &#39;riak@BBB.cluster&#39;
</code></pre>

<p>Making changes to cluster membership can be fairly resource intensive,
so Riak defaults to only performing 2 transfers at a time. You can
choose to alter this <code>transfer-limit</code> using <code>riak-admin</code>, but bear in
mind the higher the number, the greater normal operations will be
impinged.</p>

<p>At this point, if you find a mistake in the plan, you have the chance to <code>clear</code> it and try
again. When you are ready, <code>commit</code> the cluster to enact the plan.</p>

<pre><code class="bash">$ riak-admin cluster commit
Cluster changes committed
</code></pre>

<p>Without any data, adding a node to a cluster is a quick operation. However, with large amounts of
data to be transferred to a new node, it can take quite a while before the new node is ready to use.</p>

<h3>Status Options</h3>

<p>To check on a launching node&#39;s progress, you can run the <code>wait-for-service</code> command. It will
output the status of the service and stop when it&#39;s finally up. In this example, we check
the <code>riak_kv</code> service.</p>

<pre><code class="bash">$ riak-admin wait-for-service riak_kv riak@CCC.cluster
riak_kv is not up: []
riak_kv is not up: []
riak_kv is up
</code></pre>

<p>You can get a list of available services with the <code>services</code> command.</p>

<p>You can also see if the whole ring is ready to go with <code>ringready</code>. If the nodes do not agree
on the state of the ring, it will output <code>FALSE</code>, otherwise <code>TRUE</code>.</p>

<pre><code class="bash">$ riak-admin ringready
TRUE All nodes agree on the ring [&#39;riak@AAA.cluster&#39;,&#39;riak@BBB.cluster&#39;,
                                  &#39;riak@CCC.cluster&#39;]
</code></pre>

<p>For a more complete view of the status of the nodes in the ring, you can check out <code>member-status</code>.</p>

<pre><code class="bash">$ riak-admin member-status
================================= Membership ================
Status     Ring    Pending    Node
-------------------------------------------------------------
valid      34.4%      --      &#39;riak@AAA.cluster&#39;
valid      32.8%      --      &#39;riak@BBB.cluster&#39;
valid      32.8%      --      &#39;riak@CCC.cluster&#39;
-------------------------------------------------------------
Valid:3 / Leaving:0 / Exiting:0 / Joining:0 / Down:0
</code></pre>

<p>And for more details of any current handoffs or unreachable nodes, try <code>ring-status</code>. It
also lists some information from <code>ringready</code> and <code>transfers</code>. Below I turned off the C
node to show what it might look like.</p>

<pre><code class="bash">$ riak-admin ring-status
================================== Claimant =================
Claimant:  &#39;riak@AAA.cluster&#39;
Status:     up
Ring Ready: true

============================== Ownership Handoff ============
Owner:      dev1 at 127.0.0.1
Next Owner: dev2 at 127.0.0.1

Index: 182687704666362864775460604089535377456991567872
  Waiting on: []
  Complete:   [riak_kv_vnode,riak_pipe_vnode]
...

============================== Unreachable Nodes ============
The following nodes are unreachable: [&#39;riak@CCC.cluster&#39;]

WARNING: The cluster state will not converge until all nodes
are up. Once the above nodes come back online, convergence
will continue. If the outages are long-term or permanent, you
can either mark the nodes as down (riak-admin down NODE) or
forcibly remove the nodes from the cluster (riak-admin
force-remove NODE) to allow the remaining nodes to settle.
</code></pre>

<p>If all of the above information options about your nodes weren&#39;t enough, you can
list the status of each vnode per node, via <code>vnode-status</code>. It&#39;ll show each
vnode by its partition number, give any status information, and a count of each
vnode&#39;s keys. Finally, you&#39;ll get to see each vnode&#39;s backend type---something I&#39;ll
cover in the next section.</p>

<pre><code class="bash">$ riak-admin vnode-status
Vnode status information
-------------------------------------------

VNode: 0
Backend: riak_kv_bitcask_backend
Status:
[{key_count,0},{status,[]}]

VNode: 91343852333181432387730302044767688728495783936
Backend: riak_kv_bitcask_backend
Status:
[{key_count,0},{status,[]}]

VNode: 182687704666362864775460604089535377456991567872
Backend: riak_kv_bitcask_backend
Status:
[{key_count,0},{status,[]}]

VNode: 274031556999544297163190906134303066185487351808
Backend: riak_kv_bitcask_backend
Status:
[{key_count,0},{status,[]}]

VNode: 365375409332725729550921208179070754913983135744
Backend: riak_kv_bitcask_backend
Status:
[{key_count,0},{status,[]}]
...
</code></pre>

<p>Some commands we did not cover are either deprecated in favor of their <code>cluster</code>
equivalents (<code>join</code>, <code>leave</code>, <code>force-remove</code>, <code>replace</code>, <code>force-replace</code>), or
flagged for future removal <code>reip</code> (use <code>cluster replace</code>).</p>

<p>I know this was a lot to digest, and probably pretty dry. Walking through command
line tools usually is. There are plenty of details behind many of the <code>riak-admin</code>
commands, too numerous to cover in such a short book. I encourage you to toy around
with them on your own installation.</p>

<h2 id="new-in-riak-2.0">New in Riak 2.0</h2>

<p>Riak has been a project since 2009. And in that time, it has undergone a few evolutions, largely technical improvements, such as more reliability and data safety mechanisms like active anti-entropy.</p>

<p>Riak 2.0 was not a rewrite, but rather, a huge shift in how developers who use Riak interact with it. While Basho continued to make backend improvements (such as better cluster metadata) and simplified using existing options (<code>repair-2i</code> is now a <code>riak-admin</code> command, rather than code you must execute), the biggest changes are immediately obvious to developers. But many of those improvements are also made easier for operators to administrate. So here are a few highlights of the new 2.0 interface options.</p>

<h3>Bucket Types</h3>

<p>A centerpiece of the new Riak 2.0 features is the addition of a higher-level bucket configuration namespace called <em>bucket types</em>. We discussed the general idea of bucket types in the previous chapters, but one major departure from standard buckets is that they are created via the command-line. This means that operators with server access can manage the default properties that all buckets of a given bucket type inherit.</p>

<p>Bucket types have a set of tools for creating, managing and activating them.</p>

<pre><code class="bash">$ riak-admin bucket-type
Usage: riak-admin bucket-type &lt;command&gt;

The follow commands can be used to manage bucket types for the cluster:

   list                           List all bucket types and their activation status
   status &lt;type&gt;                  Display the status and properties of a type
   activate &lt;type&gt;                Activate a type
   create &lt;type&gt; &lt;json&gt;           Create or modify a type before activation
   update &lt;type&gt; &lt;json&gt;           Update a type after activation
</code></pre>

<p>It&#39;s rather straightforward to <code>create</code> a bucket type. The JSON string accepted after the bucket type name are any valid bucket propertied. Any bucket that uses this type will inherit those properties. For example, say that you wanted to create a bucket type whose n_val was always 1 (rather than the default 3), named unsafe.</p>

<pre><code class="bash">$ riak-admin bucket-type create unsafe &#39;{&quot;props&quot;:{&quot;n_val&quot;:1}}&#39;
</code></pre>

<p>Once you create the bucket type, it&#39;s a good idea to check the <code>status</code>, and ensure the properties are what you meant to set.</p>

<pre><code class="bash">$ riak-admin bucket-type status unsafe
</code></pre>

<p>A bucket type is not active until you propgate it through the system by calling the <code>activate</code> command.</p>

<pre><code class="bash">$ riak-admin bucket-type activate unsafe
</code></pre>

<p>If something is wrong with the type&#39;s properties, you can always <code>update</code> it.</p>

<pre><code class="bash">$ riak-admin bucket-type update unsafe &#39;{&quot;props&quot;:{&quot;n_val&quot;:1}}&#39;
</code></pre>

<p>You can update a bucket type after it&#39;s actived. All of the changes that you make to the type will be inherited by every bucket under that type.</p>

<p>Of course, you can always get a <code>list</code> of the current bucket types in the system. The list will also say whether the bucket type is activated or not.</p>

<p>Other than that, there&#39;s nothing interesting about bucket types from an operations point of view, per se. Sure, there are some cool internal mechanisms at work, such as propogated metadata via a path laied out by a plum-tree and causally tracked by dotted version vectors. But that&#39;s only code plumbing. What&#39;s most interesting about bucket types are the new features you can take advantage of: datatypes, strong consistency, and search.</p>

<h3>Datatypes</h3>

<p>Datatypes are useful for engineers, since they no longer have to consider the complexity of manual conflict merges that can occur in fault situations. It can also be less stress on the system, since larger objects need only communicate their changes, rather than reinsert the full object.</p>

<p>Riak 2.0 supports four datatypes: <em>map</em>, <em>set</em>, <em>counter</em>, <em>flag</em>. You create a bucket type with a single datatype. It&#39;s not required, but often good form to name the bucket type after the datatype you&#39;re setting.</p>

<pre><code class="bash">$ riak-admin bucket-type create maps &#39;{&quot;props&quot;:{&quot;datatype&quot;:&quot;map&quot;}}&#39;
$ riak-admin bucket-type create sets &#39;{&quot;props&quot;:{&quot;datatype&quot;:&quot;set&quot;}}&#39;
$ riak-admin bucket-type create counters &#39;{&quot;props&quot;:{&quot;datatype&quot;:&quot;counter&quot;}}&#39;
$ riak-admin bucket-type create flags &#39;{&quot;props&quot;:{&quot;datatype&quot;:&quot;flag&quot;}}&#39;
</code></pre>

<p>Once a bucket type is created with the given datatype, you need only active it. Developers can then use this datatype like we saw in the previous chapter, but hopefully this example makes clear the suggestion of naming bucket types after their datatype.</p>

<pre><code class="bash">curl -XPUT &quot;$RIAK/types/counters/buckets/visitors/keys/index_page&quot; \
  -H &quot;Content-Type:application/json&quot;
  -d 1
</code></pre>

<h3>Strong Consistency</h3>

<p>Strong consistency (SC) is the opposite of everything that Riak stands for. Where Riak is all about high availability in the face of network or server errors, strong consistency is about safety over liveness. Either the network and servers are working perfectly, or the reads and writes fail. So why on earth would we ever want to provide SC and give up HA? Because you asked for. Really.</p>

<p>There are some very good use-cases for strong consistency. For example, when a user is completing a purchase, you might want to ensure that the system is always in a consistent state, or fail the purchase. Communicating that a purchase was made when it in fact was not, is not a good user experience. The opposite is even worse.</p>

<p>While Riak will continue to be primarily an HA system, there are cases where SC is useful, and developers should be allowed to choose without having to install an entirely new database. So all you need to do is activate it in <code>riak.conf</code>.</p>

<pre><code class="bash">strong_consistency = on
</code></pre>

<p>One thing to note is, although we generally recommend you have five nodes in a Riak cluster, it&#39;s not a hard requirement. Strong consistency, however, requires three nodes. It will not operate with fewer.</p>

<p>Once our SC systme is active, you&#39;ll lean on bucket types again. Only buckets that live under a bucket type setup for strong consistency will be strongly consistent. This means that you can have some buckets HA, other SC, in the same database. Let&#39;s call our SC bucket type <code>strong</code>.</p>

<pre><code class="bash">$ riak-admin bucket-type create strong &#39;{&quot;props&quot;:{&quot;consistent&quot;:true}}&#39;
$ riak-admin bucket-type activate strong
</code></pre>

<p>That&#39;s all the operator should need to do. The developers can use the <code>strong</code> bucket similarly to other buckets.</p>

<pre><code class="bash">curl -XPUT &quot;$RIAK/types/strong/buckets/purchases/keys/jane&quot; \
  -d &#39;{&quot;action&quot;:&quot;buy&quot;}&#39;
</code></pre>

<p>Jane&#39;s purchases will either succeed or fail. It will not be eventually consistent. If it fails, of course, she can try again.</p>

<p>What if your system is having problems with strong consistency? Basho has provided a command to interrogate the current status of the subsystem responsible for SC named ensemble. You can check it out by running <code>ensemble-status</code>.</p>

<pre><code class="bash">$ riak-admin ensemble-status
</code></pre>

<p>It will give you the best information it has as to the state of the system. For example, if you didn&#39;t enable <code>strong_consistency</code> in every node&#39;s <code>riak.conf</code>, you might see this.</p>

<pre><code class="bash">============================== Consensus System ===============================
Enabled:     false
Active:      false
Ring Ready:  true
Validation:  strong (trusted majority required)
Metadata:    best-effort replication (asynchronous)

Note: The consensus subsystem is not enabled.

================================== Ensembles ==================================
There are no active ensembles.
</code></pre>

<p>In the common case when all is working, you should see an output similar to the following:</p>

<pre><code class="bash">============================== Consensus System ===============================
Enabled:     true
Active:      true
Ring Ready:  true
Validation:  strong (trusted majority required)
Metadata:    best-effort replication (asynchronous)

================================== Ensembles ==================================
 Ensemble     Quorum        Nodes      Leader
-------------------------------------------------------------------------------
   root       4 / 4         4 / 4      riak@riak1
    2         3 / 3         3 / 3      riak@riak2
    3         3 / 3         3 / 3      riak@riak4
    4         3 / 3         3 / 3      riak@riak1
    5         3 / 3         3 / 3      riak@riak2
    6         3 / 3         3 / 3      riak@riak2
    7         3 / 3         3 / 3      riak@riak4
    8         3 / 3         3 / 3      riak@riak4
</code></pre>

<p>This output tells you that the consensus system is both enabled and active, as well as lists details about all known consensus groups (ensembles).</p>

<p>There is plenty more information about the details of strong consistency in the online docs.</p>

<h3>Search 2.0</h3>

<p>From an operations standpoint, search is deceptively simple. Functionally, there isn&#39;t much you should need to do with search, other than activate it in <code>riak.conf</code>.</p>

<pre><code class="bash">search = on
</code></pre>

<p>However, looks are deceiving. Under the covers, Riak Search 2.0 actually runs the search index software called Solr. Solr runs as a Java service. All of the code required to convert an object that you insert into a document that Solr can recognize (by a module called an <em>Extractor</em>) is Erlang, and so is the code which keeps the Riak objects and Solr indexes in sync through faults (via AAE), as well as all of the interfaces, security, stats, and query distribution. But since Solr is Java, we have to manage the JVM.</p>

<p>If you don&#39;t have much experience running Java code, let me distill most problems for you: you need more memory. Solr is a memory hog, easily requiring a minimum of 2 GiB of RAM dedicated only to the Solr service itself. This is in addition to the 4 GiB of RAM minimum that Basho recommends per node. So, according to math, you need a minimum of 6 GiB of RAM to run Riak Search. But we&#39;re not quite through yet.</p>

<p>The most important setting in Riak Search are the JVM options. These options are passed into the JVM command-line when the Solr service is started, and most of the options chosen are excellent defaults. I recommend not getting to hung up on tweaking those, with one notable exception.</p>

<pre><code class="bash">## The options to pass to the Solr JVM.  Non-standard options,
## i.e. -XX, may not be portable across JVM implementations.
## E.g. -XX:+UseCompressedStrings
## 
## Default: -d64 -Xms1g -Xmx1g -XX:+UseStringCache -XX:+UseCompressedOops
## 
## Acceptable values:
##   - text
search.solr.jvm_options = -d64 -Xms1g -Xmx1g -XX:+UseStringCache -XX:+UseCompressedOops
</code></pre>

<p>In the default setting, Riak gives 1 GiB of RAM to the Solr JVM heap. This is fine for small clusters with small, lightly used indexes. You may want to bump those heap values up---the two args of note are: <code>-Xms1g</code> (minimum size 1 gigabyte) and <code>-Xmx1g</code> (maximum size 1 gigabyte). Push those to 2 or 4 (or even higher) and you should be fine.</p>

<p>In the interested of completeness, Riak also communicates to Solr internally through a port, which you can configure (along with an option JMX port). You should never need to connect to this port yourself.</p>

<pre><code class="bash">## The port number which Solr binds to.
## NOTE: Binds on every interface.
## 
## Default: 8093
## 
## Acceptable values:
##   - an integer
search.solr.port = 8093

## The port number which Solr JMX binds to.
## NOTE: Binds on every interface.
## 
## Default: 8985
## 
## Acceptable values:
##   - an integer
search.solr.jmx_port = 8985
</code></pre>

<p>There&#39;s generally no great reason to alter these defaults, but they&#39;re there if you need them.</p>

<p>I should also note that, thanks to fancy bucket types, you can associate a bucket type with a search index. You associate buckets (or types) with indexes by adding a search_index property, with the name of a Solr index. Like so, assuming that you&#39;ve created a solr index named <code>my_index</code>:</p>

<pre><code class="bash">$ riak-admin bucket-type create indexed &#39;{&quot;props&quot;:{&quot;search_index&quot;:&quot;my_index&quot;}}&#39;
$ riak-admin bucket-type activate indexed
</code></pre>

<p>Now, any object that a developer puts into yokozuna under that bucket type will be indexed.</p>

<p>There&#39;s a lot more to search than we can possibly cover here without making it a book in its own right. You may want to checkout the following documentation in docs.basho.com for more details.</p>

<ul>
<li><a href="http://docs.basho.com/riak/latest/ops/advanced/configs/search/">Riak Search Settings</a></li>
<li><a href="http://docs.basho.com/riak/latest/dev/using/search/">Using Search</a></li>
<li><a href="http://docs.basho.com/riak/latest/dev/advanced/search/">Search Details</a></li>
<li><a href="http://docs.basho.com/riak/latest/dev/advanced/search-schema/">Search Schema</a></li>
<li><a href="http://docs.basho.com/riak/latest/ops/advanced/upgrading-search-2/">Upgrading Search from 1.x to 2.x</a></li>
</ul>

<h3>Security</h3>

<p>Riak has lived quite well in the first five years of its life without security. So why did Basho add it now? With the kind of security you get through a firewall, you can only get coarse-grained security. Someone can either access the system or not, with a few restrictions, depending on how clever you write your firewall rules.</p>

<p>With the addition of Security, Riak now supports authentication (identifying a user) and authorization (restricting user access to a subset of commands) of users and groups. Access can also be restricted to a known set of sources. The security design was inspired by the full-featured rules in PostgreSQL.</p>

<p>Before you decide to enable security, you should consider this checklist in advance.</p>

<ol>
<li>If you use security, you must upgrade to Riak Search 2.0. The old Search will not work (neither will the deprecated Link Walking). Check any Erlang MapReduce code for invocations of Riak modules other than <code>riak_kv_mapreduce</code>. Enabling security will prevent those from succeeding unless those modules are available via <code>add_path</code></li>
<li>Make sure that your application code is using the most recent drivers</li>
<li>Define users and (optionally) groups, and their sources</li>
<li>Grant the necessary permissions to each user/group</li>
</ol>

<p>With that out of the way, you can <code>enable</code> security with a command-line option (you can <code>disable</code> security as well). You can optionally check the <code>status</code> of security at any time.</p>

<pre><code class="bash">$ riak-admin security enable
$ riak-admin security status
Enabled
</code></pre>

<p>Adding users is as easy as the <code>add-user</code> command. A username is required, and can be followed with any key/value pairs. <code>password</code> and <code>groups</code> are special cases, but everything is free form. You can alter existing users as well. Users can belong to any number of groups, and inherit a union of all group settings.</p>

<pre><code class="bash">$ riak-admin security add-group mascots type=mascot
$ riak-admin security add-user bashoman password=Test1234
$ riak-admin security alter-user bashoman groups=mascots
</code></pre>

<p>You can see the list of all users via <code>print-users</code>, or all groups via <code>print-groups</code>.</p>

<pre><code class="bash">$ riak-admin security print-users
+----------+----------+----------------------+---------------------+
| username |  groups  |       password       |       options       |
+----------+----------+----------------------+---------------------+
| bashoman | mascots  |983e8ae1421574b8733824| [{&quot;type&quot;,&quot;mascot&quot;}] |
+----------+----------+----------------------+---------------------+
</code></pre>

<p>Creating user and groups is nice and all, but the real reason for doing this is so we can distinguish authorization between different users and groups. You <code>grant</code> or <code>revoke</code> <code>permissions</code> to users and groups by way of the command line, of course. You can grant/revoke a permission to anything, a certain bucket type, or a specific bucket.</p>

<pre><code class="bash">$ riak-admin security grant riak_kv.get on any to all
$ riak-admin security grant riak_kv.delete on any to admin
$ riak-admin security grant search.query on index people to bashoman
$ riak-admin security revoke riak_kv.delete on any to bad_admin
</code></pre>

<p>There are many kinds of permissions, one for every major operation or set of operations in Riak. It&#39;s worth noting that you can&#39;t add search permissions without search enabled.</p>

<ul>
<li><strong>riak_kv.get</strong> --- Retrieve objects</li>
<li><strong>riak_kv.put</strong> --- Create or update objects</li>
<li><strong>riak_kv.delete</strong>  --- Delete objects</li>
<li><strong>riak_kv.index</strong> --- Index objects using secondary indexes (2i)</li>
<li><strong>riak_kv.list_keys</strong> --- List all of the keys in a bucket</li>
<li><strong>riak_kv.list_buckets</strong>  --- List all buckets</li>
<li><strong>riak_kv.mapreduce</strong> --- Can run MapReduce jobs</li>
<li><strong>riak_core.get_bucket</strong>  --- Retrieve the props associated with a bucket</li>
<li><strong>riak_core.set_bucket</strong>  --- Modify the props associated with a bucket</li>
<li><strong>riak_core.get_bucket_type</strong> --- Retrieve the set of props associated with a bucket type</li>
<li><strong>riak_core.set_bucket_type</strong> --- Modify the set of props associated with a bucket type</li>
<li><strong>search.admin</strong>  --- The ability to perform search admin-related tasks, like creating and deleting indexes</li>
<li><strong>search.query</strong>  --- The ability to query an index</li>
</ul>

<p>Finally, with our group and user created, and given access to a subset of permissions, we have one more major item to deal with. We want to be able to filter connection from specific sources.</p>

<pre><code class="bash">$ riak-admin security add-source all|&lt;users&gt; &lt;CIDR&gt; &lt;source&gt; [&lt;option&gt;=&lt;value&gt;[...]]
</code></pre>

<p>This is powerful security, since Riak will only accept connections that pass specific criteria, such as a certain certificate or password, or from a specific IP address. Here we trust any connection that&#39;s initiated locally.</p>

<pre><code class="bash">$ riak-admin security add-source all 127.0.0.1/32 trust
</code></pre>

<p>There&#39;s plenty more you can learn about in the <a href="http://docs.basho.com/riak/2.0.0/ops/running/authz/">Authentication and Authorization</a> online documentation.</p>

<h3>Dynamic Ring Resizing</h3>

<p>As of Riak 2.0, you can now resize the number of vnodes in the ring. The number of vnodes must be a power of 2 (eg. <code>64</code>, <code>256</code>, <code>1024</code>). It&#39;s a very heavyweight operation, and should not be a replacement for proper growth planning (aiming for <code>8</code> to <code>16</code> vnodes per node). However, if you experience greater than expected growth, this is quite a bit easier than transfering your entire dataset manually to a larger cluster. It just continues the Riak philosophy of easy operations, and no downtime!</p>

<pre><code class="bash">$ riak-admin cluster resize-ring 128
Success: staged resize ring request with new size: 128
</code></pre>

<p>Then commit the cluster plan in required two phase plan/commit steps.</p>

<pre><code class="bash">$ riak-admin cluster plan
$ riak-admin cluster commit
</code></pre>

<p>It can take quite a while for ring resizing to complete. You&#39;re effectively moving around half (or more) of the cluster&#39;s values around to new partitions. You can track the status of this resize with the a couple commands. The <code>ring-status</code> command we&#39;ve seen before, which will show you all of the changes that are queued up or in progress.</p>

<pre><code class="bash">$ riak-admin ring-status
</code></pre>

<p>If you want to see a different view of specifically handoff transfers, there&#39;s the <code>transfers</code> command.</p>

<pre><code class="bash">$ riak-admin transfers
&#39;riak@AAA.cluster&#39; waiting to handoff 3 partitions
&#39;riak@BBB.cluster&#39; waiting to handoff 1 partitions
&#39;riak@CCC.cluster&#39; waiting to handoff 1 partitions
&#39;riak@DDD.cluster&#39; waiting to handoff 2 partitions

Active Transfers:

transfer type: resize_transfer
vnode type: riak_kv_vnode
partition: 1438665674247607560106752257205091097473808596992
started: 2014-01-20 21:03:53 [1.14 min ago]
last update: 2014-01-20 21:05:01 [1.21 s ago]
total size: 111676327 bytes
objects transferred: 122598

                         1818 Objs/s                          
     riak@AAA.cluster        =======&gt;       riak@DDD.cluster      
        |=========================                  |  58%    
                         950.38 KB/s                          
</code></pre>

<p>If the resize activity is taking too much time, or consuming too many resources, you can alter the <code>handoff_concurrency</code> limit on the fly. This limit is the number of vnodes per physical node that are allowed to perform handoff at once, and defaults to 2. You can change the setting in the entire cluster, or per node. Say you want to change transfer up to 4 vnodes at a time.</p>

<pre><code class="bash">riak-admin transfer-limit 4
</code></pre>

<p>Or for a single node.</p>

<pre><code class="bash">$ riak-admin transfer-limit riak@AAA.cluster 4
</code></pre>

<p>Ring resizing will be complete once you get this message from <code>riak-admin transfers</code>:</p>

<pre><code class="bash">No transfers active
</code></pre>

<p>What if something goes wrong? What if you made a mistake? No problem, you can always abort the ring resize command.</p>

<pre><code class="bash">$ riak-admin cluster resize-ring abort
$ riak-admin cluster plan
$ riak-admin cluster commit
</code></pre>

<p>Any queued handoffs will be stopped. But any completed handoffs may have to be transfered back. Easy-peasy!</p>

<h2 id="how-riak-is-built">How Riak is Built</h2>

<p><img src="../assets/riak-stack.svg" alt="Tech Stack"></p>

<p>It&#39;s difficult to label Riak as a single project. It&#39;s probably more correct to think of
Riak as the center of gravity for a whole system of projects. As we&#39;ve covered
before, Riak is built on Erlang, but that&#39;s not the whole story. It&#39;s more correct
to say Riak is fundamentally Erlang, with some pluggable native C code components
(like leveldb), Java (Yokozuna), and even JavaScript (for MapReduce or commit hooks).</p>

<p>The way Riak stacks technologies is a good thing to keep in mind, in order to make
sense of how to configure it properly.</p>

<h3>Erlang</h3>

<p><img src="../assets/decor/riak-stack-erlang.png" alt="Tech Stack Erlang"></p>

<p>When you fire up a Riak node, it also starts up an Erlang VM (virtual machine) to run
and manage Riak&#39;s processes. These include vnodes, process messages, gossips, resource
management and more. The Erlang operating system process is found as a <code>beam.smp</code>
command with many, many arguments.</p>

<p>These arguments are configured through the <code>etc/riak.conf</code> file. There are a few
settings you should pay special attention to.</p>

<pre><code class="bash">$ ps -o command | grep beam
/riak/erts-5.9.1/bin/beam.smp \
-K true \
-A 64 \
-W w -- \
-root /riak \
-progname riak -- \
-home /Users/ericredmond -- \
-boot /riak/releases/2.0.0/riak \
-embedded \
-config /riak/data/generated.configs/app.2014.08.15.12.38.45.config \
-pa ./lib/basho-patches \
-name riak@AAA.cluster \
-setcookie testing123 -- \
console
</code></pre>

<p>The <code>name</code> setting is the name of the current Riak node. Every node in your cluster
needs a different name. It should have the IP address or dns name of the server
this node runs on, and optionally a different prefix---though some people just like
to name it <em>riak</em> for simplicity (eg: <code>riak@node15.myhost</code>).</p>

<p>The <code>setcookie</code> parameter is a setting for Erlang to perform inter-process
communication (IPC) across nodes. Every node in the cluster must have the same
cookie name. I recommend you change the name from <code>riak</code> to something a little
less likely to accidentally conflict, like <code>hihohihoitsofftoworkwego</code>.</p>

<p>My <code>riak.conf</code> sets it&#39;s node name and cookie like this:</p>

<pre><code class="bash">## Name of the Erlang node
## 
## Default: riak@127.0.0.1
## 
## Acceptable values:
##   - text
nodename = riak@AAA.cluster

## Cookie for distributed node communication.  All nodes in the
## same cluster should use the same cookie or they will not be able to
## communicate.
## 
## Default: riak
## 
## Acceptable values:
##   - text
distributed_cookie = testing123
</code></pre>

<p>Continuing down the <code>riak.conf</code> file are more Erlang settings, some environment
variables that are set up for the process (prefixed by <code>-env</code>), followed by
some optional SSL encryption settings.</p>

<h3>riak_core</h3>

<p><img src="../assets/decor/riak-stack-core.png" alt="Tech Stack Core"></p>

<p>If any single component deserves the title of &quot;Riak proper&quot;, it would
be <em>Riak Core</em>. Core shares responsibility with projects built atop it
for managing the partitioned keyspace, launching and supervising
vnodes, preference list building, hinted handoff, and things that
aren&#39;t related specifically to client interfaces, handling requests,
or storage.</p>

<p>Riak Core, like any project, has some hard-coded values (for example, how
protocol buffer messages are encoded in binary). However, many values
can be modified to fit your use case. The majority of this configuration
occurs under <code>riak.conf</code>. This file is Erlang code, so commented lines
begin with a <code>%</code> character.</p>

<p>The <code>riak_core</code> configuration section allows you to change the options in
this project. This handles basic settings, like files/directories where
values are stored or to be written to, the number of partitions/vnodes
in the cluster (<code>ring_size</code>), and several port options.</p>

<pre><code class="bash">## Default location of ringstate
ring.state_dir = $(platform_data_dir)/ring

## Number of partitions in the cluster (only valid when first
## creating the cluster). Must be a power of 2, minimum 8 and maximum
## 1024.
## 
## Default: 64
## 
## Acceptable values:
##   - an integer
ring_size = 8

## listener.http.&lt;name&gt; is an IP address and TCP port that the Riak
## HTTP interface will bind.
## 
## Default: 127.0.0.1:8098
## 
## Acceptable values:
##   - an IP/port pair, e.g. 127.0.0.1:8098
listener.http.internal = 0.0.0.0:8098

## listener.protobuf.&lt;name&gt; is an IP address and TCP port that the Riak
## Protocol Buffers interface will bind.
## 
## Default: 127.0.0.1:8087
## 
## Acceptable values:
##   - an IP/port pair, e.g. 127.0.0.1:8087
listener.protobuf.internal = 0.0.0.0:8087

## listener.https.&lt;name&gt; is an IP address and TCP port that the Riak
## HTTPS interface will bind.
## 
## Acceptable values:
##   - an IP/port pair, e.g. 127.0.0.1:8069
## listener.https.internal = 127.0.0.1:8069

## riak handoff_port is the TCP port that Riak uses for
## intra-cluster data handoff.
## handoff.port = 8099

## Platform-specific installation paths
## platform_bin_dir = ./bin
## platform_data_dir = ./data
## platform_etc_dir = ./etc
## platform_lib_dir = ./lib
## platform_log_dir = ./log
</code></pre>

<h3>riak_kv</h3>

<p><img src="../assets/decor/riak-stack-kv.png" alt="Tech Stack KV"></p>

<p>Riak KV is a key/value implementation of Riak Core. This is where the
magic happens, such as handling requests and coordinating them for
redundancy and read repair. It&#39;s what makes Riak a KV store rather
than something else like a Cassandra-style columnar data store.</p>

<!-- When configuring KV, you may scratch your head about about when a setting belongs
under `riak_kv` versus `riak_core`. For example, if `http` is under core, why
is raw_name under riak. -->

<p>KV is so integral to the function of Riak, that it&#39;s hardly worth going over
its settings as an independent topic. Many of of the values you set in other
subsystems are used by KV in some capacity. So let&#39;s move on.</p>

<h3>yokozuna</h3>

<p><img src="../assets/decor/riak-stack-yokozuna.png" alt="Tech Stack Yokozuna"></p>

<p>Yokozuna is the newest addition to the Riak ecosystem. It&#39;s an integration of
the distributed Solr search engine into Riak, and provides some extensions
for extracting, indexing, and tagging documents. The Solr server runs its
own HTTP interface, and though your Riak users should never have to access
it, you can choose which <code>solr_port</code> will be used.</p>

<pre><code class="bash">search = on
search.solr_port = 8093
</code></pre>

<h3>bitcask, eleveldb, memory, multi</h3>

<p>Several modern databases have swappable backends, and Riak is no different in that
respect. Riak currently supports three different storage engines: <em>Bitcask</em>,
<em>eLevelDB</em>, and <em>Memory</em> --- and one hybrid called <em>Multi</em>.</p>

<p>Using a backend is simply a matter of setting the <code>storage_backend</code> with one of the following values.</p>

<ul>
<li><code>bitcask</code> - The catchall Riak backend. If you don&#39;t have a compelling reason to <em>not</em> use it, this is my suggestion.</li>
<li><code>leveldb</code> - A Riak-friendly backend which uses a very customized version of Google&#39;s leveldb. This is necessary if you have too many keys to fit into memory, or wish to use 2i.</li>
<li><code>memory</code> - A main-memory backend, with time-to-live (TTL). Meant for transient data.</li>
<li><code>multi</code> - Any of the above backends, chosen on a per-bucket basis.</li>
</ul>

<pre><code class="bash">## Specifies the storage engine used for Riak&#39;s key-value data
## and secondary indexes (if supported).
## 
## Default: bitcask
## 
## Acceptable values:
##   - one of: bitcask, leveldb, memory, multi
storage_backend = memory
</code></pre>

<p>Then, with the exception of Multi, each memory configuration is under one of
the following options. TTL (time to live) is great and useful, but note, that
it&#39;s only useful if you&#39;re using Riak as a strict KV store. The backend won&#39;t
communicate to other systems (such as Search) that the value has timed out.</p>

<pre><code class="bash">## Memory Config
memory.max_memory = 4GB
memory.ttl = 86400  # 1 Day in seconds
</code></pre>

<p>Bitcask is a simple soul. It&#39;s the backend of choice, and generally the most complex setting you might want to toy with is how bitcask flushes data to disk.</p>

<pre><code class="bash">## A path under which bitcask data files will be stored.
## 
## Default: $(platform_data_dir)/bitcask
## 
## Acceptable values:
##   - the path to a directory
bitcask.data_root = $(platform_data_dir)/bitcask

## Configure how Bitcask writes data to disk.
## erlang: Erlang&#39;s built-in file API
## nif: Direct calls to the POSIX C API
## The NIF mode provides higher throughput for certain
## workloads, but has the potential to negatively impact
## the Erlang VM, leading to higher worst-case latencies
## and possible throughput collapse.
## 
## Default: erlang
## 
## Acceptable values:
##   - one of: erlang, nif
bitcask.io_mode = erlang
</code></pre>

<p>However, there are many, many more levers and knobs to pull and twist.</p>

<pre><code class="bash">## Bitcask Config
bitcask.data_root = $(platform_data_dir)/bitcask
bitcask.expiry = off
bitcask.expiry.grace_time = 0
bitcask.fold.max_age = unlimited
bitcask.fold.max_puts = 0
bitcask.hintfile_checksums = strict
bitcask.io_mode = erlang
bitcask.max_file_size = 2GB
bitcask.max_merge_size = 100GB
bitcask.merge.policy = always
bitcask.merge.thresholds.dead_bytes = 128MB
bitcask.merge.thresholds.fragmentation = 40
bitcask.merge.thresholds.small_file = 10MB
bitcask.merge.triggers.dead_bytes = 512MB
bitcask.merge.triggers.fragmentation = 60
bitcask.merge.window.end = 23
bitcask.merge.window.start = 0
bitcask.merge_check_interval = 3m
bitcask.merge_check_jitter = 30%
bitcask.open_timeout = 4s
bitcask.sync.strategy = none
</code></pre>

<p>There are many configuration values for leveldb. But most of the time, you&#39;re best served to leave them alone. The only value you may ever want to play with is the maximum_memory percent, which defines the most system memory that leveldb will ever use.</p>

<pre><code class="bash">## This parameter defines the percentage of total server memory
## to assign to LevelDB. LevelDB will dynamically adjust its internal
## cache sizes to stay within this size.  The memory size can
## alternately be assigned as a byte count via leveldb.maximum_memory
## instead.
## 
## Default: 70
## 
## Acceptable values:
##   - an integer
leveldb.maximum_memory.percent = 70
</code></pre>

<p>But, if you really want to peek into leveldb&#39;s spectrum of choices:</p>

<pre><code class="bash"># LevelDB Config
leveldb.block.restart_interval = 16
leveldb.block.size = 4KB
leveldb.block.size_steps = 16
leveldb.block_cache_threshold = 32MB
leveldb.bloomfilter = on
leveldb.compaction.trigger.tombstone_count = 1000
leveldb.compression = on
leveldb.data_root = $(platform_data_dir)/leveldb
leveldb.fadvise_willneed = false
leveldb.limited_developer_mem = on
leveldb.sync_on_write = off
leveldb.threads = 71
leveldb.tiered = off
leveldb.verify_checksums = on
leveldb.verify_compaction = on
leveldb.write_buffer_size_max = 60MB
leveldb.write_buffer_size_min = 30MB
</code></pre>

<p><img src="../assets/decor/riak-stack-backend.png" alt="Tech Stack Backend"></p>

<p>With the Multi backend, you can even choose different backends
for different buckets. This can make sense, as one bucket may hold
user information that you wish to index (use eleveldb), while another
bucket holds volatile session information that you may prefer to simply
remain resident (use memory).</p>

<p>You can set up a multi backend by adding/using an <code>advanced.config</code>
file, which lives in the riak <code>etc</code> directory alongside <code>riak.conf</code>.</p>

<pre><code class="bash">%% Riak KV config
{riak_kv, [
  %% Storage_backend specifies the Erlang module defining
  %% the storage mechanism that will be used on this node.
  {storage_backend = riak_kv_multi_backend},

  %% Choose one of the names you defined below
  {multi_backend_default, &lt;&lt;&quot;bitcask_multi&quot;&gt;&gt;},

  {multi_backend, [
    %% Heres where you set the individual backends
    {&lt;&lt;&quot;bitcask_multi&quot;&gt;&gt;,  riak_kv_bitcask_backend, [
      %% bitcask configuration
      {config1, ConfigValue1},
      {config2, ConfigValue2}
    ]},
    {&lt;&lt;&quot;memory_multi&quot;&gt;&gt;,   riak_kv_memory_backend, [
      %% memory configuration
      {max_memory, 8192}   %% 8GB
    ]}
  ]},
]}.
</code></pre>

<p>You can put the <code>memory_multi</code> configured above to the <code>session_data</code> bucket
by just setting its <code>backend</code> property.</p>

<pre><code class="bash">$ curl -XPUT $RIAK/types/default/buckets/session_data/props \
  -H &quot;Content-Type: application/json&quot; \
  -d &#39;{&quot;props&quot;:{&quot;backend&quot;:&quot;memory_multi&quot;}}&#39;
</code></pre>

<h3>riak_api</h3>

<p><img src="../assets/decor/riak-stack-api.png" alt="Tech Stack API"></p>

<p>So far, all of the components we&#39;ve seen have been inside the Riak
house. The API is the front door. <em>In a perfect world</em>, the API would
manage two implementations: HTTP and Protocol buffers (PB), an
efficient binary protocol framework designed by Google.</p>

<p>But because they are not yet separated, only PB is configured under <code>riak_api</code>,
while HTTP still remains under KV.</p>

<p>In any case, Riak API represents the client facing aspect of Riak. Implementations
handle how data is encoded and transferred, and this project handles the services
for presenting those interfaces, managing connections, providing entry points.</p>

<pre><code class="bash">## listener.protobuf.&lt;name&gt; is an IP address and TCP port that the Riak
## Protocol Buffers interface will bind.
## 
## Default: 127.0.0.1:8087
## 
## Acceptable values:
##   - an IP/port pair, e.g. 127.0.0.1:8087
listener.protobuf.internal = 0.0.0.0:8087

## The maximum length to which the queue of pending connections
## may grow. If set, it must be an integer &gt; 0. If you anticipate a
## huge number of connections being initialized *simultaneously*, set
## this number higher.
## 
## Default: 128
## 
## Acceptable values:
##   - an integer
protobuf.backlog = 128
</code></pre>

<h3>Other projects</h3>

<p>Other projects add depth to Riak but aren&#39;t strictly necessary. Two of
these projects are lager, for logging, and riak_sysmon, for
monitoring. Both have reasonable defaults and well-documented
settings.</p>

<ul>
<li><a href="https://github.com/basho/lager">https://github.com/basho/lager</a></li>
<li><a href="https://github.com/basho/riak_sysmon">https://github.com/basho/riak_sysmon</a></li>
</ul>

<p>Most of the time, you&#39;ll just use lager&#39;s default logging settings.
However, you can configure lager via <code>riak.conf</code>.</p>

<pre><code class="bash">## Where to emit the default log messages (typically at &#39;info&#39;
## severity):
## off: disabled
## file: the file specified by log.console.file
## console: to standard output (seen when using `riak attach-direct`)
## both: log.console.file and standard out.
## 
## Default: both
## 
## Acceptable values:
##   - one of: off, file, console, both
log.console = both

## The severity level of the console log, default is &#39;info&#39;.
## 
## Default: info
## 
## Acceptable values:
##   - one of: debug, info, warning, error
log.console.level = info

## When &#39;log.console&#39; is set to &#39;file&#39; or &#39;both&#39;, the file where
## console messages will be logged.
## 
## Default: $(platform_log_dir)/console.log
## 
## Acceptable values:
##   - the path to a file
log.console.file = $(platform_log_dir)/console.log

## The file where error messages will be logged.
## 
## Default: $(platform_log_dir)/error.log
## 
## Acceptable values:
##   - the path to a file
log.error.file = $(platform_log_dir)/error.log

## When set to &#39;on&#39;, enables log output to syslog.
## 
## Default: off
## 
## Acceptable values:
##   - on or off
log.syslog = off

## Whether to enable the crash log.
## 
## Default: on
## 
## Acceptable values:
##   - on or off
log.crash = on

## If the crash log is enabled, the file where its messages will
## be written.
## 
## Default: $(platform_log_dir)/crash.log
## 
## Acceptable values:
##   - the path to a file
log.crash.file = $(platform_log_dir)/crash.log

## Maximum size in bytes of individual messages in the crash log
## 
## Default: 64KB
## 
## Acceptable values:
##   - a byte size with units, e.g. 10GB
log.crash.maximum_message_size = 64KB

## Maximum size of the crash log in bytes, before it is rotated
## 
## Default: 10MB
## 
## Acceptable values:
##   - a byte size with units, e.g. 10GB
log.crash.size = 10MB

## The schedule on which to rotate the crash log.  For more
## information see:
## https://github.com/basho/lager/blob/master/README.md#internal-log-rotation
## 
## Default: $D0
## 
## Acceptable values:
##   - text
log.crash.rotation = $D0

## The number of rotated crash logs to keep. When set to
## &#39;current&#39;, only the current open log file is kept.
## 
## Default: 5
## 
## Acceptable values:
##   - an integer
##   - the text &quot;current&quot;
log.crash.rotation.keep = 5
</code></pre>

<p>If you want to set custom log messages layouts, you can set them
in <code>advanced.config</code>.</p>

<pre><code class="bash">%% Lager Config
{lager, [
  %% What handlers to install with what arguments
  %% If you wish to disable rotation, you can either set
  %% the size to 0 and the rotation time to &quot;&quot;, or instead
  %% specify 2-tuple that only consists of {Logfile, Level}.
  {handlers, [
    {lager_file_backend, [
      {&quot;./log/error.log&quot;, error, 10485760, &quot;$D0&quot;, 5},
      {&quot;./log/console.log&quot;, info, 10485760, &quot;$D0&quot;, 5}
    ]}
  ]},
]}.
</code></pre>

<p>Finally, there&#39;s a system monitor (sysmon) that tracks the Erlang VM. It&#39;s usually best to just keep these default values as-is.</p>

<pre><code class="bash">runtime_health.thresholds.busy_ports = 2
runtime_health.thresholds.busy_processes = 30
runtime_health.triggers.distribution_port = on
runtime_health.triggers.port = on
runtime_health.triggers.process.garbage_collection = off
runtime_health.triggers.process.heap_size = 160444000
</code></pre>

<h2 id="tools">Tools</h2>

<h3>Riaknostic</h3>

<p>You may recall that we skipped the <code>diag</code> command while looking through
<code>riak-admin</code>, but it&#39;s time to circle back around.</p>

<p><a href="http://riaknostic.basho.com/">Riaknostic</a> is a diagnostic tool
for Riak, meant to run a suite of checks against an installation to
discover potential problems. If it finds any, it also recommends
potential resolutions.</p>

<p>Riaknostic exists separately from the core project but as of Riak 1.3
is included and installed with the standard database packages.</p>

<pre><code class="bash">$ riak-admin diag --list
Available diagnostic checks:

  disk                 Data directory permissions and atime
  dumps                Find crash dumps
  memory_use           Measure memory usage
  nodes_connected      Cluster node liveness
  ring_membership      Cluster membership validity
  ring_preflists       Check ring satisfies n_val
  ring_size            Ring size valid
  search               Check whether search is enabled on all nodes
</code></pre>

<p>I&#39;m a bit concerned that my disk might be slow, so I ran the <code>disk</code> diagnostic.</p>

<pre><code class="bash">$ riak-admin diag disk
21:52:47.353 [notice] Data directory /riak/data/bitcask is\
not mounted with &#39;noatime&#39;. Please remount its disk with the\
&#39;noatime&#39; flag to improve performance.
</code></pre>

<p>Riaknostic returns an analysis and suggestion for improvement. Had my disk
configuration been ok, the command would have returned nothing.</p>

<h3>Riak Control</h3>

<p>The last tool we&#39;ll look at is the aptly named
<a href="http://docs.basho.com/riak/latest/ops/advanced/riak-control/">Riak Control</a>.
It&#39;s a web application for managing Riak clusters, watching, and drilling down
into the details of your nodes to get a comprehensive view of the system. That&#39;s the
idea, anyway. It&#39;s forever a work in progress, and it does not yet have parity with
all of the command-line tools we&#39;ve looked at. However, it&#39;s great for quick
checkups and routing configuration changes.</p>

<p>Riak Control is shipped with Riak as of version 1.1, but turned off by
default. You can enable it on one of your servers by editing
<code>riak.conf</code> and restarting the node.</p>

<p>If you&#39;re going to turn it on in production, do so carefully: you&#39;re
opening up your cluster to remote administration using a password that
sadly must be stored in plain text in the configuration file.</p>

<p>The first step is to enable SSL and HTTPS in the <code>riak_core</code> section
of <code>riak.conf</code>.  You can just uncomment these lines, set the <code>https</code>
port to a reasonable value like <code>8069</code>, and point the <code>certfile</code> and
<code>keyfile</code> to your SSL certificate. If you have an intermediate
authority, add the <code>cacertfile</code> too.</p>

<pre><code class="bash">## listener.https.&lt;name&gt; is an IP address and TCP port that the Riak
## HTTPS interface will bind.
## 
## Acceptable values:
##   - an IP/port pair, e.g. 127.0.0.1:8069
listener.https.internal = 127.0.0.1:8069

## Default cert location for https can be overridden
## with the ssl config variable, for example:
## 
## Acceptable values:
##   - the path to a file
ssl.certfile = $(platform_etc_dir)/cert.pem

## Default key location for https can be overridden with the ssl
## config variable, for example:
## 
## Acceptable values:
##   - the path to a file
ssl.keyfile = $(platform_etc_dir)/key.pem

## Default signing authority location for https can be overridden
## with the ssl config variable, for example:
## 
## Acceptable values:
##   - the path to a file
ssl.cacertfile = $(platform_etc_dir)/cacertfile.pem
</code></pre>

<p>Then, you&#39;ll have to <code>enable</code> Riak Control in your <code>riak.conf</code>, and add a user.
Note that the user password is plain text. Yeah it sucks, so be careful to not
open your Control web access to the rest of the world, or you risk giving away
the keys to the kingdom.</p>

<pre><code class="bash">## Set to &#39;off&#39; to disable the admin panel.
## 
## Default: off
## 
## Acceptable values:
##   - on or off
riak_control = on

## Authentication mode used for access to the admin panel.
## 
## Default: off
## 
## Acceptable values:
##   - one of: off, userlist
riak_control.auth.mode = on

## If riak control&#39;s authentication mode (riak_control.auth.mode)
## is set to &#39;userlist&#39; then this is the list of usernames and
## passwords for access to the admin panel.
## To create users with given names, add entries of the format:
## riak_control.auth.user.USERNAME.password = PASSWORD
## replacing USERNAME with the desired username and PASSWORD with the
## desired password for that user.
## 
## Acceptable values:
##   - text
riak_control.auth.user.admin.password = lovesecretsexgod
</code></pre>

<p><img src="../assets/control-snapshot.png" alt="Snapshot View"></p>

<p>With Control in place, restart your node and connect via a browser (note you&#39;re using
<code>https</code>) <code>https://localhost:8069/admin</code>. After you log in using the user you set, you
should see a snapshot page, which communicates the health of your cluster.</p>

<p>If something is wrong, you&#39;ll see a huge red &quot;X&quot; instead of the green check mark, along
with a list of what the trouble is.</p>

<p>From here you can drill down into a view of the cluster&#39;s nodes, with details on memory usage, partition distribution, and other status. You can also add and configure these nodes, then view the plan and status of those changes.</p>

<p><img src="../assets/control-cluster.png" alt="Cluster View"></p>

<p>There is more in line for Riak Control, like performing MapReduce queries, stats views,
graphs, and more coming down the pipe. It&#39;s not a universal toolkit quite yet,
but it has a phenomenal start.</p>

<p>Once your cluster is to your liking, you can manage individual nodes, either stopping or taking them down permanently. You can also find a more detailed view of an individual node, such as what percentage of the cluster it manages, or its RAM usage.</p>

<p><img src="../assets/control-node-mgmt.png" alt="Node Management View"></p>

<!-- ## Scaling Riak
Vertically (by adding bigger hardware), and Horizontally (by adding more nodes).
 -->

<h2 id="wrap-up">Wrap-up</h2>

<p>Once you comprehend the basics of Riak, it&#39;s a simple thing to manage. If this seems like
a lot to swallow, take it from a long-time relational database guy (me), Riak is a
comparatively simple construct, especially when you factor in the complexity of
distributed systems in general. Riak manages much of the daily tasks an operator might
do themselves manually, such as sharding by keys, adding/removing nodes, rebalancing data,
supporting multiple backends, and allowing growth with unbalanced nodes.
And due to Riak&#39;s architecture, the best part of all is when a server goes down at night,
you can sleep (do you remember what that was?), and fix it in the morning.</p>
<h1 id="writing-riak-applications">Writing Riak Applications</h1>

<p>Chapters 2 and 3 covered key concepts that every developer should
know. In this chapter, we look more closely at ways to build (and more
importantly not to build) Riak applications.</p>

<h2 id="how-not-to-write-a-riak-app">How Not to Write a Riak App</h2>

<p>Writing a Riak application is very much <strong>not</strong> like writing an
application that relies on a relational database. The core ideas and
vocabulary from database theory still apply, of course, but many of
the decisions that inform the application layer are transformed.</p>

<p>Effectively, <em>all</em> of these anti-patterns make some degree of sense when
writing an application against an RDBMS (such as MySQL). Unfortunately,
<em>none</em> of them lend themselves to great Riak applications.</p>

<h3 id="dynamic-querying">Dynamic querying</h3>

<p>Riak&#39;s tools for finding data (2i, MapReduce, and full-text search)
are useful but should be used judiciously. None of these scale nearly
as well as key/value operations. Queries that may work well on a few
nodes in development may run more slowly in a busy production
environment, especially as the cluster grows in size.</p>

<p>Key/value operations seem primitive (and they are) but you&#39;ll find that
they are flexible, scalable, and very, very fast (and predictably so).
One thing to always bear in mind about key/value operations:</p>

<p><em>Reads and writes in Riak should be as fast with ten billion values
in storage as with ten thousand.</em></p>

<p>Design the main functionality of your application around the straight
key/value operations that Riak provides and your software will
continue to work at blazing speeds when you have petabytes of data
stored across dozens of servers.</p>

<h3 id="normalization">Normalization</h3>

<p>Normalizing data is generally a useful approach in a relational
database, but it is unlikely to lead to happy results with Riak.</p>

<p>Riak lacks foreign key constraints and join operations, two vital
parts of the normalization story, so reconstructing a single record
from multiple objects would involve multiple read requests. This is
certainly possible and fast enough on a small scale, but it is not ideal
for larger requests.</p>

<p>In contrast, imagine the performance of your application if most of your
requests involved a single read operation. Much faster and predictably
so, even at scale. Preparing and storing the answers to
queries you&#39;re going to ask for later is a best practice for Riak.</p>

<p>See <a href="#denormalization">Denormalization</a> for more discussion.</p>

<h3 id="ducking-conflict-resolution">Ducking conflict resolution</h3>

<p>One of the first hurdles Basho faced when releasing Riak was educating
developers on the complexities of eventual consistency and the need to
intelligently resolve data conflicts.</p>

<p>Because Riak is optimized for high availability, <em>even when servers
are offline or disconnected from the cluster due to network failures</em>,
it is not uncommon for two servers to have different versions of a
piece of data.</p>

<p>The simplest approach to coping with this is to allow Riak to choose a
winner based on timestamps. It can do this more effectively if
developers follow Basho&#39;s guidance on sending updates with <em>vector
clock</em> metadata to help track causal history. But concurrent updates
cannot always be automatically resolved via vector clocks, and
trusting server clocks to determine which write was the last to arrive
is a <strong>terrible</strong> conflict resolution method.</p>

<p>Even if your server clocks are magically always in sync, are your
business needs well served by blindly applying the most recent update?
Some databases have no alternative but to handle it that way, but we think
you deserve better.</p>

<p>Typed buckets in Riak 2.0 default to retaining conflicts and requiring
the application to resolve them, but we&#39;re also providing replicated,
conflict-free data types (we call them Riak Data Types) to automate
conflict resolution on the server side.</p>

<p>If you want to minimize the need for conflict resolution, modeling
with as much immutable data as possible is a big win.</p>

<p><a href="#conflict-resolution">Conflict Resolution</a> covers this in much more detail.</p>

<h3 id="mutability">Mutability</h3>

<p>For years, functional programmers have been singing the praises of
immutable data, which can confer significant advantages when using a
distributed datastore like Riak.</p>

<p>Most obviously, conflict resolution is dramatically simplified when
objects are never updated (because it is avoided entirely).</p>

<p>Even in the world of single-server database servers, updating records
in place carries costs. Most databases lose all sense of history when
data is updated, and it&#39;s entirely possible for two different clients
to overwrite the same field in rapid succession, leading to unexpected
results.</p>

<p>Some data is always going to be mutable, but thinking about the
alternative can lead to better design.</p>

<h3 id="select-*-from-&lt;table&gt;">SELECT * FROM &lt;table&gt;</h3>

<p>A perfectly natural response when first encountering a populated
database is to see what&#39;s in it. In a relational database, you can
easily retrieve a list of tables and start browsing their records.</p>

<p>As it turns out, this is a terrible idea in Riak.</p>

<p>Not only is Riak optimized for unstructured, opaque data, it is also
not designed to allow for trivial retrieval of lists of buckets (very
loosely analogous to tables) and keys.</p>

<p>Doing so can put a great deal of stress on a large cluster and can
significantly impact performance.</p>

<p>It&#39;s a rather unusual idea for someone coming from a relational
mindset, but being able to algorithmically determine the key that you
need for the data you want to retrieve is a major part of the Riak
application story.</p>

<h3 id="large-objects">Large objects</h3>

<p>Because Riak sends multiple copies of your data around the network for
every request, values that are too large can clog the pipes, so to
speak, causing significant latency problems.</p>

<p>Basho generally recommends 1-4MB objects as a soft cap; larger sizes
are possible with careful tuning, however.</p>

<p>We&#39;ll return to object size when discussing <a href="#conflict-resolution">Conflict Resolution</a>; for
the moment, suffice it to say that if you&#39;re planning on storing
<em>mutable</em> objects in the upper ranges of our recommendations, you&#39;re
particularly at risk of latency problems.</p>

<p>For significantly larger objects,
<a href="http://basho.com/riak-cloud-storage/">Riak CS</a> offers an Amazon
S3-compatible (and also OpenStack Swift-compatible) key/value object
store that uses Riak under the hood.</p>

<h3 id="running-a-single-server">Running a single server</h3>

<p>This is more of an operations anti-pattern, but it is a common
misunderstanding of Riak&#39;s architecture.</p>

<p>It is quite common to install Riak in a development environment using
its <code>devrel</code> build target, which creates 5 full Riak stacks (including
Erlang virtual machines) to run on one server to simulate a cluster.</p>

<p>However, running Riak on a single server for benchmarking or
production use is counterproductive, regardless of whether you have 1
stack or 5 on the box.</p>

<p>It is possible to argue that Riak is more of a database coordination
platform than a database itself. It uses Bitcask or LevelDB to persist
data to disk, but more importantly, it commonly uses <em>at least</em> 64
such embedded databases in a cluster.</p>

<p>Needless to say, if you run 64 databases simultaneously on a single
filesystem you are risking significant I/O and CPU contention unless
the environment is carefully tuned (and has some pretty fast disks).</p>

<p>Perhaps more importantly, Riak&#39;s core design goal, its raison d&#39;être,
is high availability via data redundancy and related
mechanisms. Writing three copies of all your data to a single
server is mostly pointless, both contributing to resource contention
and throwing away Riak&#39;s ability to survive server failure.</p>

<h3 id="further-reading">Further reading</h3>

<ul>
<li><a href="http://docs.basho.com/riak/latest/theory/why-riak/">Why Riak</a> (docs.basho.com)</li>
<li><a href="http://docs.basho.com/riak/latest/dev/data-modeling/">Data Modeling</a> (docs.basho.com)</li>
<li><a href="https://basho.com/clocks-are-bad-or-welcome-to-distributed-systems/">Clocks Are Bad, Or, Welcome to the Wonderful World of Distributed Systems</a> (Basho blog)</li>
</ul>

<h2 id="denormalization">Denormalization</h2>

<p>Normal forms are the holy grail of schema design in the relational
world. Duplication is misery, we learn. Disk space is constrained, so
let foreign keys and join operations and views reassemble your data.</p>

<p>Conversely, when you step into a world <em>without</em> join operations,
<strong>stop normalizing</strong>. In fact, go the other direction, and duplicate
your data as much as you need to. Denormalize all the things!</p>

<p>I&#39;m sure you immediately thought of a few objections to
denormalization; I&#39;ll do what I can to dispel your fears. Read on,
Macduff.</p>

<h3 id="disk-space">Disk space</h3>

<p>Let me get the easy concern out of the way: don&#39;t worry about disk
space. I&#39;m not advocating complete disregard for it, but one of the
joys of operating a Riak database is that adding more computing
resources and disk space is not a complex, painful operation that
risks downtime for your application or, worst of all, manual sharding
of your data.</p>

<p>Need more disk space? Add another server. Install your OS, install
Riak, tell the cluster you want to join it, and then pull the
trigger. Doesn&#39;t get much easier than that.</p>

<h3 id="performance-over-time">Performance over time</h3>

<p>If you&#39;ve ever created a <em>really</em> large table in a relational
database, you have probably discovered that your performance is
abysmal. Yes, indexes help with searching large tables, but
maintaining those indexes are <strong>expensive</strong> at large data sizes.</p>

<p>Riak includes a data organization structure vaguely similar to a
table, called a <em>bucket</em>, but buckets don&#39;t carry the indexing
overhead of a relational table. As you dump more and more data into a
bucket, write (and read) performance is constant.</p>

<h3 id="performance-per-request">Performance per request</h3>

<p>Yes, writing the same piece of data multiple times is slower than
writing it once, by definition.</p>

<p>However, for many Riak use cases, writes can be asynchronous. No one
is (or should be) sitting at a web browser waiting for a sequence of
write requests to succeed.</p>

<p>What users care about is <strong>read</strong> performance. How quickly can you
extract the data that you want?</p>

<p>Unless your application is receiving many hundreds or thousands of new
pieces of data per second to be stored, you should have plenty of time
to write those entries individually, even if you write them multiple
times to different keys to make future queries faster. If you really
<em>are</em> receiving so many objects for storage that you don&#39;t have time
to write them individually, you can buffer and write blocks of them in
chunks.</p>

<p>In fact, a common data pattern is to assemble multiple objects into
larger collections for later retrieval, regardless of the ingest rate.</p>

<h3 id="what-about-updates?">What about updates?</h3>

<p>One key advantage to normalization is that you only have to update any
given piece of data once.</p>

<p>However, many use cases that require large quantities of storage deal
with mostly immutable data, such as log entries, sensor readings, and
media storage. You don&#39;t change your sensor data after it arrives, so
why do you care if each set of inputs appears in five different places
in your database?</p>

<p>Any information which must be updated frequently should be confined to
small objects that are limited in scope.</p>

<p>We&#39;ll talk much more about data modeling to account for mutable and
immutable data.</p>

<h3 id="further-reading">Further reading</h3>

<ul>
<li><a href="http://highlyscalable.wordpress.com/2012/03/01/nosql-data-modeling-techniques/">NoSQL Data Modeling Techniques</a> (Highly Scalable Blog)</li>
</ul>

<h2 id="data-modeling">Data modeling</h2>

<p>It can be hard to think outside the table, but once you do, you may
find interesting patterns to use in any database, even a
relational one.[^sql-databases]</p>

<p>[^sql-databases]: Feel free to use a relational database when you&#39;re
willing to sacrifice the scalability, performance, and availability of
Riak...but why would you?</p>

<p>If you thoroughly absorbed the earlier content, some of this may feel
redundant, but the implications of the key/value model are not always
obvious.</p>

<h3 id="rules-to-live-by">Rules to live by</h3>

<p>As with most such lists, these are guidelines rather than hard rules,
but take them seriously.</p>

<p>(@keys) Know your keys.</p>

<pre><code>The cardinal rule of any key/value datastore: the fastest way to get
data is to know what to look for, which means knowing which key you want.

How do you pull that off? Well, that&#39;s the trick, isn&#39;t it?

The best way to always know the key you want is to be able to
programmatically reproduce it based on information you already
have. Need to know the sales data for one of your client&#39;s
magazines in December 2013? Store it in a **sales** bucket and
name the key after the client, magazine, and month/year combo.

Guess what? Retrieving it will be much faster than running a SQL
`SELECT *` statement in a relational database.

And if it turns out that the magazine didn&#39;t exist yet, and there
are no sales figures for that month? No problem. A negative
response, especially for immutable data, is among the fastest
operations Riak offers.

Because keys are only unique within a bucket, the same unique
identifier can be used in different buckets to represent different
information about the same entity (e.g., a customer address might
be in an `address` bucket with the customer id as its key, whereas
the customer id as a key in a `contacts` bucket would presumably
contain contact information).
</code></pre>

<p>(@namespace) Know your namespaces.</p>

<pre><code>Riak has several levels of namespaces when storing data.

Historically, buckets have been what most thought of as Riak&#39;s
virtual namespaces.

The newest level is provided by **bucket types**, introduced in Riak 2.0, which
allow you to group buckets for configuration and security purposes.

Less obviously, keys are their own namespaces. If you want a
hierarchy for your keys that looks like `sales/customer/month`,
you don&#39;t need nested buckets: you just need to name your keys
appropriately, as discussed in (@keys). `sales` can be your
bucket, while each key is prepended with customer name and month.
</code></pre>

<p>(@views) Know your queries.</p>

<pre><code>Writing data is cheap. Disk space is cheap. Dynamic queries in Riak
are very, very expensive.

As your data flows into the system, generate the views you&#39;re going to
want later. That magazine sales example from (@keys)? The December
sales numbers are almost certainly aggregates of smaller values, but
if you know in advance that monthly sales numbers are going to be
requested frequently, when the last data arrives for that month the
application can assemble the full month&#39;s statistics for later
retrieval.

Yes, getting accurate business requirements is non-trivial, but
many Riak applications are version 2 or 3 of a system, written
once the business discovered that the scalability of MySQL,
Postgres, or MongoDB simply wasn&#39;t up to the job of handling their
growth.
</code></pre>

<p>(@small) Take small bites.</p>

<pre><code>Remember your parents&#39; advice over dinner? They were right.

When creating objects that will be updated, constrain their scope
and keep the number of contained elements low to reduce the odds
of multiple clients attempting to update the data concurrently.
</code></pre>

<p>(@indexes) Create your own indexes.</p>

<pre><code>Riak offers metadata-driven secondary indexes (2i) and full-text indexes
(Riak Search) for values, but these face scaling challenges: in
order to identify all objects for a given index value, roughly a
third of the cluster must be involved.

For many use cases, creating your own indexes is straightforward
and much faster/more scalable, since you&#39;ll be managing and
retrieving a single object.

See [Conflict Resolution](#conflict-resolution) for more discussion of this.
</code></pre>

<p>(@immutable) Embrace immutability.</p>

<pre><code>As we discussed in [Mutability], immutable data offers a way out
of some of the challenges of running a high-volume, high-velocity
datastore.

If possible, segregate mutable from non-mutable data, ideally
using different buckets for [request tuning][Request tuning].

[Datomic](http://www.datomic.com) is a unique data storage system
that leverages immutability for all data, with Riak commonly used
as a backend datastore. It treats any data item in its system as
a &quot;fact,&quot; to be potentially superseded by later facts but never
updated.
</code></pre>

<p>(@hybrid) Don&#39;t fear hybrid solutions.</p>

<pre><code>As much as we would all love to have a database that is an excellent
solution for any problem space, we&#39;re a long way from that goal.

In the meantime, it&#39;s a perfectly reasonable (and very common)
approach to mix and match databases for different needs. Riak is
very fast and scalable for retrieving keys, but it&#39;s decidedly
suboptimal at ad hoc queries. If you can&#39;t model your way out of
that problem, don&#39;t be afraid to store keys alongside searchable
metadata in a relational or other database that makes querying
simpler, and once you have the keys you need, grab the values
from Riak.

Just make sure that you consider failure scenarios when doing so;
it would be unfortunate to compromise Riak&#39;s availability by
rendering it useless when your other database is offline.
</code></pre>

<h3 id="further-reading">Further reading</h3>

<ul>
<li><a href="http://docs.basho.com/riak/latest/dev/data-modeling/">Use Cases</a></li>
</ul>

<h2 id="conflict-resolution">Conflict Resolution</h2>

<p>Conflict resolution is an inherent part of nearly any Riak
application, whether or not the developer knows it.</p>

<h3 id="conflict-resolution-strategies">Conflict resolution strategies</h3>

<p>There are basically 6 distinct approaches for dealing with conflicts
in Riak, and well-written applications will typically use a
combination of these strategies depending on the nature of the data.[^conflict-tuning]</p>

<p>[^conflict-tuning]: If each bucket has its own conflict resolution
strategy, requests against that bucket can be tuned appropriately. For
an example, see [Tuning for immutable data].</p>

<ul>
<li>Ignore the problem and let Riak pick a winner based on timestamp and
context if concurrent writes are received (aka &quot;last write wins&quot;).</li>
<li>Immutability: never update values, and thus never risk conflicts.</li>
<li>Instruct Riak to retain conflicting writes and resolve them with
custom business logic in the application.</li>
<li>Instruct Riak to retain conflicting writes and resolve them using
client-side data types designed to resolve conflicts automatically.</li>
<li>Instruct Riak to retain conflicting writes and resolve them using
server-side data types designed to resolve conflicts automatically.</li>
</ul>

<p>And, as of Riak 2.0, strong consistency can be used to avoid conflicts
(but as we&#39;ll discuss below there are significant downsides to doing
so).</p>

<h3 id="last-write-wins">Last write wins</h3>

<p>Prior to Riak 2.0, the default behavior was for Riak to resolve
siblings by default (see <a href="#tuning-parameters">Tuning parameters</a> for the parameter
<code>allow_mult</code>). With Riak 2.0, the default behavior changes to
retaining siblings for the application to resolve, although this will
not impact legacy Riak applications running on upgraded clusters.</p>

<p>For some use cases, letting Riak pick a winner is perfectly fine, but
make sure you&#39;re monitoring your system clocks and are comfortable
losing occasional (or not so occasional) updates.</p>

<h3 id="data-types">Data types</h3>

<p>It has always been possible to define data types on the client side to
merge conflicts automatically.</p>

<p>With Riak 1.4, Basho started introducing distributed data types
(formally known as <strong>CRDTs</strong>, or conflict-free replicated data types)
to allow the cluster to resolve conflicting writes automatically. The
first such type was a simple counter; Riak 2.0 adds sets and maps.</p>

<p>These types are still bound by the same basic constraints as the rest
of Riak. For example, if the same set is updated on either side of a
network split, requests for the set will respond differently until the
split heals; also, these objects should not be allowed to grow to
multiple megabytes in size.</p>

<h3 id="strong-consistency">Strong consistency</h3>

<p>As of Riak 2.0, it is possible to indicate that values should be
managed using a consensus protocol, so a quorum of the servers
responsible for that data must agree to a change before it is
committed.</p>

<p>This is a useful tool, but keep in mind the tradeoffs: writes will be
slower due to the coordination overhead, and Riak&#39;s ability to
continue to serve requests in the presence of network partitions and
server failures will be compromised.</p>

<p>For example, if a majority of the primary servers for the data are
unavailable, Riak will refuse to answer read requests if the surviving
servers are not certain the data they contain is accurate.</p>

<p>Thus, use this only when necessary, such as when the consequences of
conflicting writes are painful to cope with. An example of the need
for this comes from Riak CS: because users are allowed to create new
accounts, and because there&#39;s no convenient way to resolve username
conflicts if two accounts are created at the same time with the same
name, it is important to coordinate such requests.</p>

<h3 id="conflicting-resolution">Conflicting resolution</h3>

<p>Resolving conflicts when data is being rapidly updated can feel
Sysiphean.</p>

<p>It&#39;s always possible that two different clients will attempt to
resolve the same conflict at the same time, or that another client
will update a value between the time that one client retrieves
siblings and it attempts to resolve them. In either case you may have
new conflicts created by conducting conflict resolution.</p>

<p>Consider this yet another plug to consider immutability.</p>

<h3 id="further-reading">Further reading</h3>

<ul>
<li><a href="http://basho.com/clocks-are-bad-or-welcome-to-distributed-systems/">Clocks Are Bad, Or, Welcome to the Wonderful World of Distributed Systems</a> (Basho blog)</li>
<li><a href="http://basho.com/index-for-fun-and-for-profit/">Index for Fun and for Profit</a> (Basho blog)</li>
<li><a href="http://christophermeiklejohn.com/crdt/2014/07/22/readings-in-crdts.html">Readings in conflict-free replicated data types</a> (Chris Meiklejohn&#39;s blog)</li>
</ul>

<h2 id="request-tuning">Request tuning</h2>

<p>Riak is extensively (perhaps <em>too</em> extensively) configurable. Much of
that flexibility involves platform tuning accessible only via the host
operating system, but several core behavioral values can (and should)
be managed by applications.</p>

<p>With the notable exceptions of <code>n_val</code> (commonly referred to as <code>N</code>)
and <code>allow_mult</code>, the parameters described below can be overridden
with each request. All of them can be configured per-bucket type
(available with Riak 2.0) or per-bucket.</p>

<h3 id="key-concepts">Key concepts</h3>

<p>Any default value listed below as <strong>quorum</strong> is equivalent to
<code>n_val/2+1</code>, or <strong>2</strong> whenever <code>n_val</code> has not been modified.</p>

<p><strong>Primary</strong> servers are the cluster members that, in the absence of any
network or server failure, are supposed to &quot;own&quot; any given key/value
pair.</p>

<p>Riak&#39;s key/value engine does not itself write values to storage. That
job is left to the <strong>backends</strong> that Riak supports: Bitcask, LevelDB,
and Memory.</p>

<p>No matter what the parameters below are set to, requests will be
sent to <code>n_val</code> servers on behalf of the client, <strong>except</strong> for
strongly-consistent read requests with Riak 2.0, which can be safely
retrieved from the current leader for that key/value pair.</p>

<h3 id="tuning-parameters">Tuning parameters</h3>

<h4 id="leave-this-alone">Leave this alone</h4>

<p><code>n_val</code>
:   The number of copies of data that are written. This is independent of the number of servers in the cluster. Default: <strong>3</strong>.</p>

<p>The <code>n_val</code> is vital to nearly everything that Riak does. The default
value of 3 should never be lowered except in special circumstances,
and changing it after a bucket has data can lead to unexpected
behavior.</p>

<h4 id="configure-at-the-bucket">Configure at the bucket</h4>

<p><code>allow_mult</code>
:    Specify whether this bucket retains conflicts for the application to resolve (<code>true</code>) or pick a winner using vector clocks and server timestamp even if the causality history does not indicate that it is safe to do so (<code>false</code>). See <a href="#conflict-resolution">Conflict Resolution</a> for more. Default: <strong><code>false</code></strong> for untyped buckets (including all buckets prior to Riak 2.0), <strong><code>true</code></strong> otherwise</p>

<pre><code>You **should** give this value careful thought. You **must** know what it will be in your environment to do proper key/value data modeling.
</code></pre>

<p><code>last_write_wins</code>
:    Setting this to <code>true</code> is a slightly stronger version of <code>allow_mult=false</code>: when possible, Riak will write new values to storage without bothering to compare against existing values. Default: <strong><code>false</code></strong></p>

<h4 id="configure-at-the-bucket-or-per-request">Configure at the bucket or per-request</h4>

<p><code>r</code>
:   The number of servers that must <em>successfully</em> respond to a read request before the client will be sent a response. Default: <strong><code>quorum</code></strong></p>

<p><code>w</code>
:   The number of servers that must <em>successfully</em> respond to a write request before the client will be sent a response. Default: <strong><code>quorum</code></strong></p>

<p><code>pr</code>
:    The number of <em>primary</em> servers that must successfully respond to a read request before the client will be sent a response. Default: <strong>0</strong></p>

<p><code>pw</code>
:    The number of <em>primary</em> servers that must successfully respond to a write request before the client will be sent a response. Default: <strong>0</strong></p>

<p><code>dw</code>
:    The number of servers that must respond indicating that the value has been successfully handed off to the <em>backend</em> for durable storage before the client will be sent a response. Default: <strong>2</strong> (effective minimum <strong>1</strong>)</p>

<p><code>notfound_ok</code>
:    Specifies whether the absence of a value on a server should be treated as a successful assertion that the value doesn&#39;t exist (<code>true</code>) or as an error that should not count toward the <code>r</code> or <code>pr</code> counts (<code>false</code>). Default: <strong><code>true</code></strong></p>

<h4 id="impact">Impact</h4>

<p>Generally speaking, the higher the integer values listed above, the
more latency will be involved, as the server that received the request
will wait for more servers to respond before replying to the client.</p>

<p>Higher values can also increase the odds of a timeout failure or, in
the case of the primary requests, the odds that insufficient primary
servers will be available to respond.</p>

<h3 id="write-failures">Write failures</h3>

<p><strong><em>Please read this. Very important. Really.</em></strong></p>

<p>The semantics for write failure are <em>very different</em> under eventually
consistent Riak than they are with the optional strongly consistent
writes available in Riak 2.0, so I&#39;ll tackle each separately.</p>

<h4 id="eventual-consistency">Eventual consistency</h4>

<p>In most cases when the client receives an error message on a write
request, <em>the write was not a complete failure</em>. Riak is designed to
preserve your writes whenever possible, even if the parameters for a
request are not met. <strong>Riak will not roll back writes.</strong></p>

<p>Even if you attempt to read the value you just tried to write and
don&#39;t find it, that is <strong>not</strong> definitive proof that the write was a
complete failure. (Sorry.)</p>

<p>If the write is present on at least one server, <em>and</em> that server
doesn&#39;t crash and burn, <em>and</em> future updates don&#39;t supersede it,
the key and value written should make their way to all servers
responsible for them.</p>

<p>Retrying any updates that resulted in an error, with the appropriate
vector clock to help Riak intelligently resolve conflicts, won&#39;t cause
problems.</p>

<h4 id="strong-consistency">Strong consistency</h4>

<p>Strong consistency is the polar opposite from the default Riak
behaviors. If a client receives an error when attempting to write a
value, it is a safe bet that the value is not stashed somewhere in the
cluster waiting to be propagated, <strong>unless</strong> the error is a timeout,
the least useful of all possible responses.</p>

<p>No matter what response you receive, if you read the key and get the
new value back[^client-libs], you can be confident that all future
successful reads (until the next write) will return that same value.</p>

<p>[^client-libs]: To be <em>absolutely certain</em> your value is in Riak after
a write error and a successful read, you can issue a new read request
not tied to any existing object; your client library could be caching
the value you just wrote.</p>

<h3 id="tuning-for-immutable-data">Tuning for immutable data</h3>

<p>If you constrain a bucket to contain nothing but immutable data, you
can tune for very fast responses to read requests by setting <code>r=1</code> and
<code>notfound_ok=false</code>.</p>

<p>This means that read requests will (as always) be sent to all <code>n_val</code>
servers, but the first server that responds with <strong>a value other than
<code>notfound</code></strong> will be considered &quot;good enough&quot; for a response to the
client.</p>

<p>Ordinarily with <code>r=1</code> and the default value <code>notfound_ok=true</code> if the
first server that responds doesn&#39;t have a copy of your data you&#39;ll get
a <code>not found</code> response; if a failover server happens to be actively
serving requests, there&#39;s a very good chance it&#39;ll be the first to
respond since it won&#39;t yet have a copy of that key.</p>

<h3 id="further-reading">Further reading</h3>

<ul>
<li><a href="http://docs.basho.com/riak/latest/theory/concepts/Buckets/">Buckets</a> (docs.basho.com)</li>
<li><a href="http://docs.basho.com/riak/latest/theory/concepts/Eventual-Consistency/">Eventual Consistency</a> (docs.basho.com)</li>
<li><a href="http://docs.basho.com/riak/latest/theory/concepts/Replication/">Replication</a> (docs.basho.com)</li>
<li><a href="http://basho.com/understanding-riaks-configurable-behaviors-part-1/">Understanding Riak&#39;s Configurable Behaviors</a> (Basho blog series)</li>
</ul>
<h1 id="notes">Notes</h1>

<h2 id="a-short-note-on-riakcs">A Short Note on RiakCS</h2>

<p><em>Riak CS</em> is Basho&#39;s open-source extension to Riak that allows your cluster to act as
a remote object storage mechanism, comparable to (and compatible with) Amazon&#39;s
S3. There are several reasons why you may want to host your own cloud storage mechanism
(security, legal reasons, you already own lots of hardware, cheaper at scale).
Riak CS is not covered in this short book, but I may certainly be bribed to
write one.</p>

<h2 id="a-short-note-on-mdc">A Short Note on MDC</h2>

<p><em>MDC</em>, or Multi-Datacenter, is a commercial extension to Riak provided by Basho.
While the documentation is freely available, the source code is not. If you reach
a scale where keeping multiple Riak clusters in sync on a local or global scale is
necessary, I would recommend considering this option.</p>

<h2 id="locks,-a-cautionary-tale">Locks, a cautionary tale</h2>

<p>While assembling the <em>Writing Applications</em> chapter, I (John) tried to develop a data model
that would allow for reliable locking without strong consistency. While that attempt failed,
I thought it would be better to include it to illustrate the complexities of coping
with eventual consistency than to throw it away entirely.</p>

<p>Basic premise: multiple workers may be assigned datasets to process,
but each dataset should be assigned to no more than one worker.</p>

<p>In the absence of strong consistency, the best an application can do
is to use the <code>pr</code> and <code>pw</code> (primary read and primary write) parameters with
a value of <code>quorum</code> or <code>n_val</code>.</p>

<p>Common features to all of these models:</p>

<ul>
<li>Lock for any given dataset is a known key</li>
<li>Value is a unique identifier for the worker</li>
</ul>

<h3 id="lock,-a-first-draft">Lock, a first draft</h3>

<p><strong>Sequence</strong></p>

<p>Bucket: <code>allow_mult=false</code></p>

<ol>
<li>Worker reads with <code>pr=quorum</code> to determine whether a lock exists</li>
<li>If it does, move on to another dataset</li>
<li>If it doesn&#39;t, create a lock with <code>pw=quorum</code></li>
<li>Process dataset to completion</li>
<li>Remove the lock</li>
</ol>

<p><strong>Failure scenario</strong></p>

<ol>
<li>Worker #1 reads the non-existent lock</li>
<li>Worker #2 reads the non-existent lock</li>
<li>Worker #2 writes its ID to the lock</li>
<li>Worker #2 starts processing the dataset</li>
<li>Worker #1 writes its ID to the lock</li>
<li>Worker #1 starts processing the dataset</li>
</ol>

<h3 id="lock,-a-second-draft">Lock, a second draft</h3>

<p>Bucket: <code>allow_mult=false</code></p>

<p><strong>Sequence</strong></p>

<ol>
<li>Worker reads with <code>pr=quorum</code> to determine whether a lock exists</li>
<li>If it does, move on to another dataset</li>
<li>If it doesn&#39;t, create a lock with <code>pw=quorum</code></li>
<li>Read lock again with <code>pr=quorum</code></li>
<li>If the lock exists with another worker&#39;s ID, move on to another
dataset</li>
<li>Process dataset to completion</li>
<li>Remove the lock</li>
</ol>

<p><strong>Failure scenario</strong></p>

<ol>
<li>Worker #1 reads the non-existent lock</li>
<li>Worker #2 reads the non-existent lock</li>
<li>Worker #2 writes its ID to the lock</li>
<li>Worker #2 reads the lock and sees its ID</li>
<li>Worker #1 writes its ID to the lock</li>
<li>Worker #1 reads the lock and sees its ID</li>
<li>Both workers process the dataset</li>
</ol>

<p>If you&#39;ve done any programming with threads before, you&#39;ll recognize
this as a common problem with non-atomic lock operations.</p>

<h3 id="lock,-a-third-draft">Lock, a third draft</h3>

<p>Bucket: <code>allow_mult=true</code></p>

<p><strong>Sequence</strong></p>

<ol>
<li>Worker reads with <code>pr=quorum</code> to determine whether a lock exists</li>
<li>If it does, move on to another dataset</li>
<li>If it doesn&#39;t, create a lock with <code>pw=quorum</code></li>
<li>Read lock again with <code>pr=quorum</code></li>
<li>If the lock exists with another worker&#39;s ID <strong>or</strong> the lock
contains siblings, move on to another dataset</li>
<li>Process dataset to completion</li>
<li>Remove the lock</li>
</ol>

<p><strong>Failure scenario</strong></p>

<ol>
<li>Worker #1 reads the non-existent lock</li>
<li>Worker #2 reads the non-existent lock</li>
<li>Worker #2 writes its ID to the lock</li>
<li>Worker #1 writes its ID to the lock</li>
<li>Worker #1 reads the lock and sees a conflict</li>
<li>Worker #2 reads the lock and sees a conflict</li>
<li>Both workers move on to another dataset</li>
</ol>

<h3 id="lock,-a-fourth-draft">Lock, a fourth draft</h3>

<p>Bucket: <code>allow_mult=true</code></p>

<p><strong>Sequence</strong></p>

<ol>
<li>Worker reads with <code>pr=quorum</code> to determine whether a lock exists</li>
<li>If it does, move on to another dataset</li>
<li>If it doesn&#39;t, create a lock with <code>pw=quorum</code> and a timestamp</li>
<li>Read lock again with <code>pr=quorum</code></li>
<li>If the lock exists with another worker&#39;s ID <strong>or</strong> the lock
contains siblings <strong>and</strong> its timestamp is not the earliest, move on
to another dataset</li>
<li>Process dataset to completion</li>
<li>Remove the lock</li>
</ol>

<p><strong>Failure scenario</strong></p>

<ol>
<li>Worker #1 reads the non-existent lock</li>
<li>Worker #2 reads the non-existent lock</li>
<li>Worker #2 writes its ID and timestamp to the lock</li>
<li>Worker #2 reads the lock and sees its ID</li>
<li>Worker #2 starts processing the dataset</li>
<li>Worker #1 writes its ID and timestamp to the lock</li>
<li>Worker #1 reads the lock and sees its ID with the lowest timestamp</li>
<li>Worker #1 starts processing the dataset</li>
</ol>

<p>At this point I may hear you grumbling: clearly worker #2 would have
the lower timestamp because it attempted its write first, and thus #1
would skip the dataset and try another one.</p>

<p><em>Even if</em> both workers are running on the same server (and thus
<em>probably</em> have timestamps that can be compared)[^clock-comparisons],
perhaps worker #1 started its write earlier but contacted an overloaded cluster
member that took longer to process the request.</p>

<p>[^clock-comparisons]: An important part of any distributed systems
discussion is the fact that clocks are inherently untrustworthy, and
thus calling any event the &quot;last&quot; to occur is an exercise in faith:
faith that a system clock wasn&#39;t set backwards in time by <code>ntpd</code> or an
impatient system administrator, faith that all clocks involved are in
perfect sync.</p>

<pre><code>And, to be clear, perfect synchronization of clocks across
multiple systems is unattainable. Google is attempting to solve
this by purchasing lots of atomic and GPS clocks at great expense,
and even that only narrows the margin of error.
</code></pre>

<p>The same failure could occur if, instead of using timestamps for
comparison purposes, the ID of each worker was used for comparison
purposes. All it takes is for one worker to read its own write before
another worker&#39;s write request arrives.</p>

<h3 id="conclusion">Conclusion</h3>

<p>We can certainly come up with algorithms that limit the number of
times that multiple workers tackle the same job, but I have yet to
find one that guarantees exclusion.</p>

<p>What I found surprising about this exercise is that none of the
failure scenarios required any of the odder edge conditions that can
cause unexpected outcomes. For example, <code>pw=quorum</code> writes will return
an error to the client if 2 of the primary servers are not available,
but the value will <em>still</em> be written to the 3rd server and 2 fallback
servers. Predicting what will happen the next time someone tries to
read the key is challenging.</p>

<p>None of these algorithms required deletion of a value, but that is
particularly fraught with peril. It&#39;s not difficult to construct
scenarios in which deleted values reappear if servers are temporarily
unavailable during the deletion request.</p>
</body></html>
